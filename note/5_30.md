1. **卷积神经网络（Convolutional Neural Network，CNN）**：以卷积层（Convolution Layer）为主，池化层（Pooling Layer），全连接层（Fully Connected Layer）等算子（Operator）的组合形成的 AI 网络模型，并在计算机视觉领域取得明显效果和广泛应用的模型结构。
2. **循环神经网络（Recurrent Neural Network，RNN）**：以循环神经网络、长短时记忆（LSTM）等基本单元组合形成的适合时序数据预测（例如，自然语言处理、语音识别、监控时序数据等）的模型结构。
3. **图神经网络（Graph Neural Network，GNN）**：使用神经网络来学习图结构数据，提取和发掘图结构数据中的特征和模式，满足聚类、分类、预测、分割、生成等图学习任务需求的算法总称。目的是为了尽可能多的提取 “图” 中潜在的表征信息。
4. **生成对抗网络（Generative Adversarial Network，GAN）**：该架构训练两个神经网络相互竞争，从而从给定的训练数据集生成更真实的新数据。例如，可以从现有图像数据库生成新图像，也可以从歌曲数据库生成原创音乐。GAN 之所以被称为对抗网络，是因为该架构训练两个不同的网络并使其相互对抗。
5. **扩散概率模型（Diffusion Probabilistic Models）**：扩散概率模型是一类潜变量模型，是用变分估计训练的马尔可夫链。目标是通过对数据点在潜空间中的扩散方式进行建模，来学习数据集的潜结构。如计算机视觉中，意味着通过学习逆扩散过程训练神经网络，使其能对叠加了高斯噪声的图像进行去噪。
6. **混合结构网络（Model Ensemble）**：组合卷积神经网络和循环神经网络，进而解决如光学字符识别（OCR）等复杂应用场景的预测任务。



 AI 芯片也大放光彩。其中一个代表就是 Google TPU（Tensor Processing Unit），通过对深度学习模型中的算子进行抽象，转换为矩阵乘法或非线性变换，根据专用负载特点进一步定制流水线化执行的脉动阵列（Systolic Array），进一步减少访存提升计算密度，提高了 AI 模型的执行性能。华为昇腾 NPU（神经网络处理器）针对矩阵运算专门优化设计，可解决传统芯片在神经网络运算时效率低下的问题。此外，华为达芬奇架构面向 AI 计算设计，通过独创 3D Cube 设计，每时钟周期可进行 4096 次 MAC 运算，为 AI 提供强大算力支持。

除了算子层面驱动的定制，AI 层面的计算负载本身在算法层常常应用的稀疏性和量化等加速手段也逐渐被硬件厂商，根据通用算子定制到专用加速器中（例如，英伟达推出的 Transformer Engine），在专用计算领域进一步协同优化加速。通过定制化硬件，厂商又将处理器性能提升了大约  10^5 量级。

![img](https://chenzomi12.github.io/_images/03Architecture06.png)

## AI 框架层

AI 框架不仅仅是指如 PyTorch 等训练框架，还包括推理框架。负责静态程序分析与计算图构建，编译优化等工作。AI 框架本身通过提供供用户编程的 API 获取用户表达的模型，数据读取等意图，在静态程序分析阶段完成尽可能的自动前向计算图构建，自动求导补全反向传播计算图，计算图整体编译优化，算子内循环编译优化等。包括并不限于以下领域：

- **计算图构建**：静态计算图、动态计算图构建等。不同的 AI 框架类型决定了其使用静态还是动态图进行构建，静态图有利于获取更多信息做全图优化，动态图有利于调试，目前实际处于一个融合的状态，如 PyTorch2.X 版本后推出 Dynamo 特性支持原生静态图。
- **自动求导**：高效地对网络模型自动求导等。由于网络模型中大部分算子较为通用，AI 框架提前封装好算子的自动求导函数，待用户触发训练过程自动透明的进行全模型的自动求导，以支持梯度下降等训练算法需要的权重梯度数据的获取。
- **中间表达构建**：多层次中间表达等。通过构建网络模型的中间表达及多层中间表达，让模型本身可以更好的被下层 AI 编译器编译生成高效的后端代码。

 AI 框架一般会提供以下功能：

1. 以 Python API 供读者编写网络模型计算图结构；
2. 提供调用基本算子实现，大幅降低开发代码量；
3. 自动化内存管理、不暴露指针和内存管理给用户；
4. 实现自动微分功能，自动构建反向传播计算图；
5. 调用或生成运行时优化代码，调度算子在指定设备的执行；
6. 并在运行期应用并行算子，提升设备利用率等优化（动态优化）。

AI 框架帮助开发者解决了很多 AI System 底层问题，隐藏了很多工程的实现细节，但是这些细节和底层实现又是 AI System 工程师比较关注的点。





### 编译与运行时

负责 AI 模型在真正运行前的编译和系统运行时的动态调度与优化。当获取的网络模型计算图部署于单卡、多卡甚至是分布式 AI 集群的环境，运行期的框架需要对整体的计算图按照执行顺序调度算子与任务的执行、多路复用资源，做好内存等资源的分配与释放。包括并不限于以下部分：

- **编译优化**：如算子融合等。编译器根据算子的语义或者 IR 定义，对适合进行算子融合（多个算子和并为一个算子）的算子进行合并，降低内核启动与访存代价。同时 AI 编译器还支持循环优化等类似传统编译器的优化策略和面向深度学习的优化策略（如牺牲一定精度的计算图等价代换等）。
- **优化器**：运行时即时（Just-in-Time）优化，内省（Introspective）优化等。运行时根据硬件，隐藏的软件栈信息，数据分布等只能运行时所获取的信息，进一步对模型进行优化。
- **调度与执行**：调度优算子并行与调度，执行有单线程和多线程执行等。调度方面根据设备提供的软件栈和硬件调度策略，以及模型的算子间并行机会，进行类装箱的并行调度。另外再算子执行过程中，如果特定设备没有做过多的运行时调度与干预，框架可以设计高效的运行时算子内的线程调度策略。
- **硬件接口抽象**：GPU、NPU、TPU、CPU、FPGA 和 ASIC 等硬件的接口抽象。统一的硬件接口抽象可以复用编译优化策略，让优化方案与具体底层的 AI 硬件设备和 AI 体系结构适当解耦。

### 硬件体系结构与 AI 芯片

负责程序的真正执行、互联与加速。在更广的层面，作业与作业间需要平台提供调度，运行期资源分配与环境隔离。包括并不限于以下部分：

- **资源池化管理与调度**：异构资源集群管理等。将服务器资源池化，通过高效的调度器结合深度学习作业特点和异构硬件拓扑进行高效调度，这方面在对于云资源管理和云化较为重要。
- **可扩展的网络栈**：RDMA，InifiBand，NVLink 等。提供更高效的加速器到加速器的互联（例如 NVLink、NVSwitch 等）提供更高的网络带宽，更灵活的通信原语与高效的通信聚合算法（例如 AllReduce 算法）。

虽然 AI 系统在总的方向上分为开发体验层、框架层、编译与运行时和硬件体系结构和 AI 芯片 4 层结构。但是我们将在后续章节中，将会围绕核心系统软硬件，如 AI 训练和推理框架，AI 编译器，AI 芯片，部分涉及更广泛的 AI 系统生态中的重要内容如算法等展开介绍。









