自答一下，虽然近几年一直在这个领域工作，但毕竟题目太大，能力有限，抛砖引玉吧。

[https://deeplearningsystems.aideeplearningsystems.ai/](https://link.zhihu.com/?target=https%3A//deeplearningsystems.ai/)

由于题目源于上面这本书，我们就基于这本书的内容来展开讨论。

这本书的讨论路径如下：

Chapter 2: Building Blocks -> Chapter 3: Models and Applications -> Chapter 4: Training a Model -> Chapter 5: Distributed Training -> Chapter 6: Reducing the Model Size -> Chapter 7: Hardware -> Chapter 8: Compiler Optimizations -> Chapter 9: Frameworks and Compilers -> Chapter 10: Opportunities and Challenges

前两部分是算法和模型的情况，3,4,5是模型的训练和部署的话题，然后是硬件，[编译优化](https://www.zhihu.com/search?q=编译优化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2495099540})，框架和编译器的内容。

首先，作为MLSys需求源头的算法和模型，近几年还是有很多发展。但模型的backbone（或者说基本的building block）似乎变化不大。之前我也讨论过“[Hardware Lottery](https://zhuanlan.zhihu.com/p/346596743)”的现象，即成功的算法和模型往往是对底层硬件架构友好的。既底层的硬件架构不会有快速迭代，算法的building blocks趋于稳定也是比较正常的。当然不排除未来出现有breakthrough的新算法，那么可能就会驱动硬件架构有一个大的变化，进入一个新的循环。

模型的[back-bone](https://www.zhihu.com/search?q=back-bone&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2495099540})虽然比较稳定，但近两年模型的规模还是不断提升。这本书已经介绍了GPT-3，之后我们有看到一些更大的模型，规模到了500B的水平（这里指的是[dense model](https://www.zhihu.com/search?q=dense model&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2495099540})；sparse model，如MoE的规模还可以更大，但在系统设计角度不具可比性）。但一个有趣的现象是，GPT-4到今天也还没有推出。前一段时间有一篇文章，[What can we expect from GPT-4? (analyticsindiamag.com)](https://link.zhihu.com/?target=https%3A//analyticsindiamag.com/what-can-we-expect-from-gpt-4/)，对GPT-4有一些推测，认为GPT-4还是dense model，但它的规模不会比[GPT-3](https://www.zhihu.com/search?q=GPT-3&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2495099540})有太大的提升（GPT-3模型规模是GPT-2的约100倍）。

如果实际情况确实如此，那么是否模型规模提升的速度是否会放缓？这种情况的原因是现有系统已经无法支持更大模型，还是更大模型在性能上也无法有更大提升。如果是前者，那么支持大模型训练的超级系统还是有很强的需求。

这本书里介绍了[distributed training](https://link.zhihu.com/?target=https%3A//deeplearningsystems.ai/%23ch05/)的概念，但还比较简单。说到怎么训练[巨型模型](https://www.zhihu.com/search?q=巨型模型&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2495099540})的问题，近两年的工作很多。“[Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2104.04473v3)”，有很好的参考价值（结合NV的superpod做了大量优化）。另外，微软的[DeepSpeed - Microsoft Research](https://link.zhihu.com/?target=https%3A//www.microsoft.com/en-us/research/project/deepspeed/)，也是近几年很突出的工作。

模型的训练和部署，当然是ML/DL System讨论的一个重要话题。这本书在这部分覆盖的不够，产业最关注的还是从模型到部署的端到端解决方案。因此，这两年MLOps的概念也很热。这个概念有不同的解读，也有很多讨论，这里就不再深入。应该说现在AI落地的过程还有很多的优化空间。

------

下面讨论一下ML/DL System最核心的部分，硬件，框架和编译器。

硬件部分，这本书覆盖的到2020年的大部分重要工作，包括大厂和初创公司，也都给出了相应的参考，这里不再赘述。更新的重要信息，大家也可以看看"[AI Chip List](https://link.zhihu.com/?target=https%3A//basicmi.github.io/AI-Chip/)"，我的专栏的一些文章“[片上神经网络 - 知乎 (zhihu.com)](https://www.zhihu.com/column/DNN-on-Chip)”。另外， 

[@夏晶晶](https://www.zhihu.com/people/111caac9d15849bd5fdf5110ca84d465)

 也有很多精彩分析，非常值得阅读。总的来说，近两年我们看到很多有意思的硬件设计（包括芯片，互联和系统），以及大家挑战领域霸主的努力，还是非常精彩的。至于未来如何，更多的还是靠“干出来”。



框架层面，这两年Pytorch的势头有目共睹，它目前面临的一个问题是“易用性”和“性能”的平衡。Pytorch看家的“易用性”对新硬件特别是AI DSA硬件是不太友好的（光是怎么capture model这件事就。。。），导致大家接入的工作量很大。虽然这方面的努力和开展的项目很多，但这两者本身就是矛盾的事情，在不可能有完美方案的前提下，提供一个稳定统一的实用方案还是继续解决的问题。

除了Pytorch，框架方面还是有很多很多的努力，[JAX](https://link.zhihu.com/?target=https%3A//github.com/google/jax)，[Paddle](https://link.zhihu.com/?target=https%3A//www.paddlepaddle.org.cn/)，[Mindspore](https://link.zhihu.com/?target=https%3A//www.mindspore.cn/)，[Oneflow](https://link.zhihu.com/?target=https%3A//github.com/Oneflow-Inc/oneflow)等等都在努力做强自己的独特优势。业界和学术界还有很多更专用的框架，这里不再赘述。

在[编译器方面](https://link.zhihu.com/?target=https%3A//deeplearningsystems.ai/%23ch09/)，这本书覆盖的比较完整的。不过AI编译器从17年到现在，也已经从百花齐放，到逐渐收敛了。目前影响力最大的还是XLA，TVM和MLIR。XLA是较早的尝试，也是最成熟的AI编译器（主要针对TPU，但其IR（HLO）和编译优化还是具有很好的参考价值）。[TVM](https://link.zhihu.com/?target=https%3A//tvm.apache.org/)也在很多新硬件公司得到应用，有非常活跃的社区和[TVMConf](https://link.zhihu.com/?target=https%3A//www.tvmcon.org/)。从 

[@陈天奇](https://www.zhihu.com/people/1d09d935246523c51f69bd13d3c35cfe)

 这篇文章“[新一代深度学习编译技术变革和展望 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/446935289)”，可以很好的了解TVM的发展思路。[MLIR](https://link.zhihu.com/?target=https%3A//mlir.llvm.org/)出现更晚，但也有很多相关工作在开展。Chris Lattner的talk是了解MLIR思路的很好参考。



[MLIR中国社区：LLVM之父Chris Lattner：编译器的黄金时代518 赞同 · 18 评论文章](https://zhuanlan.zhihu.com/p/500904014)

从目前的发展来看，AI编译器离大家的期待还有很大的差距，还有很长的路要走。

------

如果我们完整的来看ML/DL System，近几年从软件到硬件有很多新的尝试，可以说是激动人心。但是，由于涉及到[全栈优化](https://www.zhihu.com/search?q=全栈优化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2495099540})的问题，还是有很多技术或非技术的障碍，特别是DSA系统。我在之前的一个talk里做了一些分析，[唐杉博士：人工智能芯片发展及挑战 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/440152476)，供参考。

目前看最成功的AI DSA系统还是Google的，DeepMind/JAX/XLA/TPU，毕竟是从算法一路下来都可以自己own。

------

最后，讨论ML/DL System的问题，一个很好的参考当然是MLSys会议。这个会议2018年开始，我当时还写了一篇文章“[ML + System = ? - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/34530781)”进行了分析。我们不妨看看今年的会议将会讨论哪些话题。