

# 一文了解高速互联技术CXL


Compute Express Link（CXL）作为一种先进的互连技术，在当今高性能计算领域引起了广泛关注，其高带宽、低延迟的特性使其成为连接处理器、加速器、存储等关键组件的理想选择。本文将深入探讨CXL技术，从其起源、特点，到应用领域和与其他技术的比较，全面了解CXL对现代数据中心生态系统的重要性。

**什么是CXL？**

CXL的起源可追溯到数据中心和高性能计算领域对更快速、更高效互连技术的需求。过去，HDD磁盘和内存之间差距很多，但随着SSD、NVMe 设备的出现逐渐弥补了中间的鸿沟。然而即使采用了NVMe设备，其与内存的差异仍然有10倍以上。传统数据库对于这一差异已经不再敏感，原因是因为系统的瓶颈已经来到了CPU侧，因此这几年所有人都在关注列存、向量化等技术来降低内存使用。对于许多应用而言，尽管NVMe的延迟已经足够满足要求，但是吞吐依然是很明显的瓶颈，因此并不能完全替代内存，这其中模型训练、向量数据都是非常典型的场景。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326513-01.png)

CXL的出现很好地解决了这个问题，通过将设备挂载到PCIe总线上，CXL实现了设备到CPU之间的互联，实现了存储计算分离。CXL 还允许 CPU 以低延迟和高带宽访问连接设备上更大的内存池，从而扩展内存。这可以增加 AI/ML 应用程序的内存容量和性能。

CXL利用灵活的处理器端口，可以在 PCIe 或 CXL 模式下运行。这两种设备类别均可在 PCIe5.0 中实现 32 GT/s 的数据速率，在 PCIe6.0 中实现高达 64 GT/s 的数据速率，为 AI/ML 应用提供了额外的功能和优势。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326514-02.png)

**为什么需要 CXL？**

随着可用数据量的增长，数据中心必须适应更复杂、要求更高的工作负载。已有数十年历史的服务器架构正在发生变化，使高性能计算系统能够处理人工智能/机器学习应用程序产生的大量数据。

这就是 CXL 的用武之地。CXL提供有效的资源共享/池来提高性能，最大限度地减少对复杂软件的需求，并降低系统总成本。

**CXL 的好处**

CXL 为企业和数据中心运营商带来多种优势，包括：

实现了计算和存储资源的分离，不再局限于CPU，GPU、FPGA都可以实现CXL协议共享内存资源并实现跟CPU的数据交互。

提供了比内存插槽所能容纳的更多的容量和带宽。

通过 CXL 连接设备，计算资源的扩展会变的更加容易。

内存变得更加弹性，按需分配、动态迁移都将变成可能。

允许 CPU 结合 DRAM 内存使用额外内存 。

**CXL vs. PCIe vs. NVLink**

互连技术在计算领域的进步中发挥着关键作用，而CXL、PCIe和NVLink则代表了当前领先的互连标准。以下是它们之间的对比：

**带宽和速度**

CXL：CXL在带宽方面表现卓越，CXL2.0支持32 GT/s的数据传输速率，CXL3.0支持64 GT/s的数据传输速率。这使得CXL能够更有效地满足处理大规模数据和高性能计算工作负载的需求。

PCIe：PCIe 4.0的速度为16 GT/s，PCIe 5.0为32 GT/s，PCIe 6.0提供了64 GT/s的速度。PCIe一直是计算系统中最广泛使用的互连技术之一。

NVLink：NVLink同样提供高带宽，适用于连接NVIDIA GPU。其速度通常与PCIe 5.0相当，但在GPU之间的通信中具有更低的延迟。

**内存共享**

CXL：CXL引入了内存共享的概念，使得不同设备能够直接访问共享的内存。这大大提高了内存访问的效率，尤其在大规模数据处理的场景中。

PCIe：PCIe通常需要通过额外的数据传输步骤实现设备之间的内存共享，这可能引入一些额外的延迟。

NVLink：NVLink也支持GPU之间的内存共享，优化了大规模并行计算的性能。

**多用途性**

CXL：CXL被设计成一种通用的互连技术，可连接处理器、加速器、存储等多种设备。其多用途性使得CXL在构建灵活且高性能的系统时更具优势。

PCIe：PCIe同样是一种通用的互连技术，广泛应用于连接图形卡、存储设备、网络适配器等。

NVLink：NVLink主要用于连接NVIDIA GPU，优化了GPU之间的通信，适用于图形处理、深度学习等对GPU性能要求较高的领域。

**生态系统支持**

CXL：CXL的生态系统正在迅速发展，越来越多的厂商开始支持CXL，并推出相应的硬件和软件产品。

PCIe：PCIe是当前计算领域中最成熟和广泛应用的互连技术之一，具有庞大的生态系统。

NVLink：NVLink主要由NVIDIA支持，因此在生态系统上可能相对受限。

**应用领域**

CXL：CXL广泛适用于数据中心、人工智能、科学计算等多个领域，具有灵活性和高性能的特点。

PCIe：PCIe在各种应用场景中都有广泛应用，从个人计算机到数据中心。

NVLink：NVLink主要用于连接NVIDIA GPU，在图形处理和深度学习等领域表现出色。

CXL、PCIe和NVLink各自具有独特的特点和优势，选择取决于特定应用的需求和硬件架构的要求。CXL以其高带宽、内存共享和多用途性等方面的优势，在未来有望在高性能计算领域取得更大的影响。PCIe作为成熟的互连标准在各个层面都有强大的生态系统。NVLink则在与NVIDIA GPU的协同工作方面表现突出，适用于对GPU性能要求较高的领域。

**CXL协议和标准**

CXL 联盟于 2019 年第三季度成立，是一个开放的行业标准组织，旨在创建技术规范，促进数据中心加速器和其他高速改进的开放生态系统，同时为新的使用模式实现突破性的性能。目前 CXL 已经发布了三个版本。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326515-03.png)

CXL 1.0 ：CXL 的第一个版本于 2019 年 3 月发布，基于 PCIe 5.0。它允许主机 CPU 通过缓存一致性协议 (CXL.cache) 访问加速器设备上的共享内存，并通过内存语义 (CXL.mem) 实现内存扩展。

CXL 2.0 ：CXL的第二个版本于2020年11月发布，基于PCIe 5.0。它支持 CXL 交换，将多个 CXL 设备连接到一个主机处理器或将每个设备汇集到多个主机处理器。它还实现了设备完整性和数据加密功能。

CXL 3.0 ：CXL的第三个版本于2022年8月发布，基于PCIe 6.0。它支持比 CXL 2.0 更高的带宽和更低的延迟，并增加了设备热插拔、电源管理和错误处理等新功能。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326516-04.png)

CXL 规范描述了允许设备相互通信的三种协议。

CXL.io：PCIe 5.0 协议的增强版本，可用于初始化、链接、设备发现、枚举和寄存器访问。它为 I/O 设备提供非一致的加载/存储接口。

CXL.cache ：一种缓存一致性协议，定义主机和设备之间的交互，允许连接的 CXL 设备使用请求和响应方法以极低的延迟有效地缓存主机内存。

CXL.mem ：一种内存协议，主机处理器可以使用加载和存储命令访问所连接设备的内存，主机 CPU 充当主设备，CXL 设备充当从设备。它可以支持易失性和持久性内存架构。

所有 CXL 设备都必须使用 CXL.io，但可以选择支持 CXL.cache 或 CXL.mem，或两者都支持。这些组合派生出三种设备类型：

类型 1 ：没有本地内存的专用加速器（例如智能网卡）。设备依赖于使用 CXL.io 和 CXL.cache 协议对主机 CPU 内存的一致访问。它们可以扩展 PCIe 协议功能（例如原子操作），并且可能需要实现自定义排序模型。

类型 2 ：具有高性能本地内存（GDDR 或 HBM）的通用加速器（GPU、ASIC 或 FPGA）。要访问主机 CPU 和设备内存，设备可以使用 CXL.io、CXL.cache 和 CXL.mem 协议。它们可以支持一致和非一致事务。

类型 3 ：内存扩展板和没有本地缓存的持久内存设备。设备可以使用 CXL.io 和 CXL.mem 协议，通过加载和存储命令为主机 CPU 提供对内存的访问。它们可以支持易失性和持久性内存架构。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326517-05.png)

由于CXL与PCIe紧密相关，新版本的CXL依赖于新版本的PCIe，接下来我们仔细研究一下 CXL 2.0 和 CXL 3.0 的区别。

**什么是 CXL 2.0**

如前所述，CXL 建立在 PCIe 物理基础之上，是一种连接标准，旨在管理比 PCIe 多得多的功能。除了充当主机和设备之间的数据传输之外，CXL 还支持三个分支：IO、Cache 和 Memory。

这三者构成了CXL 1.0和1.1标准中定义的连接主机和设备的新方法的核心。更新后的CXL 2.0标准对其进行了改进。

CXL 2.0 没有带宽或延迟升级，因为它仍然基于相同的 PCIe 5.0 物理标准，但它确实包含了某些急需的 PCIe 特定功能。

相同的 CXL.io、CXL.cache 和 CXL.mem 内在特性（处理如何处理数据以及在什么上下文中处理数据）是 CXL 2.0 的核心，同时还引入了交换功能、更多的加密以及对永久内存的支持。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326518-06.png)

**CXL 2.0 特性和优点**

**＞内存池**

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326519-07.png)

CXL 2.0 支持交换机启用内存池。主机可以通过 CXL 2.0 交换机访问池中的一台或多台设备。虽然主机必须支持 CXL 2.0 才能利用此功能，但可以在内存设备中使用支持 CXL 1.0、1.1 和 2.0 的硬件的组合。

在 1.0/1.1 版本下，设备只能充当单个逻辑设备，一次只能由一台主机访问。而一个2.0级别的设备可以被划分为无数个逻辑设备，从而使多达16个主机能够同时访问内存的各个部分。

例如，为了将其工作负载的内存需求与内存池中的可用容量精确匹配，主机1 (H1)可以使用设备1 (D1)中一半的内存和设备2 (D2)中四分之一的内存。

最多16台主机可以使用D1和D2设备的剩余空间。只有一台主机可以使用设备D3和D4，分别兼容CXL 1.0和1.1。

**＞CXL 2.0交换**

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326520-08.png)

了解 PCIe 交换机的用户应该知道，它们连接到具有一定数量通道（例如8个或16个通道）的主机处理器，然后支持下游大量附加通道以增加支持的设备数量。

例如，典型的 PCIe 交换机有 16 个用于 CPU 连接的通道，但下游有 48 个 PCIe 通道以支持六个链接的 GPU（每个通道为 x8）。

尽管存在上游瓶颈，但对于依赖 GPU 到 GPU 传输的工作负载来说，交换机是最佳选择，尤其是在 CPU 通道受限的系统上。CXL 2.0 现在支持交换标准。

**＞CXL 2.0 持久内存**

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326521-09.png)

CXL 2.0持久内存几乎与 DRAM 一样快，同时可以像 NAND 一样存储数据。

长期以来，人们一直不清楚这种存储器是通过 DRAM 接口作为慢速大容量存储器运行，还是通过类存储接口作为紧凑、快速存储运行。

原始 CXL 标准的 CXL.memory 标准并不直接提供持久内存，除非已经连接了设备。但这一次，CXL 2.0 提供了额外的 PMEM 支持。

**CXL 2.0 安全**

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326522-10.png)

CXL 链路的点对点安全性是最重要的功能改进。

CXL 2.0标准支持CXL控制器中的硬件加速进行任意对任意通信加密。

这是标准的一个可选组件，这意味着芯片提供商不必将其内置，如果已经内置了也可以选择启用或禁用它。

**CXL 2.0规范**

CXL 2.0 规范完全向后兼容 CXL 1.1 和 1.0，同时增加了对扇出交换的支持以连接到更多设备、内存池以提高内存利用效率、按需提供内存容量，以及对持久性内存的支持。

**CXL 2.0 规范的主要亮点**

增加了交换功能，支持资源迁移、内存扩展和设备扇出。

提供对内存池的支持以增加内存并减少或消除过度配置内存的需要。

通过添加链路级完整性和数据加密 (CXL IDE)，为通过 CXL 链路传输的数据提供机密性、完整性和重放保护。

**什么是 CXL 3.0**

处理器、存储、网络和其他加速器都可以通过 CXL 3.0 中的各种主机和加速器进行池化和动态寻址，从而进一步分解服务器的架构。这与 CXL 2.0 处理内存的方式类似。

此外，CXL 3.0 支持跨交换机或交换结构的组件/设备之间的直接通信。例如，两个 GPU 可以在不使用主机 CPU、内存或网络的情况下相互通信。

**CXL 3.0 规范的亮点**

Fabric 功能

多头和fabric连接设备

增强的fabric管理

可组合的分解基础设施

更好的可扩展性和更高的资源利用率

增强的内存池

多级交换

新的增强一致性功能

改进了软件功能

带宽加倍至 64 GT

与 CXL 2.0 相比零延迟

完全向后兼容CXL 2.0、CXL 1.1和CXL 1.0

**CXL 3.0的特点**

**＞ CXL 3.0 开关和扇出功能**
新的 CXL 交换和扇出功能是 CXL 3.0 的主要特性之一。CXL 2.0 中添加了交换功能，使众多主机和设备能够位于单个 CXL 交换机级别上。

得益于 CXL 3.0，CXL 拓扑现在可以支持多交换层。可以添加更多设备，每个 EDSFF 机架除了连接主机的架顶式 CXL 交换机外，还可以有一个 CXL 交换机。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326523-11.png)

**＞CXL 3.0 设备到设备通信**
CXL 还将 P2P 设备添加到设备通信中。P2P 允许设备直接通信，无需通过主机进行通信。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326524-12.png)

**＞CXL 3.0 一致性内存共享**
CXL 3.0 支持一致内存共享。这很重要，CXL 2.0 只允许在各种主机和加速器之间划分内存设备。CXL 3.0 允许一致性域中的所有主机共享内存，内存得到了更有效的利用。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326525-13.png)

**＞CXL 3.0：每个根端口支持多个设备**
CXL 3.0 中消除了先前对单个 CXL 根端口下行连接的 Type-1/Type-2 设备数量的限制。CXL 2.0 只允许这些处理设备中的一个出现在根端口的下游。CXL 根端口现在可以实现 Type-1/2/3 设备的完整混合搭配设置。

这尤其需要增加密度（每个主机有更多加速器）以及通过将多个加速器连接到单个交换机来使用新的点对点传输功能。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326526-14.png)

**＞CXL 3.0：Fabric**
CXL 3.0允许非树形拓扑，例如环形、网状和其他结构，即使只有两层交换机也是如此。各个节点的类型没有限制，可以是主机，也可以是设备。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326527-15.png)

CXL 3.0 甚至可以处理spine/leaf设计，其中流量通过顶部spine节点进行路由，将流量进一步路由回较低级别（leaf）节点，而这些节点又包含实际的主机/设备。

![img](https://cdn.eetrend.com/files/2023-11/wen_zhang_/100576398-326528-16.png)

**＞CXL 3.0：全局结构附加内存（Global Fabric Attached Memory）**
全局结构附加内存 (GFAM)支持使用新的内存、拓扑和结构功能，通过进一步分解特定主机的内存，推进了 CXL 的Type-3概念。

从这个意义上说，GFAM 设备本质上是主机和其他设备可以根据需要访问的共享内存池。此外，易失性和非易失性存储器（例如 DRAM 和闪存）可以组合在 GFAM 器件中。

**CXL 3.0 如何工作？**

CXL 3.0 与早期版本相比提供了多项升级。最重要的是引入了一种称为混合模式的新模式，它结合了批处理和实时方法的最佳元素。CXL 3.0 中包含的其他改进包括对更大数据集的支持、增强的性能等等。

**＞CXL 3.0的优势**
CXL 3.0 具有多项优势，跨对等点的直接内存访问是最有趣的特性之一（P2P DMA）。借助此功能，许多主机可以共享相同的内存空间和资源。因此，模型灵活性和可扩展性的使用以及性能也可以得到改善。

CXL 3.0 的另一个优势是支持更快的速度和更高的电源效率，从而更好地利用资源并提高性能。

此外，CXL 3.0 使设备能够更快地相互交互，提高系统吞吐量。

**CXL 3.1发布**

近日，CXL最新发布了3.1版，提供了更快、更安全的计算环境和更强大的技术基础，可将数据中心转变为巨型服务器。

新规范将支持 DDR6 内存，该内存仍在开发中。DDR 标准制定组织 JEDEC 尚未广泛讨论 DDR6。

CXL 3.1 协议可以打开更多的点对点通信，将内存和存储分解到单独的机箱中。CXL 3.1 规范提供了一个支持新型内存的开放标准，可以更有效地将数据重新路由到内存和加速器。一项重要的改进是将结构上的内存资源汇集到一个全局地址下，该功能称为全局集成内存，对于在内存和其它资源之间建立更快的连接非常重要。加速器还将能够直接与内存资源通信，基于端口的路由的新功能有助于更快地访问内存资源。

CXL 3.1 还提供了在受保护环境中执行数据的指令，引入这项技术是为了支持机密计算。新规范定义了一种安全协议，该协议可保证数据在内存、处理器和存储之间移动时扩展安全环境。该协议将检测需要对连接进行身份验证以打开硬件保险库，以访问代码或信息的环境。这些信息可能位于处理器、内存或存储上。

**总 结**

总的来说，CXL作为一项创新性的互连技术，为未来计算的发展描绘了令人兴奋的前景。其高性能、高带宽、内存共享和多用途性等特征，使其在处理大规模数据、高性能计算和人工智能等领域具有独特的优势。

随着CXL的逐步成熟和生态系统的不断扩大，我们可以期待看到更多的厂商加入支持CXL的行列，推动其在不同行业和应用场景中的广泛应用。未来，CXL的发展将不仅仅局限于数据中心，还可以扩展到边缘计算、量子计算等领域，并带来更大的创新和突破。

参考：
[https://www.spiceworks.com/tech/hardware/guest-article/what-is-compute-e...](https://www.spiceworks.com/tech/hardware/guest-article/what-is-compute-express-link/)
https://www.logic-fruit.com/glossary/compute-express-link/
https://zhuanlan.zhihu.com/p/646858357
[https://www.rambus.com/blogs/cxl-3-1-whats-next-for-cxl-based-memory-in-...](https://www.rambus.com/blogs/cxl-3-1-whats-next-for-cxl-based-memory-in-the-data-center/)

[CXL](https://fpga.eetrend.com/tag/cxl)

 

[数据中心](https://fpga.eetrend.com/tag/数据中心)



## 最新文章

- [Napatech IPU解决方案助力优化数据中心存储工作负载](https://fpga.eetrend.com/content/2024/100580889.html)
- [CXL技术：全面升级数据中心架构](https://fpga.eetrend.com/content/2024/100579866.html)
- [CXL，行业首选？](https://fpga.eetrend.com/content/2023/100576449.html)
- [CXL，扮演什么角色？](https://fpga.eetrend.com/content/2024/100581014.html)
- [基于FPGA的SmartNIC技术及其在数据中心的应用](https://fpga.eetrend.com/content/2021/100114009.html)
- [是时候了解CXL 3.1了](https://fpga.eetrend.com/content/2023/100576525.html)

## 最新文章

| [![img](https://cdn.eetrend.com/files/styles/medium/public/2023-08/wen_zhang_/100573312-312635-alveoqia-1.jpg?itok=Ryom2IHa)Alveo数据中心加速卡RMA 流程](https://fpga.eetrend.com/content/2023/100573312.html) | [![img](https://cdn.eetrend.com/files/styles/medium/public/2023-08/wen_zhang_/100573337-312761-cxl.jpg?itok=BlOBGtUz)即将掀起数据中心架构变革的CXL](https://fpga.eetrend.com/content/2023/100573337.html) | [![img](https://cdn.eetrend.com/files/styles/medium/public/2024-04/wen_zhang_/100579866-340732-shujuzhongxinip.jpg?itok=t4IX4hlH)CXL技术：全面升级数据中心架构](https://fpga.eetrend.com/content/2024/100579866.html) |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [![img](https://cdn.eetrend.com/files/styles/medium/public/2024-05/wen_zhang_/100580889-345395-tu6napatechf2070xipu.jpg?itok=hNn8l4Qn)Napatech IPU解决方案助力优化数据中心存储工作负载](https://fpga.eetrend.com/content/2024/100580889.html) | [![img](https://cdn.eetrend.com/files/styles/medium/public/2023-12/wen_zhang_/100576449-326706-shuju2.jpg?itok=1FlZVWkb)CXL，行业首选？](https://fpga.eetrend.com/content/2023/100576449.html) | [![img](https://cdn.eetrend.com/files/styles/medium/public/2024-05/wen_zhang_/100581014-345918-1.jpg?itok=Lc_RzI4l)CXL，扮演什么角色？](https://fpga.eetrend.com/content/2024/100581014.html) |

<iframe frameborder="0" height="210" marginheight="0" marginwidth="0" scrolling="no" src="https://images.eetrend.com/page/2021/100060312.html" width="100%" style="box-sizing: border-box;"></iframe>

