# 计算体系

## 传统AI深度网络

由于模型的发展，从DNN到RNN到Transformer

### AI 计算模式思考

结合上面介绍的经典 AI 模型结构特点，AI 芯片设计可以引出如下对于 AI 计算模式的思考：

1. **需要支持神经网络模型的计算逻辑。**比如不同神经元之间权重共享逻辑的支持，以及除了卷积/全连接这种计算密集型算子，还需要支持激活如 softmax, layernorm 等 Vector 类型访存密集型的算子。
2. **能够支持高维的张量存储与计算。**比如在卷积运算中，每一层都有大量的输入和输出通道，并且对于某些特定场景如遥感领域，Feature Map 的形状都非常大，在有限的芯片面积里最大化计算访存比，高效的内存管理设计非常重要。
3. **需要有灵活的软件配置接口，支持更多的神经网络模型结构。**针对不同领域，如计算机视觉、语音、自然语言处理，AI 模型具有不同形式的设计，但是作为 AI 芯片，需要尽可能全的支持所有应用领域的模型，并且支持未来可能出现的新模型结构，这样在一个漫长的芯片设计到流片的周期中，才能降低研发成本，获得市场的认可。







## 模型量化与压缩

![img](https://chenzomi12.github.io/_images/02ArchSlim08.png)



### AI 计算模式思考

结合上面 AI 模型的量化和剪枝算法的研究进展，AI 芯片设计可以引出如下对于 AI 计算模式的思考：

- **提供不同的 bit 位数的计算单元**。为了提升芯片性能并适应低比特量化研究的需求，我们可以考虑在芯片设计中集成多种不同位宽的计算单元和存储格式。例如，可以探索实现 fp8、int8、int4 等不同精度的计算单元，以适应不同精度要求的应用场景。
- **提供不同的 bit 位数存储格式**。在数据存储方面，在 M-bits（如 FP32）和 E-bits（如 TF32）格式之间做出明智的权衡，以及在 FP16 与 BF16 等不同精度的存储格式之间做出选择，以平衡存储效率和计算精度。
- **利用硬件提供专门针对稀疏结构计算的优化逻辑**。为了优化稀疏结构的计算，在硬件层面提供专门的优化逻辑，这将有助于提高稀疏数据的处理效率。同时，支持多种稀疏算法逻辑，并对硬件的计算取数模块进行设计上的优化，将进一步增强对稀疏数据的处理能力（如 NVIDA A100 对稀疏结构支持）。
- **硬件提供专门针对量化压缩算法的硬件电路**。通过增加数据压缩模块的设计，我们可以有效地减少内存带宽的压力，从而提升整体的系统性能。这些措施将共同促进芯片在处理低比特量化任务时的效率和性能。



## 轻量化模型

1. **减少内存空间的设计**

为了减小模型的参数量，在 VGG 和 inceptionNet 系列网络中，提出了将两个 3𝑥3 卷积核一个 5𝑥5 卷积核，和将一个 5𝑥1 卷积核和一个 1𝑥5 卷积核代替一个 5𝑥5 的卷积核的模型卷积层设计，如下图所示。

比如使用 2 个 3𝑥3 卷积核来代替 5𝑥5 卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果，并且模型参数可以由 5𝑥5𝑥𝐶𝑖𝑥𝐶𝑜 变成了 3𝑥3𝑥𝐶𝑖𝑥𝐶𝑜+3𝑥3𝑥𝐶𝑖𝑥𝐶𝑜，假设 𝐶𝑖=𝐶𝑜, 该层参数可以减小为原来的 18/25。

![img](https://chenzomi12.github.io/_images/03MobileParallel01.png)

1. **减少通道数的设计**

MobileNet 系列的网络设计中，提出了深度可分离卷积的设计策略，其中通过 Depthwise 逐层卷积加 1𝑥1 的卷积核来实现一个正常的卷积操作（如下图所示），1𝑥1 的 Pointwise 卷积负责完成卷积核通道的缩减来减小模型参数量。

![img](https://chenzomi12.github.io/_images/03MobileParallel02.png)

比如一个 3𝑥3 卷积核大小的卷积层，输入通道是 16， 输出通道是 32，正常的卷积模型参数是 3𝑥3𝑥16𝑥32=4608，而将其模型替代设计为一个 3𝑥3 卷积核的 Depthwise 卷积，和 1𝑥1 卷积核的 Pointwise 卷积，模型参数为 3𝑥3𝑥16+1𝑥1𝑥16𝑥32=656，可以看出模型参数量得到了很大的减少。

1. **减少卷积核个数的设计**

在 DenseNet 和 GhostNet 的模型设计中，提出了一种通过 Reuse Feature Map 的设计方式来减少模型参数和运算量。

如下图，对于 DenseNetV1 的结构设计来说，第 n 层的参数量由于复用了之前层的 Feature Map, 由 𝑘𝑥𝑘𝑥𝐶𝑖𝑥(𝐶1+𝐶2) 变为了 𝑘𝑥𝑘𝑥𝐶𝑖𝑥𝐶2，即为原来的 𝐶2/(𝐶1+𝐶2)，而 C2 远小于 C1，其中 k 表示卷积核尺寸, C1 表示前 n-1 层的 Feature Map 个数，C2 表示第 n 层的输出 Feature Map 个数。

![img](https://chenzomi12.github.io/_images/03MobileParallel03.png)

### AI 计算模式思考

通过上面模型网络轻量化的分类，可以看到 AI 模型网络中对卷积层的不同设计方法，这些都是芯片设计时候需要考虑支持的 AI 计算模式特性。

卷积核尺寸：

1. 小卷积核替代：用多个小卷积核代替单个大卷积核，以降低计算成本。
2. 多尺寸卷积核：采用不同尺寸的卷积核来捕捉多尺度特征。
3. 可变形卷积核：从固定形状转向可变形卷积核，以适应不同输入特征。
4. 1×1 卷积核：使用 1×1 卷积核构建 bottleneck 结构，有效减少参数和计算量。

卷积层运算：

1. Depthwise 卷积：用 Depthwise 卷积代替标准卷积，减少参数，保持特征表达。
2. Group 卷积：应用分组卷积，提高计算效率，降低模型复杂度。
3. Channel Shuffle：通过通道混洗（Channel Shuffle）增强特征融合，提升模型性能。
4. 通道加权：实施通道加权计算，动态调整通道贡献，优化特征表示。

卷积层连接：

1. Skip Connection：采用跳跃连接（Skip Connection），使网络能够更深，同时避免梯度消失问题。
2. Dense Connection：利用密集连接（Densely Connection），整合不同层的特征，增强特征融合和信息流。



## 大模型分布式并行

大模型算法作为一个火热的 AI 的研究领域，本身具有超高的模型参数量和计算量的特点。如何在 AI 芯片上高效的支持大模型算法是芯片设计公式必须要考虑的问题。在单芯片或者加速卡上无法提供所需的算力和内存需求的情况下，考虑大模型分布式并行技术是一个重要的研究方向。

分布式并行分为数据并行、模型并行，模型并行又分为张量并行和流水线并行。下面先介绍并行计算时候经常用到的集合通信原语，然后分别对数据并行和模型并行做一个简单的回顾。

### 集合通信原语

在并行计算中，通信原语是指用于在不同计算节点或设备之间进行数据传输和同步的基本操作。这些通信原语在并行计算中起着重要作用，能够实现节点间的数据传输和同步，从而实现复杂的并行算法和应用。一些常见的通信原语包括：

- All-reduce：所有节点上的数据都会被收集起来，然后进行某种操作（通常是求和或求平均），然后将结果广播回每个节点。这个操作在并行计算中常用于全局梯度更新。
- All-gather： 每个节点上的数据都被广播到其他所有节点上。每个节点最终都会收到来自所有其他节点的数据集合。这个操作在并行计算中用于收集各个节点的局部数据，以进行全局聚合或分析。
- Broadcast：一台节点上的数据被广播到其他所有节点上。通常用于将模型参数或其他全局数据分发到所有节点。
- Reduce： 将所有节点上的数据进行某种操作（如求和、求平均、取最大值等）后，将结果发送回指定节点。这个操作常用于在并行计算中进行局部聚合。
- Scatter： 从一个节点的数据集合中将数据分发到其他节点上。通常用于将一个较大的数据集合分割成多个部分，然后分发到不同节点上进行并行处理。
- Gather： 将各个节点上的数据收集到一个节点上。通常用于将多个节点上的局部数据收集到一个节点上进行汇总或分析。



根据模型在设备之间的通信程度，数据并行技术可以分为 DP, DDP, FSDP 三种。

1. Data parallelism, DP 数据并行

数据并行是最简单的一种分布式并行技术，具体实施是将大规模数据集分割成多个小批量，每个批量被发送到不同的计算设备（如 NPU）上并行处理。每个计算设备拥有完整的模型副本，并单独计算梯度，然后通过 all_reduce 通信机制在计算设备上更新模型参数，以保持模型的一致性。

2. Distribution Data Parallel, DDP 分布式数据并行

DDP 是一种分布式训练方法，它允许模型在多个计算节点上进行并行训练，每个节点都有自己的本地模型副本和本地数据。DDP 通常用于大规模的数据并行任务，其中模型参数在所有节点之间同步，但每个节点独立处理不同的数据批次。

在 DDP 中，每个节点上的模型副本执行前向和后向传播计算，并计算梯度。然后，这些梯度在不同的节点之间进行通信和平均，以便所有节点都可以使用全局梯度来更新其本地模型参数。这种方法的优点是可以扩展到大量的节点，并且可以显著减少每个节点的内存需求，因为每个节点只需要存储整个模型的一个副本。

DDP 通常与深度学习框架（如 PyTorch）一起使用，这些框架提供了对 DDP 的内置支持。例如，在 PyTorch 中，`torch.nn.parallel.DistributedDataParallel` 模块提供了 DDP 实现，它可以自动处理模型和梯度的同步，以及分布式训练的通信。

3. Fully Sharded Data Parallel, FSDP 全分片数据并行

Fully Sharded Data Parallelism (FSDP) 技术是 DP 和 DDP 技术的结合版本，可以实现更高效的模型训练和更好的横向扩展性。这种技术的核心思想是将神经网络的权重参数以及梯度信息进行分片（shard），并将这些分片分配到不同的设备或者计算节点上进行并行处理。FSDP 分享所有的模型参数，梯度，和优化状态。所以在计算的相应节点需要进行参数、梯度和优化状态数据的同步通信操作。

![img](https://chenzomi12.github.io/_images/03MobileParallel04.png)



### 模型并行技术

模型的并行技术可以总结为张量并行和流水并行。

1. **张量并行**

将模型的张量操作分解成多个子张量操作，并且在不同的设备上并行执行这些操作。这样做的好处是可以将大型模型的计算负载分布到多个设备上，从而提高模型的计算效率和训练速度。在张量并行中，需要考虑如何划分模型的不同层，并且设计合适的通信机制来在不同设备之间交换数据和同步参数。通常会使用诸如 All-reduce 等通信原语来实现梯度的聚合和参数的同步。

如下图是一个矩阵乘算子的张量并行示意。X 作为激活输入，A 作为算子权重，将 A 按列切分。每个计算节点保留一份完整的 A 和部分 A，最后通过 All Gather 通信将两个计算节点的数据进行同步拼接为一份完整的 Y 输出，供下一层使用。

![img](https://chenzomi12.github.io/_images/03MobileParallel05.png)

2. **流水并行**

将模型的不同层划分成多个阶段，并且每个阶段在不同的设备上并行执行。每个设备负责计算模型的一部分，并将计算结果传递给下一个设备，形成一个计算流水线。在流水并行中，需要设计合适的数据流和通信机制来在不同设备之间传递数据和同步计算结果。通常会使用缓冲区和流水线控制器来管理数据流，并确保计算的正确性和一致性。

如下图是一个流水线并行示意过程。假设一个模型有 Forward，Backward 两个阶段，有 0-3 共 4 层网络设计，分布在 4 个计算设备处理，右图展示了在时间维度下，不同层不同阶段的执行顺序示意。为了减少每个设备等待的时间（即中间空白的区域，称为 Bubble），一个简单的优化设计就是增加 data parallelism，让每层数据切分为若干个 batch，来提高流水线并行设备利用率。

![img](https://chenzomi12.github.io/_images/03MobileParallel06.png)

### AI 计算模式思考

根据上面对大模型并行技术的了解，不同的并行策略其实展示了 AI 计算模式是如何体现在硬件设计技术上。在芯片架构设计中可以从如下几个方面进行考虑。

1. **模型并行与数据并行支持**：AI 芯片需要能够同时支持模型并行和数据并行两种并行策略。对于模型并行，芯片需要具备灵活的计算资源分配和通信机制，以支持模型的不同部分在多个设备上进行计算。对于数据并行，芯片需要提供高带宽、低延迟的通信和同步机制，以支持多个设备之间的数据交换和同步。
2. **异构计算资源管理**：AI 芯片通常会包含多种计算资源，如 CPU、GPU、TPU 等。对于分布式并行计算，芯片需要提供统一的异构计算资源管理机制，以实现不同计算资源之间的协同工作和资源调度。
3. **高效的通信与同步机制**：分布式并行计算通常会涉及到大量的数据交换和同步操作。因此，AI 芯片需要提供高效的通信和同步机制，以降低通信延迟和提高通信带宽，从而实现高效的分布式计算。
4. **端到端的优化**：AI 芯片需要支持端到端的优化，包括模型设计、算法优化、系统设计等方面。通过综合考虑各个环节的优化策略，可以实现高效的大型模型分布式并行计算。比如 Transformer 是很多大模型结构的基础组件，可以提供专用高速 Transformer 引擎设计。

## 小结与讨论

通过以上我们对 AI 算法几个角度的研究进展分析，下面我们对 AI 特有计算模式做一个总结，这些特点将会对 AI 芯片的硬件设计具有很强的指导意义。

1. AI 芯片架构对神经网络模型架构的灵活支持以及对特有计算逻辑的高效执行能力

在深度学习和神经网络领域，不同的应用场景，AI 模型具有不同的网络架构设计，这意味着 AI 芯片需要具有灵活的架构设计，能够适应不同类型的任务和应用。比如可能支持模块化设计、分层结构或者其他形式的架构定义。

另一方面，神经网络包含前向传播、反向传播、优化算法等计算过程，数据通常以张量的形式表示，而高维张量可能包含大量的数据，例如图像、音频或文本数据。因此，AI 芯片架构必须支持高维张量的存储和计算逻辑，以达到高效的模型推理吞吐性能。

2. AI 芯片架构对模型压缩算法的支持

AI 模型压缩是提高模型推理部署性能的有效方法，压缩的方式有两种：量化为低比特网络和稀疏剪枝模型结构，这就需要 AI 芯片能够支持不同精度的运算和数据走线设计逻辑，比如设计专门的浮点运算，定点运算，混合运算模块，权重数据的稀疏压缩处理模块等，达到软件算法和硬件执行的高效协同配合，提高模型在终端部署的推理性能。

3. AI 芯片对轻量化网络结构的支持

轻量化网络结构设计也是 AI 模型的重要演变方向，可以让 AI 模型能够部署到算力和带宽受限的更多场景。轻量网络的结构设计对卷积运算做了更复杂的设计，如 Depthwise 的卷积核，以及卷积分组策略，Feature Map 复用等数据逻辑，这些都可以促使市场上出现一些更加定制化的芯片架构设计。

4. AI 芯片对大模型分布式并行策略的支持

模型在芯片的执行效率一方面取决于芯片自有的算力和面积限制，另一方面，上层软件对任务的调度策略也至关重要，尤其是应对大模型时代，芯片制程技术无法快速突破条件下，通过多芯片堆叠来提高性能。所以如何组建高效的片上网络接口和总线设计，支持大内存容量、高速互联带宽，都是未来芯片架构设计时重点的考虑方向。





## 关键设计指标

## 计算单位

**OPS**

OPS，Operations Per Second, 每秒操作数。 1 TOPS 代表处理器每秒进行一万亿次（1012）计算。

OPS/W：每瓦特运算性能。TOPS/W 评价处理器在 1W 功耗下运算能力的性能指标。

**MACs**

Multiply-Accumulate Operations，乘加累计操作。 1 MACs 包含一个乘法操作与一个加法操作，通常 1𝑀𝐴𝐶𝑠=2𝐹𝐿𝑂𝑃𝑠。

**FLOPs**

Floating Point Operations, 浮点运算次数，用来衡量模型计算复杂度，常用作神经网络模型速度的间接衡量标准。对于卷积层来说，FLOPs 的计算公式如下：

𝐹𝐿𝑂𝑃𝑠=2⋅𝐻⋅𝑊⋅𝐶𝑖𝑛⋅𝐾⋅𝐾⋅𝐶𝑜𝑢𝑡

**MAC**

Memory Access Cost，内存占用量，用来衡量模型在运行时的内存占用情况。对卷积层来说，MAC 的计算公式如下：

𝐻𝑖𝑛⋅𝑊𝑖𝑛⋅𝐶𝑖𝑛+𝐻𝑜𝑢𝑡⋅𝑊𝑜𝑢𝑡⋅𝐶𝑜𝑢𝑡+𝐾⋅𝐾⋅𝐶𝑖𝑛⋅𝐶𝑜𝑢𝑡

## AI 芯片关键指标







# AI芯片基础

## CPU 工作流

接下来我们来介绍一下这些主要单元是如何相互配合完成的也就是 CPU 的工作流，主要分为 4 步：

1. 从内存提取指令：取指令阶段是将内存中的指令读取到 CPU 中寄存器的过程，程序寄存器用于存储下一条指令所在的地址
2. 解码：解码指令译码阶段，在取指令完成后，立马进入指令译码阶段，在指令译码阶段，指令译码器按照预定的指令格式，对取回的指令进行拆分和解释，识别区分出不同的指令类别以及各种获取操作数的方法。
3. 执行：执行指令阶段，译码完成后，就需要执行这一条指令了，此阶段的任务是完成指令所规定的各种操作，具体实现指令的功能。根据指令的需要，有可能需要从内存中提取数据，根据指令地址码，得到操作数在主存中的地址，并从主存中读取该操作数用于运算。
4. 写回：结果写回阶段，作为最后一个阶段，结果写回（Write Back，WB）阶段把执行指令阶段的运行结果数据写回到 CPU 的内部寄存器中，以便被后续的指令快速地存取；

结合下图简单解释，第一步就是从内存里面去读取一些指令，给到我们的控制单元 CU，而控制单元就会对我们的刚才读取的一些指令来进行解码，变成正式的一些 command 命令，然后 ALU 就会去执行这些 command，这些命令执行完之后就会存储回来我们的内存进行汇总也就是写回。

![CPU 工作流](https://chenzomi12.github.io/_images/01CPUBase13.png)

### 单指令多线程（SIMT）

SIMT 中文译为单指令多线程，英文全称为 Single Instruction Multiple Threads，其主要应用于 GPU。从硬件层面看，GPU 本质上与前面提到的 SIMD 相同，都是少量的指令部件带一大堆运算部件。但是英伟达希望能够解决 SIMD 的这两个痛点。首先，在指令集的设计上，指令仍然像 SISD 一样，几个操作数就是几元运算，只不过在执行时，调度器会给这条指令分配很多套计算元件和寄存器。

这样做的第一个好处就是，这样的可执行代码可以通过一个类似高级语言多线程的编程模式编译而来，这个模式就是 CUDA，从而解决了第一个痛点。在用户看来，上述的“一套运算部件和寄存器”就像一个线程一样，因此这种模式被称为 SIMT。

此外，英伟达还为这些指令提供了很多修饰符，比如一个 Bit Mask 可以指定哪些线程干活，哪些空转，这样 SIMT 就可以很好地支持分支语句了，从而解决了第二个痛点。

因此，总的来说，英伟达提出 SIMT 的初衷是希望硬件像 SIMD 一样高效，编程起来又像多核多线程一样轻松。

![SIMT](https://chenzomi12.github.io/_images/01CPUBase20.png)







CPU 优化的目标是尽可能快地在尽可能低的延迟下执行完成任务，同时保持在任务之间具体快速切换的能力。它的本质是以序列化的方式处理任务。 GPU 的优化则全部都是用于增大吞吐量的，它允许一次将尽可能多的任务推送到 GPU 内部。然后 GPU 通过大数量的 Core 并行处理任务。





## 并发与并行

并行和并发是两个在计算机科学领域经常被讨论的概念，它们都涉及到同时处理多个任务的能力，但在具体含义和应用上有一些区别。

1. 并行（Parallelism）

并行指的是同时执行多个任务或操作，通常是在多个处理单元上同时进行。在计算机系统中，这些处理单元可以是多核处理器、多线程、分布式系统等。并行计算可以显著提高系统的性能和效率，特别是在需要处理大量数据或复杂计算的情况下。例如，一个计算机程序可以同时在多个处理器核心上运行，加快整体计算速度。

1. 并发（Concurrency）

并发指的是系统能够同时处理多个任务或操作，但不一定是同时执行。在并发系统中，任务之间可能会交替执行，通过时间片轮转或事件驱动等方式来实现。并发通常用于提高系统的响应能力和资源利用率，特别是在需要处理大量短时间任务的情况下。例如，一个 Web 服务器可以同时处理多个客户端请求，通过并发处理来提高系统的吞吐量。

因此并行和并发的主要区别如下：

- 并行是指同时执行多个任务，强调同时性和并行处理能力，常用于提高计算性能和效率。
- 并发是指系统能够同时处理多个任务，强调任务之间的交替执行和资源共享，常用于提高系统的响应能力和资源利用率。

在实际应用中，并行和并发通常结合使用，根据具体需求和系统特点来选择合适的技术和策略。同时，理解并行和并发的概念有助于设计和优化复杂的计算机系统和应用程序。在实际硬件工作的过程当中，更倾向于利用多线程对循环展开来提高整体硬件的利用率，这就是 GPU 的最主要的原理。



## GPU 线程分级

在 AI 计算模式中，不是所有的计算都可以是线程独立的。计算中数据结构元素之间的对应关系有以下三种：

- 1）Element-wise（逐元素）：逐元素操作是指对数据结构中的每个元素独立执行操作。这意味着操作应用于输入数据结构中对应元素的每一对，以生成输出数据结构。例如，对两个向量进行逐元素相加或相乘就是将对应元素相加或相乘，得到一个新的向量。
- 2）Local（局部）：局部操作是指仅针对数据的特定子集执行的操作，而不考虑整个数据结构。这些操作通常涉及局部区域或元素的计算。例如，对图像的卷积运算中元素之间是有交互的，因为它仅影响该区域内的像素值，计算一个元素往往需要周边的元素参与配合。
- 3）All to All（全对全）：全对全操作是指数据结构中的每个元素与同一数据结构或不同数据结构中的每个其他元素进行交互的操作。这意味着所有可能的元素对之间进行信息交换，产生完全连接的通信模式，一个元素的求解得到另一个数据时数据之间的交换并不能够做到完全的线程独立。全对全操作通常用于并行计算和通信算法中，其中需要在所有处理单元之间交换数据。

![AI 计算模式与线程的关系](https://chenzomi12.github.io/_images/02principle06.png)



## 计算强度

计算强度（Arithmetic Intensity）是指在执行计算任务时所需的算术运算量与数据传输量之比。它是衡量计算任务的计算密集程度的重要指标，可以帮助评估算法在不同硬件上的性能表现。通过计算强度，可以更好地理解计算任务的特性，有助于选择合适的优化策略和硬件配置，以提高计算任务的性能表现。计算强度的公式如下：

计算强度算术运算量数据传输量计算强度=算术运算量数据传输量

其中，算术运算量是指执行计算任务所需的浮点运算次数，数据传输量是指从内存读取数据或将数据写入内存的数据传输量。计算强度的值可以用来描述计算任务对计算和数据传输之间的依赖关系：

- 高计算强度：当计算强度较高时，意味着算术运算量较大，计算操作占据主导地位，相对较少的时间用于数据传输。在这种情况下，性能优化的重点通常是提高计算效率，如优化算法、并行计算等。
- 低计算强度：当计算强度较低时，意味着数据传输量较大，数据传输成为性能瓶颈。在这种情况下，性能优化的关键是减少数据传输、优化数据访问模式等。









## GPU架构和CUDA

GPU硬件基本概念

