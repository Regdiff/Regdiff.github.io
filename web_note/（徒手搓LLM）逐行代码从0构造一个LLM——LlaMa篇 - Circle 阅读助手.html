<!DOCTYPE html><html><head><meta name="author" content="Circle 阅读助手" /><meta name="description" content="由 [Circle 阅读助手](https://circlereader.com) 生成" /><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"  /><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><style>html,body{padding:0;margin:0;}.card{min-height:100vh;}.container .ant-app{padding-right: 0 !important;}
.anticon {
  display: inline-flex;
  alignItems: center;
  color: inherit;
  font-style: normal;
  line-height: 0;
  text-align: center;
  text-transform: none;
  vertical-align: -0.125em;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

.anticon > * {
  line-height: 1;
}

.anticon svg {
  display: inline-block;
}

.anticon::before {
  display: none;
}

.anticon .anticon-icon {
  display: block;
}

.anticon[tabindex] {
  cursor: pointer;
}

.anticon-spin::before,
.anticon-spin {
  display: inline-block;
  -webkit-animation: loadingCircle 1s infinite linear;
  animation: loadingCircle 1s infinite linear;
}

@-webkit-keyframes loadingCircle {
  100% {
    -webkit-transform: rotate(360deg);
    transform: rotate(360deg);
  }
}

@keyframes loadingCircle {
  100% {
    -webkit-transform: rotate(360deg);
    transform: rotate(360deg);
  }
}
.css-16cpc8y a{color:#416ed2;text-decoration:none;background-color:transparent;outline:none;cursor:pointer;transition:color 0.3s;-webkit-text-decoration-skip:objects;}.css-16cpc8y a:hover{color:#305ab7;}.css-16cpc8y a:active{color:#305ab7;}.css-16cpc8y a:active,.css-16cpc8y a:hover{text-decoration:none;outline:0;}.css-16cpc8y a:focus{text-decoration:none;outline:0;}.css-16cpc8y a[disabled]{color:rgba(0, 0, 0, 0.25);cursor:not-allowed;}.css-16cpc8y[class^="ant-avatar"],.css-16cpc8y[class*=" ant-avatar"]{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y[class^="ant-avatar"]::before,.css-16cpc8y[class*=" ant-avatar"]::before,.css-16cpc8y[class^="ant-avatar"]::after,.css-16cpc8y[class*=" ant-avatar"]::after{box-sizing:border-box;}.css-16cpc8y[class^="ant-avatar"] [class^="ant-avatar"],.css-16cpc8y[class*=" ant-avatar"] [class^="ant-avatar"],.css-16cpc8y[class^="ant-avatar"] [class*=" ant-avatar"],.css-16cpc8y[class*=" ant-avatar"] [class*=" ant-avatar"]{box-sizing:border-box;}.css-16cpc8y[class^="ant-avatar"] [class^="ant-avatar"]::before,.css-16cpc8y[class*=" ant-avatar"] [class^="ant-avatar"]::before,.css-16cpc8y[class^="ant-avatar"] [class*=" ant-avatar"]::before,.css-16cpc8y[class*=" ant-avatar"] [class*=" ant-avatar"]::before,.css-16cpc8y[class^="ant-avatar"] [class^="ant-avatar"]::after,.css-16cpc8y[class*=" ant-avatar"] [class^="ant-avatar"]::after,.css-16cpc8y[class^="ant-avatar"] [class*=" ant-avatar"]::after,.css-16cpc8y[class*=" ant-avatar"] [class*=" ant-avatar"]::after{box-sizing:border-box;}.css-16cpc8y.ant-avatar{box-sizing:border-box;margin:0;padding:0;color:#fff;font-size:14px;line-height:1.5714285714285714;list-style:none;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';position:relative;display:inline-flex;justify-content:center;align-items:center;overflow:hidden;white-space:nowrap;text-align:center;vertical-align:middle;background:rgb(127,127,127);border:1px solid transparent;width:32px;height:32px;border-radius:50%;}.css-16cpc8y.ant-avatar-image{background:transparent;}.css-16cpc8y.ant-avatar .ant-image-img{display:block;}.css-16cpc8y.ant-avatar.ant-avatar-square{border-radius:6px;}.css-16cpc8y.ant-avatar.ant-avatar-icon{font-size:18px;}.css-16cpc8y.ant-avatar.ant-avatar-icon >.anticon{margin:0;}.css-16cpc8y.ant-avatar-lg{width:40px;height:40px;border-radius:50%;}.css-16cpc8y.ant-avatar-lg.ant-avatar-square{border-radius:8px;}.css-16cpc8y.ant-avatar-lg.ant-avatar-icon{font-size:24px;}.css-16cpc8y.ant-avatar-lg.ant-avatar-icon >.anticon{margin:0;}.css-16cpc8y.ant-avatar-sm{width:24px;height:24px;border-radius:50%;}.css-16cpc8y.ant-avatar-sm.ant-avatar-square{border-radius:4px;}.css-16cpc8y.ant-avatar-sm.ant-avatar-icon{font-size:14px;}.css-16cpc8y.ant-avatar-sm.ant-avatar-icon >.anticon{margin:0;}.css-16cpc8y.ant-avatar >img{display:block;width:100%;height:100%;object-fit:cover;}.css-16cpc8y.ant-avatar-group{display:inline-flex;}.css-16cpc8y.ant-avatar-group .ant-avatar{border-color:#ffffff;}.css-16cpc8y.ant-avatar-group >*:not(:first-child){margin-left:-8px;}.css-16cpc8y.ant-avatar-group-popover .ant-avatar+.ant-avatar{margin-left:4px;}.css-16cpc8y.ant-wave{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-wave::before,.css-16cpc8y.ant-wave::after{box-sizing:border-box;}.css-16cpc8y.ant-wave [class^="ant-wave"],.css-16cpc8y.ant-wave [class*=" ant-wave"]{box-sizing:border-box;}.css-16cpc8y.ant-wave [class^="ant-wave"]::before,.css-16cpc8y.ant-wave [class*=" ant-wave"]::before,.css-16cpc8y.ant-wave [class^="ant-wave"]::after,.css-16cpc8y.ant-wave [class*=" ant-wave"]::after{box-sizing:border-box;}.css-16cpc8y.ant-wave{position:absolute;background:transparent;pointer-events:none;box-sizing:border-box;color:var(--wave-color, #416ed2);box-shadow:0 0 0 0 currentcolor;opacity:0.2;}.css-16cpc8y.ant-wave.wave-motion-appear{transition:box-shadow 0.4s cubic-bezier(0.08, 0.82, 0.17, 1),opacity 2s cubic-bezier(0.08, 0.82, 0.17, 1);}.css-16cpc8y.ant-wave.wave-motion-appear-active{box-shadow:0 0 0 6px currentcolor;opacity:0;}.css-16cpc8y.ant-wave.wave-motion-appear.wave-quick{transition:box-shadow 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.35s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-tag{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-tag::before,.css-16cpc8y.ant-tag::after{box-sizing:border-box;}.css-16cpc8y.ant-tag [class^="ant-tag"],.css-16cpc8y.ant-tag [class*=" ant-tag"]{box-sizing:border-box;}.css-16cpc8y.ant-tag [class^="ant-tag"]::before,.css-16cpc8y.ant-tag [class*=" ant-tag"]::before,.css-16cpc8y.ant-tag [class^="ant-tag"]::after,.css-16cpc8y.ant-tag [class*=" ant-tag"]::after{box-sizing:border-box;}.css-16cpc8y.ant-tag{box-sizing:border-box;margin:0;padding:0;color:#1b1b1b;font-size:12px;line-height:20px;list-style:none;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';display:inline-block;height:auto;margin-right:8px;padding-left:7px;padding-right:7px;white-space:nowrap;background:#fafafa;border:1px solid rgba(237,237,237, 0.6);border-radius:4px;opacity:1;transition:all 0.2s;text-align:start;position:relative;}.css-16cpc8y.ant-tag.ant-tag-rtl{direction:rtl;}.css-16cpc8y.ant-tag,.css-16cpc8y.ant-tag a,.css-16cpc8y.ant-tag a:hover{color:#1b1b1b;}.css-16cpc8y.ant-tag .ant-tag-close-icon{margin-left:3px;font-size:10px;color:rgb(57,57,57);cursor:pointer;transition:all 0.2s;}.css-16cpc8y.ant-tag .ant-tag-close-icon:hover{color:#1b1b1b;}.css-16cpc8y.ant-tag.ant-tag-has-color{border-color:transparent;}.css-16cpc8y.ant-tag.ant-tag-has-color,.css-16cpc8y.ant-tag.ant-tag-has-color a,.css-16cpc8y.ant-tag.ant-tag-has-color a:hover,.css-16cpc8y.ant-tag.ant-tag-has-color .anticon-close,.css-16cpc8y.ant-tag.ant-tag-has-color .anticon-close:hover{color:#fff;}.css-16cpc8y.ant-tag-checkable{background-color:transparent;border-color:transparent;cursor:pointer;}.css-16cpc8y.ant-tag-checkable:not(.ant-tag-checkable-checked):hover{color:#416ed2;background-color:rgba(0, 0, 0, 0.06);}.css-16cpc8y.ant-tag-checkable:active,.css-16cpc8y.ant-tag-checkable-checked{color:#fff;}.css-16cpc8y.ant-tag-checkable-checked{background-color:#416ed2;}.css-16cpc8y.ant-tag-checkable-checked:hover{background-color:#6891de;}.css-16cpc8y.ant-tag-checkable:active{background-color:#2c50ab;}.css-16cpc8y.ant-tag-hidden{display:none;}.css-16cpc8y.ant-tag >.anticon+span,.css-16cpc8y.ant-tag >span+.anticon{margin-left:7px;}.css-16cpc8y.ant-tag-borderless{border-color:transparent;background:#fafafa;}.css-16cpc8y.ant-space{display:inline-flex;}.css-16cpc8y.ant-space-rtl{direction:rtl;}.css-16cpc8y.ant-space-vertical{flex-direction:column;}.css-16cpc8y.ant-space-align{flex-direction:column;}.css-16cpc8y.ant-space-align-center{align-items:center;}.css-16cpc8y.ant-space-align-start{align-items:flex-start;}.css-16cpc8y.ant-space-align-end{align-items:flex-end;}.css-16cpc8y.ant-space-align-baseline{align-items:baseline;}.css-16cpc8y.ant-space .ant-space-item:empty{display:none;}.css-16cpc8y.ant-space .ant-space-item>.ant-badge-not-a-wrapper:only-child{display:block;}.css-16cpc8y.ant-space-gap-row-small{row-gap:8px;}.css-16cpc8y.ant-space-gap-row-middle{row-gap:16px;}.css-16cpc8y.ant-space-gap-row-large{row-gap:24px;}.css-16cpc8y.ant-space-gap-col-small{column-gap:8px;}.css-16cpc8y.ant-space-gap-col-middle{column-gap:16px;}.css-16cpc8y.ant-space-gap-col-large{column-gap:24px;}.css-16cpc8y.ant-space-block{display:flex;width:100%;}.css-16cpc8y.ant-space-vertical{flex-direction:column;}.css-16cpc8y.ant-btn{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-btn::before,.css-16cpc8y.ant-btn::after{box-sizing:border-box;}.css-16cpc8y.ant-btn [class^="ant-btn"],.css-16cpc8y.ant-btn [class*=" ant-btn"]{box-sizing:border-box;}.css-16cpc8y.ant-btn [class^="ant-btn"]::before,.css-16cpc8y.ant-btn [class*=" ant-btn"]::before,.css-16cpc8y.ant-btn [class^="ant-btn"]::after,.css-16cpc8y.ant-btn [class*=" ant-btn"]::after{box-sizing:border-box;}.css-16cpc8y.ant-btn{outline:none;position:relative;display:inline-block;font-weight:400;white-space:nowrap;text-align:center;background-image:none;background:transparent;border:1px solid transparent;cursor:pointer;transition:all 0.2s cubic-bezier(0.645, 0.045, 0.355, 1);user-select:none;touch-action:manipulation;color:#1b1b1b;}.css-16cpc8y.ant-btn:disabled>*{pointer-events:none;}.css-16cpc8y.ant-btn >span{display:inline-block;}.css-16cpc8y.ant-btn .ant-btn-icon{line-height:0;}.css-16cpc8y.ant-btn >.anticon+span,.css-16cpc8y.ant-btn >span+.anticon{margin-left:8px;}.css-16cpc8y.ant-btn:not(.ant-btn-icon-only)>.ant-btn-icon.ant-btn-loading-icon,.css-16cpc8y.ant-btn:not(.ant-btn-icon-only)>.ant-btn-icon:not(:last-child){margin-right:8px;}.css-16cpc8y.ant-btn >a{color:currentColor;}.css-16cpc8y.ant-btn:not(:disabled):focus-visible{outline:4px solid #c3d9f7;outline-offset:1px;transition:outline-offset 0s,outline 0s;}.css-16cpc8y.ant-btn.ant-btn-two-chinese-chars::first-letter{letter-spacing:0.34em;}.css-16cpc8y.ant-btn.ant-btn-two-chinese-chars>*:not(.anticon){margin-right:-0.34em;letter-spacing:0.34em;}.css-16cpc8y.ant-btn-icon-only.ant-btn-compact-item{flex:none;}.css-16cpc8y.ant-btn{font-size:14px;line-height:1.5714285714285714;height:32px;padding:4px 15px;border-radius:6px;}.css-16cpc8y.ant-btn.ant-btn-icon-only{width:32px;padding-left:0;padding-right:0;}.css-16cpc8y.ant-btn.ant-btn-icon-only.ant-btn-round{width:auto;}.css-16cpc8y.ant-btn.ant-btn-icon-only .anticon{font-size:16px;}.css-16cpc8y.ant-btn.ant-btn-loading{opacity:0.65;cursor:default;}.css-16cpc8y.ant-btn .ant-btn-loading-icon{transition:width 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.3s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-btn.ant-btn-circle.ant-btn{min-width:32px;padding-left:0;padding-right:0;border-radius:50%;}.css-16cpc8y.ant-btn.ant-btn-round.ant-btn{border-radius:32px;padding-left:16px;padding-right:16px;}.css-16cpc8y.ant-btn-sm{font-size:14px;line-height:1.5714285714285714;height:24px;padding:0px 7px;border-radius:4px;}.css-16cpc8y.ant-btn-sm.ant-btn-icon-only{width:24px;padding-left:0;padding-right:0;}.css-16cpc8y.ant-btn-sm.ant-btn-icon-only.ant-btn-round{width:auto;}.css-16cpc8y.ant-btn-sm.ant-btn-icon-only .anticon{font-size:14px;}.css-16cpc8y.ant-btn-sm.ant-btn-loading{opacity:0.65;cursor:default;}.css-16cpc8y.ant-btn-sm .ant-btn-loading-icon{transition:width 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.3s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-btn.ant-btn-circle.ant-btn-sm{min-width:24px;padding-left:0;padding-right:0;border-radius:50%;}.css-16cpc8y.ant-btn.ant-btn-round.ant-btn-sm{border-radius:24px;padding-left:12px;padding-right:12px;}.css-16cpc8y.ant-btn-lg{font-size:16px;line-height:1.5;height:40px;padding:7px 15px;border-radius:8px;}.css-16cpc8y.ant-btn-lg.ant-btn-icon-only{width:40px;padding-left:0;padding-right:0;}.css-16cpc8y.ant-btn-lg.ant-btn-icon-only.ant-btn-round{width:auto;}.css-16cpc8y.ant-btn-lg.ant-btn-icon-only .anticon{font-size:18px;}.css-16cpc8y.ant-btn-lg.ant-btn-loading{opacity:0.65;cursor:default;}.css-16cpc8y.ant-btn-lg .ant-btn-loading-icon{transition:width 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.3s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-btn.ant-btn-circle.ant-btn-lg{min-width:40px;padding-left:0;padding-right:0;border-radius:50%;}.css-16cpc8y.ant-btn.ant-btn-round.ant-btn-lg{border-radius:40px;padding-left:20px;padding-right:20px;}.css-16cpc8y.ant-btn.ant-btn-block{width:100%;}.css-16cpc8y.ant-btn-default{background:#ffffff;border-color:rgba(237,237,237, 0.6);color:#1b1b1b;box-shadow:0 2px 0 rgba(0, 0, 0, 0.02);}.css-16cpc8y.ant-btn-default:disabled,.css-16cpc8y.ant-btn-default.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-default:not(:disabled):not(.ant-btn-disabled):hover{color:#6891de;border-color:#6891de;background:#ffffff;}.css-16cpc8y.ant-btn-default:not(:disabled):not(.ant-btn-disabled):active{color:#2c50ab;border-color:#2c50ab;background:#ffffff;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost{color:#ffffff;background:transparent;border-color:#ffffff;box-shadow:none;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-default.ant-btn-dangerous{color:#ff4d4f;border-color:#ff4d4f;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;border-color:#ffa39e;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#d9363e;border-color:#d9363e;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost{color:#ff4d4f;background:transparent;border-color:#ff4d4f;box-shadow:none;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-default.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-primary{color:#fff;background:#416ed2;box-shadow:0 2px 0 rgba(5, 122, 255, 0.06);}.css-16cpc8y.ant-btn-primary:disabled,.css-16cpc8y.ant-btn-primary.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-primary:not(:disabled):not(.ant-btn-disabled):hover{color:#fff;background:#6891de;}.css-16cpc8y.ant-btn-primary:not(:disabled):not(.ant-btn-disabled):active{color:#fff;background:#2c50ab;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost{color:#416ed2;background:transparent;border-color:#416ed2;box-shadow:none;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;color:#6891de;border-color:#6891de;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;color:#2c50ab;border-color:#2c50ab;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous{background:#ff4d4f;box-shadow:0 2px 0 rgba(255, 38, 5, 0.06);color:#fff;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{background:#ff7875;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{background:#d9363e;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost{color:#ff4d4f;background:transparent;border-color:#ff4d4f;box-shadow:none;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;color:#ff7875;border-color:#ff7875;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;color:#d9363e;border-color:#d9363e;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-dashed{background:#ffffff;border-color:rgba(237,237,237, 0.6);color:#1b1b1b;box-shadow:0 2px 0 rgba(0, 0, 0, 0.02);border-style:dashed;}.css-16cpc8y.ant-btn-dashed:disabled,.css-16cpc8y.ant-btn-dashed.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-dashed:not(:disabled):not(.ant-btn-disabled):hover{color:#6891de;border-color:#6891de;background:#ffffff;}.css-16cpc8y.ant-btn-dashed:not(:disabled):not(.ant-btn-disabled):active{color:#2c50ab;border-color:#2c50ab;background:#ffffff;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost{color:#ffffff;background:transparent;border-color:#ffffff;box-shadow:none;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous{color:#ff4d4f;border-color:#ff4d4f;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;border-color:#ffa39e;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#d9363e;border-color:#d9363e;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost{color:#ff4d4f;background:transparent;border-color:#ff4d4f;box-shadow:none;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-link{color:#416ed2;}.css-16cpc8y.ant-btn-link:not(:disabled):not(.ant-btn-disabled):hover{color:#305ab7;background:transparent;}.css-16cpc8y.ant-btn-link:not(:disabled):not(.ant-btn-disabled):active{color:#305ab7;}.css-16cpc8y.ant-btn-link:disabled,.css-16cpc8y.ant-btn-link.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-link.ant-btn-dangerous{color:#ff4d4f;}.css-16cpc8y.ant-btn-link.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;}.css-16cpc8y.ant-btn-link.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#d9363e;}.css-16cpc8y.ant-btn-link.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-link.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-text:not(:disabled):not(.ant-btn-disabled):hover{color:#1b1b1b;background:rgba(0, 0, 0, 0.06);}.css-16cpc8y.ant-btn-text:not(:disabled):not(.ant-btn-disabled):active{color:#1b1b1b;background:rgba(0, 0, 0, 0.15);}.css-16cpc8y.ant-btn-text:disabled,.css-16cpc8y.ant-btn-text.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-text.ant-btn-dangerous{color:#ff4d4f;}.css-16cpc8y.ant-btn-text.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-text.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-text.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;background:#fff2f0;}.css-16cpc8y.ant-btn-text.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#ff7875;background:#fff2f0;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost{color:#ffffff;background:transparent;border-color:#ffffff;box-shadow:none;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-group{position:relative;display:inline-flex;}.css-16cpc8y.ant-btn-group >span:not(:last-child),.css-16cpc8y.ant-btn-group >.ant-btn:not(:last-child),.css-16cpc8y.ant-btn-group >span:not(:last-child)>.ant-btn,.css-16cpc8y.ant-btn-group >.ant-btn:not(:last-child)>.ant-btn{border-top-right-radius:0;border-bottom-right-radius:0;}.css-16cpc8y.ant-btn-group >span:not(:first-child),.css-16cpc8y.ant-btn-group >.ant-btn:not(:first-child){margin-left:-1px;}.css-16cpc8y.ant-btn-group >span:not(:first-child),.css-16cpc8y.ant-btn-group >.ant-btn:not(:first-child),.css-16cpc8y.ant-btn-group >span:not(:first-child)>.ant-btn,.css-16cpc8y.ant-btn-group >.ant-btn:not(:first-child)>.ant-btn{border-top-left-radius:0;border-bottom-left-radius:0;}.css-16cpc8y.ant-btn-group .ant-btn{position:relative;z-index:1;}.css-16cpc8y.ant-btn-group .ant-btn:hover,.css-16cpc8y.ant-btn-group .ant-btn:focus,.css-16cpc8y.ant-btn-group .ant-btn:active{z-index:2;}.css-16cpc8y.ant-btn-group .ant-btn[disabled]{z-index:0;}.css-16cpc8y.ant-btn-group .ant-btn-icon-only{font-size:14px;}.css-16cpc8y.ant-btn-group >span:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:last-child)>.ant-btn-primary:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:last-child)>.ant-btn-primary:not(:disabled){border-right-color:#6891de;}.css-16cpc8y.ant-btn-group >span:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:first-child)>.ant-btn-primary:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:first-child)>.ant-btn-primary:not(:disabled){border-left-color:#6891de;}.css-16cpc8y.ant-btn-group >span:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:last-child)>.ant-btn-danger:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:last-child)>.ant-btn-danger:not(:disabled){border-right-color:#ff7875;}.css-16cpc8y.ant-btn-group >span:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:first-child)>.ant-btn-danger:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:first-child)>.ant-btn-danger:not(:disabled){border-left-color:#ff7875;}.css-16cpc8y[class^="ant-modal"],.css-16cpc8y[class*=" ant-modal"]{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y[class^="ant-modal"]::before,.css-16cpc8y[class*=" ant-modal"]::before,.css-16cpc8y[class^="ant-modal"]::after,.css-16cpc8y[class*=" ant-modal"]::after{box-sizing:border-box;}.css-16cpc8y[class^="ant-modal"] [class^="ant-modal"],.css-16cpc8y[class*=" ant-modal"] [class^="ant-modal"],.css-16cpc8y[class^="ant-modal"] [class*=" ant-modal"],.css-16cpc8y[class*=" ant-modal"] [class*=" ant-modal"]{box-sizing:border-box;}.css-16cpc8y[class^="ant-modal"] [class^="ant-modal"]::before,.css-16cpc8y[class*=" ant-modal"] [class^="ant-modal"]::before,.css-16cpc8y[class^="ant-modal"] [class*=" ant-modal"]::before,.css-16cpc8y[class*=" ant-modal"] [class*=" ant-modal"]::before,.css-16cpc8y[class^="ant-modal"] [class^="ant-modal"]::after,.css-16cpc8y[class*=" ant-modal"] [class^="ant-modal"]::after,.css-16cpc8y[class^="ant-modal"] [class*=" ant-modal"]::after,.css-16cpc8y[class*=" ant-modal"] [class*=" ant-modal"]::after{box-sizing:border-box;}.css-16cpc8y.ant-modal-root .ant-modal-wrap-rtl{direction:rtl;}.css-16cpc8y.ant-modal-root .ant-modal-centered{text-align:center;}.css-16cpc8y.ant-modal-root .ant-modal-centered::before{display:inline-block;width:0;height:100%;vertical-align:middle;content:"";}.css-16cpc8y.ant-modal-root .ant-modal-centered .ant-modal{top:0;display:inline-block;padding-bottom:0;text-align:start;vertical-align:middle;}@media (max-width: 767px){.css-16cpc8y.ant-modal-root .ant-modal{max-width:calc(100vw - 16px);margin:8px auto;}.css-16cpc8y.ant-modal-root .ant-modal-centered .ant-modal{flex:1;}}.css-16cpc8y.ant-modal{box-sizing:border-box;margin:0 auto;padding:0;color:#1b1b1b;font-size:14px;line-height:1.5714285714285714;list-style:none;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';pointer-events:none;position:relative;top:100px;width:auto;max-width:calc(100vw - 32px);padding-bottom:24px;}.css-16cpc8y.ant-modal .ant-modal-title{margin:0;color:#1b1b1b;font-weight:600;font-size:16px;line-height:1.5;word-wrap:break-word;}.css-16cpc8y.ant-modal .ant-modal-content{position:relative;background-color:#ffffff;background-clip:padding-box;border:0;border-radius:8px;box-shadow:0 6px 16px 0 rgba(0, 0, 0, 0.08),0 3px 6px -4px rgba(0, 0, 0, 0.12),0 9px 28px 8px rgba(0, 0, 0, 0.05);pointer-events:auto;padding:20px 24px;}.css-16cpc8y.ant-modal .ant-modal-close{position:absolute;top:12px;right:12px;z-index:1010;padding:0;color:rgb(57,57,57);font-weight:600;line-height:1;text-decoration:none;background:transparent;border-radius:4px;width:32px;height:32px;border:0;outline:0;cursor:pointer;transition:color 0.2s,background-color 0.2s;}.css-16cpc8y.ant-modal .ant-modal-close-x{display:flex;font-size:16px;font-style:normal;line-height:32px;justify-content:center;text-transform:none;text-rendering:auto;}.css-16cpc8y.ant-modal .ant-modal-close:hover{color:#1b1b1b;background-color:rgba(0, 0, 0, 0.06);text-decoration:none;}.css-16cpc8y.ant-modal .ant-modal-close:active{background-color:rgba(0, 0, 0, 0.15);}.css-16cpc8y.ant-modal .ant-modal-close:focus-visible{outline:4px solid #c3d9f7;outline-offset:1px;transition:outline-offset 0s,outline 0s;}.css-16cpc8y.ant-modal .ant-modal-header{color:#1b1b1b;background:#ffffff;border-radius:8px 8px 0 0;margin-bottom:8px;padding:0;border-bottom:none;}.css-16cpc8y.ant-modal .ant-modal-body{font-size:14px;line-height:1.5714285714285714;word-wrap:break-word;padding:0;}.css-16cpc8y.ant-modal .ant-modal-footer{text-align:end;background:transparent;margin-top:12px;padding:0;border-top:none;border-radius:0;}.css-16cpc8y.ant-modal .ant-modal-footer >.ant-btn+.ant-btn{margin-left:8px;}.css-16cpc8y.ant-modal .ant-modal-open{overflow:hidden;}.css-16cpc8y.ant-modal-pure-panel{top:auto;padding:0;display:flex;flex-direction:column;}.css-16cpc8y.ant-modal-pure-panel .ant-modal-content,.css-16cpc8y.ant-modal-pure-panel .ant-modal-body,.css-16cpc8y.ant-modal-pure-panel .ant-modal-confirm-body-wrapper{display:flex;flex-direction:column;flex:auto;}.css-16cpc8y.ant-modal-pure-panel .ant-modal-confirm-body{margin-bottom:auto;}.css-16cpc8y.ant-modal-root .ant-modal-wrap-rtl{direction:rtl;}.css-16cpc8y.ant-modal-root .ant-modal-wrap-rtl .ant-modal-confirm-body{direction:rtl;}.css-16cpc8y.ant-modal-root .ant-modal.ant-zoom-enter,.css-16cpc8y.ant-modal-root .ant-modal.ant-zoom-appear{transform:none;opacity:0;animation-duration:0.3s;user-select:none;}.css-16cpc8y.ant-modal-root .ant-modal.ant-zoom-leave .ant-modal-content{pointer-events:none;}.css-16cpc8y.ant-modal-root .ant-modal-mask{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1000;height:100%;background-color:rgba(0, 0, 0, 0.45);pointer-events:none;}.css-16cpc8y.ant-modal-root .ant-modal-mask .ant-modal-hidden{display:none;}.css-16cpc8y.ant-modal-root .ant-modal-wrap{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1000;overflow:auto;outline:0;-webkit-overflow-scrolling:touch;}.css-16cpc8y.ant-modal-root .ant-fade-enter,.css-16cpc8y.ant-modal-root .ant-fade-appear{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-modal-root .ant-fade-leave{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-modal-root .ant-fade-enter.ant-fade-enter-active,.css-16cpc8y.ant-modal-root .ant-fade-appear.ant-fade-appear-active{animation-name:css-16cpc8y-antFadeIn;animation-play-state:running;}.css-16cpc8y.ant-modal-root .ant-fade-leave.ant-fade-leave-active{animation-name:css-16cpc8y-antFadeOut;animation-play-state:running;pointer-events:none;}.css-16cpc8y.ant-modal-root .ant-fade-enter,.css-16cpc8y.ant-modal-root .ant-fade-appear{opacity:0;animation-timing-function:linear;}.css-16cpc8y.ant-modal-root .ant-fade-leave{animation-timing-function:linear;}.css-16cpc8y.ant-zoom-enter,.css-16cpc8y.ant-zoom-appear{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-zoom-leave{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-zoom-enter.ant-zoom-enter-active,.css-16cpc8y.ant-zoom-appear.ant-zoom-appear-active{animation-name:css-16cpc8y-antZoomIn;animation-play-state:running;}.css-16cpc8y.ant-zoom-leave.ant-zoom-leave-active{animation-name:css-16cpc8y-antZoomOut;animation-play-state:running;pointer-events:none;}.css-16cpc8y.ant-zoom-enter,.css-16cpc8y.ant-zoom-appear{transform:scale(0);opacity:0;animation-timing-function:cubic-bezier(0.08, 0.82, 0.17, 1);}.css-16cpc8y.ant-zoom-enter-prepare,.css-16cpc8y.ant-zoom-appear-prepare{transform:none;}.css-16cpc8y.ant-zoom-leave{animation-timing-function:cubic-bezier(0.78, 0.14, 0.15, 0.86);}@keyframes css-16cpc8y-antFadeIn{0%{opacity:0;}100%{opacity:1;}}@keyframes css-16cpc8y-antFadeOut{0%{opacity:1;}100%{opacity:0;}}@keyframes css-16cpc8y-antZoomIn{0%{transform:scale(0.2);opacity:0;}100%{transform:scale(1);opacity:1;}}@keyframes css-16cpc8y-antZoomOut{0%{transform:scale(1);}100%{transform:scale(0.2);opacity:0;}}.css-16cpc8y.ant-typography{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-typography::before,.css-16cpc8y.ant-typography::after{box-sizing:border-box;}.css-16cpc8y.ant-typography [class^="ant-typography"],.css-16cpc8y.ant-typography [class*=" ant-typography"]{box-sizing:border-box;}.css-16cpc8y.ant-typography [class^="ant-typography"]::before,.css-16cpc8y.ant-typography [class*=" ant-typography"]::before,.css-16cpc8y.ant-typography [class^="ant-typography"]::after,.css-16cpc8y.ant-typography [class*=" ant-typography"]::after{box-sizing:border-box;}.css-16cpc8y.ant-typography{color:#1b1b1b;word-break:break-word;line-height:1.5714285714285714;}.css-16cpc8y.ant-typography.ant-typography-secondary{color:rgb(57,57,57);}.css-16cpc8y.ant-typography.ant-typography-success{color:#52c41a;}.css-16cpc8y.ant-typography.ant-typography-warning{color:#faad14;}.css-16cpc8y.ant-typography.ant-typography-danger{color:#ff4d4f;}a.css-16cpc8y.ant-typography.ant-typography-danger:active,a.css-16cpc8y.ant-typography.ant-typography-danger:focus{color:#d9363e;}a.css-16cpc8y.ant-typography.ant-typography-danger:hover{color:#ff7875;}.css-16cpc8y.ant-typography.ant-typography-disabled{color:rgba(0, 0, 0, 0.25);cursor:not-allowed;user-select:none;}div.css-16cpc8y.ant-typography,.css-16cpc8y.ant-typography p{margin-bottom:1em;}h1.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h1,div.css-16cpc8y.ant-typography-h1>textarea,.css-16cpc8y.ant-typography h1{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:38px;line-height:1.2105263157894737;}h2.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h2,div.css-16cpc8y.ant-typography-h2>textarea,.css-16cpc8y.ant-typography h2{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:30px;line-height:1.2666666666666666;}h3.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h3,div.css-16cpc8y.ant-typography-h3>textarea,.css-16cpc8y.ant-typography h3{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:24px;line-height:1.3333333333333333;}h4.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h4,div.css-16cpc8y.ant-typography-h4>textarea,.css-16cpc8y.ant-typography h4{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:20px;line-height:1.4;}h5.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h5,div.css-16cpc8y.ant-typography-h5>textarea,.css-16cpc8y.ant-typography h5{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:16px;line-height:1.5;}.css-16cpc8y.ant-typography+h1.ant-typography,.css-16cpc8y.ant-typography+h2.ant-typography,.css-16cpc8y.ant-typography+h3.ant-typography,.css-16cpc8y.ant-typography+h4.ant-typography,.css-16cpc8y.ant-typography+h5.ant-typography{margin-top:1.2em;}.css-16cpc8y.ant-typography div +h1,.css-16cpc8y.ant-typography ul +h1,.css-16cpc8y.ant-typography li +h1,.css-16cpc8y.ant-typography p +h1,.css-16cpc8y.ant-typography h1 +h1,.css-16cpc8y.ant-typography h2 +h1,.css-16cpc8y.ant-typography h3 +h1,.css-16cpc8y.ant-typography h4 +h1,.css-16cpc8y.ant-typography h5 +h1,.css-16cpc8y.ant-typography div +h2,.css-16cpc8y.ant-typography ul +h2,.css-16cpc8y.ant-typography li +h2,.css-16cpc8y.ant-typography p +h2,.css-16cpc8y.ant-typography h1 +h2,.css-16cpc8y.ant-typography h2 +h2,.css-16cpc8y.ant-typography h3 +h2,.css-16cpc8y.ant-typography h4 +h2,.css-16cpc8y.ant-typography h5 +h2,.css-16cpc8y.ant-typography div +h3,.css-16cpc8y.ant-typography ul +h3,.css-16cpc8y.ant-typography li +h3,.css-16cpc8y.ant-typography p +h3,.css-16cpc8y.ant-typography h1 +h3,.css-16cpc8y.ant-typography h2 +h3,.css-16cpc8y.ant-typography h3 +h3,.css-16cpc8y.ant-typography h4 +h3,.css-16cpc8y.ant-typography h5 +h3,.css-16cpc8y.ant-typography div +h4,.css-16cpc8y.ant-typography ul +h4,.css-16cpc8y.ant-typography li +h4,.css-16cpc8y.ant-typography p +h4,.css-16cpc8y.ant-typography h1 +h4,.css-16cpc8y.ant-typography h2 +h4,.css-16cpc8y.ant-typography h3 +h4,.css-16cpc8y.ant-typography h4 +h4,.css-16cpc8y.ant-typography h5 +h4,.css-16cpc8y.ant-typography div +h5,.css-16cpc8y.ant-typography ul +h5,.css-16cpc8y.ant-typography li +h5,.css-16cpc8y.ant-typography p +h5,.css-16cpc8y.ant-typography h1 +h5,.css-16cpc8y.ant-typography h2 +h5,.css-16cpc8y.ant-typography h3 +h5,.css-16cpc8y.ant-typography h4 +h5,.css-16cpc8y.ant-typography h5 +h5{margin-top:1.2em;}.css-16cpc8y.ant-typography code{margin:0 0.2em;padding-left:0.4em;padding-right:0.4em;padding-top:0.2em;padding-bottom:0.1em;font-size:85%;font-family:'SFMono-Regular',Consolas,'Liberation Mono',Menlo,Courier,monospace;background:rgba(150, 150, 150, 0.1);border:1px solid rgba(100, 100, 100, 0.2);border-radius:3px;}.css-16cpc8y.ant-typography kbd{margin:0 0.2em;padding-left:0.4em;padding-right:0.4em;padding-top:0.15em;padding-bottom:0.1em;font-size:90%;font-family:'SFMono-Regular',Consolas,'Liberation Mono',Menlo,Courier,monospace;background:rgba(150, 150, 150, 0.06);border:1px solid rgba(100, 100, 100, 0.2);border-bottom-width:2px;border-radius:3px;}.css-16cpc8y.ant-typography mark{padding:0;background-color:#ffe58f;}.css-16cpc8y.ant-typography u,.css-16cpc8y.ant-typography ins{text-decoration:underline;text-decoration-skip-ink:auto;}.css-16cpc8y.ant-typography s,.css-16cpc8y.ant-typography del{text-decoration:line-through;}.css-16cpc8y.ant-typography strong{font-weight:600;}.css-16cpc8y.ant-typography ul,.css-16cpc8y.ant-typography ol{margin-left:0;margin-right:0;margin-top:0;margin-bottom:1em;padding:0;}.css-16cpc8y.ant-typography ul li,.css-16cpc8y.ant-typography ol li{margin-left:20px;margin-right:0;margin-top:0;margin-bottom:0;padding-left:4px;padding-right:0;padding-top:0;padding-bottom:0;}.css-16cpc8y.ant-typography ul{list-style-type:circle;}.css-16cpc8y.ant-typography ul ul{list-style-type:disc;}.css-16cpc8y.ant-typography ol{list-style-type:decimal;}.css-16cpc8y.ant-typography pre,.css-16cpc8y.ant-typography blockquote{margin:1em 0;}.css-16cpc8y.ant-typography pre{padding:0.4em 0.6em;white-space:pre-wrap;word-wrap:break-word;background:rgba(150, 150, 150, 0.1);border:1px solid rgba(100, 100, 100, 0.2);border-radius:3px;font-family:'SFMono-Regular',Consolas,'Liberation Mono',Menlo,Courier,monospace;}.css-16cpc8y.ant-typography pre code{display:inline;margin:0;padding:0;font-size:inherit;font-family:inherit;background:transparent;border:0;}.css-16cpc8y.ant-typography blockquote{padding-left:0.6em;padding-right:0;padding-top:0;padding-bottom:0;border-left:4px solid rgba(100, 100, 100, 0.2);opacity:0.85;}a.css-16cpc8y.ant-typography,.css-16cpc8y.ant-typography a{color:#416ed2;text-decoration:none;outline:none;cursor:pointer;transition:color 0.3s;}a.css-16cpc8y.ant-typography:focus,.css-16cpc8y.ant-typography a:focus,a.css-16cpc8y.ant-typography:hover,.css-16cpc8y.ant-typography a:hover{color:#305ab7;}a.css-16cpc8y.ant-typography:active,.css-16cpc8y.ant-typography a:active{color:#305ab7;}a.css-16cpc8y.ant-typography:active,.css-16cpc8y.ant-typography a:active,a.css-16cpc8y.ant-typography:hover,.css-16cpc8y.ant-typography a:hover{text-decoration:none;}a.css-16cpc8y.ant-typography[disabled],.css-16cpc8y.ant-typography a[disabled],a.css-16cpc8y.ant-typography.ant-typography-disabled,.css-16cpc8y.ant-typography a.ant-typography-disabled{color:rgba(0, 0, 0, 0.25);cursor:not-allowed;}a.css-16cpc8y.ant-typography[disabled]:active,.css-16cpc8y.ant-typography a[disabled]:active,a.css-16cpc8y.ant-typography.ant-typography-disabled:active,.css-16cpc8y.ant-typography a.ant-typography-disabled:active,a.css-16cpc8y.ant-typography[disabled]:hover,.css-16cpc8y.ant-typography a[disabled]:hover,a.css-16cpc8y.ant-typography.ant-typography-disabled:hover,.css-16cpc8y.ant-typography a.ant-typography-disabled:hover{color:rgba(0, 0, 0, 0.25);}a.css-16cpc8y.ant-typography[disabled]:active,.css-16cpc8y.ant-typography a[disabled]:active,a.css-16cpc8y.ant-typography.ant-typography-disabled:active,.css-16cpc8y.ant-typography a.ant-typography-disabled:active{pointer-events:none;}.css-16cpc8y.ant-typography .ant-typography-expand,.css-16cpc8y.ant-typography .ant-typography-edit,.css-16cpc8y.ant-typography .ant-typography-copy{color:#416ed2;text-decoration:none;outline:none;cursor:pointer;transition:color 0.3s;margin-left:4px;}.css-16cpc8y.ant-typography .ant-typography-expand:focus,.css-16cpc8y.ant-typography .ant-typography-edit:focus,.css-16cpc8y.ant-typography .ant-typography-copy:focus,.css-16cpc8y.ant-typography .ant-typography-expand:hover,.css-16cpc8y.ant-typography .ant-typography-edit:hover,.css-16cpc8y.ant-typography .ant-typography-copy:hover{color:#305ab7;}.css-16cpc8y.ant-typography .ant-typography-expand:active,.css-16cpc8y.ant-typography .ant-typography-edit:active,.css-16cpc8y.ant-typography .ant-typography-copy:active{color:#305ab7;}.css-16cpc8y.ant-typography-edit-content{position:relative;}div.css-16cpc8y.ant-typography-edit-content{left:-12px;margin-top:-12px;margin-bottom:calc(1em - 12px);}.css-16cpc8y.ant-typography-edit-content .ant-typography-edit-content-confirm{position:absolute;right:10px;bottom:8px;color:rgb(57,57,57);font-weight:normal;font-size:14px;font-style:normal;pointer-events:none;}.css-16cpc8y.ant-typography-edit-content textarea{margin:0!important;-moz-transition:none;height:1em;}.css-16cpc8y.ant-typography .ant-typography-copy-success,.css-16cpc8y.ant-typography .ant-typography-copy-success:hover,.css-16cpc8y.ant-typography .ant-typography-copy-success:focus{color:#52c41a;}.css-16cpc8y.ant-typography .ant-typography-copy-icon-only{margin-left:0;}a.css-16cpc8y.ant-typography-ellipsis,span.css-16cpc8y.ant-typography-ellipsis{display:inline-block;max-width:100%;}.css-16cpc8y.ant-typography-single-line{white-space:nowrap;}.css-16cpc8y.ant-typography-ellipsis-single-line{overflow:hidden;text-overflow:ellipsis;}a.css-16cpc8y.ant-typography-ellipsis-single-line,span.css-16cpc8y.ant-typography-ellipsis-single-line{vertical-align:bottom;}.css-16cpc8y.ant-typography-ellipsis-single-line >code{padding-top:0;padding-bottom:0;max-width:calc(100% - 1.2em);display:inline-block;overflow:hidden;text-overflow:ellipsis;vertical-align:bottom;box-sizing:content-box;}.css-16cpc8y.ant-typography-ellipsis-multiple-line{display:-webkit-box;overflow:hidden;-webkit-line-clamp:3;-webkit-box-orient:vertical;}.css-16cpc8y.ant-typography-rtl{direction:rtl;}.css-16cpc8y.ant-app{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-app::before,.css-16cpc8y.ant-app::after{box-sizing:border-box;}.css-16cpc8y.ant-app [class^="ant-app"],.css-16cpc8y.ant-app [class*=" ant-app"]{box-sizing:border-box;}.css-16cpc8y.ant-app [class^="ant-app"]::before,.css-16cpc8y.ant-app [class*=" ant-app"]::before,.css-16cpc8y.ant-app [class^="ant-app"]::after,.css-16cpc8y.ant-app [class*=" ant-app"]::after{box-sizing:border-box;}.css-16cpc8y.ant-app{color:#1b1b1b;font-size:14px;line-height:1.5714285714285714;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';}.css-16cpc8y[class^="ant-image"],.css-16cpc8y[class*=" ant-image"]{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y[class^="ant-image"]::before,.css-16cpc8y[class*=" ant-image"]::before,.css-16cpc8y[class^="ant-image"]::after,.css-16cpc8y[class*=" ant-image"]::after{box-sizing:border-box;}.css-16cpc8y[class^="ant-image"] [class^="ant-image"],.css-16cpc8y[class*=" ant-image"] [class^="ant-image"],.css-16cpc8y[class^="ant-image"] [class*=" ant-image"],.css-16cpc8y[class*=" ant-image"] [class*=" ant-image"]{box-sizing:border-box;}.css-16cpc8y[class^="ant-image"] [class^="ant-image"]::before,.css-16cpc8y[class*=" ant-image"] [class^="ant-image"]::before,.css-16cpc8y[class^="ant-image"] [class*=" ant-image"]::before,.css-16cpc8y[class*=" ant-image"] [class*=" ant-image"]::before,.css-16cpc8y[class^="ant-image"] [class^="ant-image"]::after,.css-16cpc8y[class*=" ant-image"] [class^="ant-image"]::after,.css-16cpc8y[class^="ant-image"] [class*=" ant-image"]::after,.css-16cpc8y[class*=" ant-image"] [class*=" ant-image"]::after{box-sizing:border-box;}.css-16cpc8y.ant-image{position:relative;display:inline-block;}.css-16cpc8y.ant-image .ant-image-img{width:100%;height:auto;vertical-align:middle;}.css-16cpc8y.ant-image .ant-image-img-placeholder{background-color:rgba(0, 0, 0, 0.04);background-image:url('data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJNMTQuNSAyLjVoLTEzQS41LjUgMCAwIDAgMSAzdjEwYS41LjUgMCAwIDAgLjUuNWgxM2EuNS41IDAgMCAwIC41LS41VjNhLjUuNSAwIDAgMC0uNS0uNXpNNS4yODEgNC43NWExIDEgMCAwIDEgMCAyIDEgMSAwIDAgMSAwLTJ6bTguMDMgNi44M2EuMTI3LjEyNyAwIDAgMS0uMDgxLjAzSDIuNzY5YS4xMjUuMTI1IDAgMCAxLS4wOTYtLjIwN2wyLjY2MS0zLjE1NmEuMTI2LjEyNiAwIDAgMSAuMTc3LS4wMTZsLjAxNi4wMTZMNy4wOCAxMC4wOWwyLjQ3LTIuOTNhLjEyNi4xMjYgMCAwIDEgLjE3Ny0uMDE2bC4wMTUuMDE2IDMuNTg4IDQuMjQ0YS4xMjcuMTI3IDAgMCAxLS4wMi4xNzV6IiBmaWxsPSIjOEM4QzhDIiBmaWxsLXJ1bGU9Im5vbnplcm8iLz48L3N2Zz4=');background-repeat:no-repeat;background-position:center center;background-size:30%;}.css-16cpc8y.ant-image .ant-image-mask{position:absolute;top:0;right:0;bottom:0;left:0;display:flex;align-items:center;justify-content:center;color:#fff;background:rgba(0, 0, 0, 0.5);cursor:pointer;opacity:0;transition:opacity 0.3s;}.css-16cpc8y.ant-image .ant-image-mask .ant-image-mask-info{overflow:hidden;white-space:nowrap;text-overflow:ellipsis;padding:0 4px;}.css-16cpc8y.ant-image .ant-image-mask .ant-image-mask-info .anticon{margin-right:4px;}.css-16cpc8y.ant-image .ant-image-mask .ant-image-mask-info .anticon svg{vertical-align:baseline;}.css-16cpc8y.ant-image .ant-image-mask:hover{opacity:1;}.css-16cpc8y.ant-image .ant-image-placeholder{position:absolute;top:0;right:0;bottom:0;left:0;}.css-16cpc8y.ant-image-preview-root .ant-image-preview{height:100%;text-align:center;pointer-events:none;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-body{position:absolute;top:0;right:0;bottom:0;left:0;overflow:hidden;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-img{max-width:100%;max-height:70%;vertical-align:middle;transform:scale3d(1, 1, 1);cursor:grab;transition:transform 0.3s cubic-bezier(0.215, 0.61, 0.355, 1) 0s;user-select:none;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-img-wrapper{position:absolute;top:0;right:0;bottom:0;left:0;transition:transform 0.3s cubic-bezier(0.215, 0.61, 0.355, 1) 0s;display:flex;justify-content:center;align-items:center;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-img-wrapper>*{pointer-events:auto;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-img-wrapper::before{display:inline-block;width:1px;height:50%;margin-right:-1px;content:"";}.css-16cpc8y.ant-image-preview-root .ant-image-preview-moving .ant-image-preview-preview-img{cursor:grabbing;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-moving .ant-image-preview-preview-img-wrapper{transition-duration:0s;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-wrap{z-index:1080;}.css-16cpc8y.ant-image-preview-operations-wrapper{position:fixed;z-index:1081;}.css-16cpc8y .ant-image-preview-footer{position:fixed;bottom:32px;left:0;width:100%;display:flex;flex-direction:column;align-items:center;color:rgba(255, 255, 255, 0.65);}.css-16cpc8y .ant-image-preview-progress{margin-bottom:16px;}.css-16cpc8y .ant-image-preview-close{position:fixed;top:32px;right:32px;display:flex;color:#fff;background-color:rgba(0, 0, 0, 0.1);border-radius:50%;padding:12px;outline:0;border:0;cursor:pointer;transition:all 0.3s;}.css-16cpc8y .ant-image-preview-close:hover{background-color:rgba(0, 0, 0, 0.2);}.css-16cpc8y .ant-image-preview-close>.anticon{font-size:18px;}.css-16cpc8y .ant-image-preview-operations{display:flex;align-items:center;padding:0 24px;background-color:rgba(0, 0, 0, 0.1);border-radius:100px;}.css-16cpc8y .ant-image-preview-operations-operation{margin-left:12px;padding:12px;cursor:pointer;transition:all 0.3s;user-select:none;}.css-16cpc8y .ant-image-preview-operations-operation:not(.ant-image-preview-operations-operation-disabled):hover>.anticon{color:rgba(255, 255, 255, 0.85);}.css-16cpc8y .ant-image-preview-operations-operation-disabled{color:rgba(255, 255, 255, 0.25);cursor:not-allowed;}.css-16cpc8y .ant-image-preview-operations-operation:first-of-type{margin-left:0;}.css-16cpc8y .ant-image-preview-operations-operation>.anticon{font-size:18px;}.css-16cpc8y .ant-image-preview-switch-left,.css-16cpc8y .ant-image-preview-switch-right{position:fixed;top:50%;z-index:1081;display:flex;align-items:center;justify-content:center;width:40px;height:40px;margin-top:-20px;color:rgba(255, 255, 255, 0.65);background:rgba(0, 0, 0, 0.1);border-radius:50%;transform:translateY(-50%);cursor:pointer;transition:all 0.3s;user-select:none;}.css-16cpc8y .ant-image-preview-switch-left:hover,.css-16cpc8y .ant-image-preview-switch-right:hover{background:rgba(0, 0, 0, 0.2);}.css-16cpc8y .ant-image-preview-switch-left-disabled,.css-16cpc8y .ant-image-preview-switch-right-disabled,.css-16cpc8y .ant-image-preview-switch-left-disabled:hover,.css-16cpc8y .ant-image-preview-switch-right-disabled:hover{color:rgba(255, 255, 255, 0.25);background:transparent;cursor:not-allowed;}.css-16cpc8y .ant-image-preview-switch-left-disabled >.anticon,.css-16cpc8y .ant-image-preview-switch-right-disabled >.anticon,.css-16cpc8y .ant-image-preview-switch-left-disabled:hover >.anticon,.css-16cpc8y .ant-image-preview-switch-right-disabled:hover >.anticon{cursor:not-allowed;}.css-16cpc8y .ant-image-preview-switch-left >.anticon,.css-16cpc8y .ant-image-preview-switch-right >.anticon{font-size:18px;}.css-16cpc8y .ant-image-preview-switch-left{left:12px;}.css-16cpc8y .ant-image-preview-switch-right{right:12px;}.css-16cpc8y.ant-image-preview-root .ant-image-preview.ant-zoom-enter,.css-16cpc8y.ant-image-preview-root .ant-image-preview.ant-zoom-appear{transform:none;opacity:0;animation-duration:0.3s;user-select:none;}.css-16cpc8y.ant-image-preview-root .ant-image-preview.ant-zoom-leave .ant-image-preview-content{pointer-events:none;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-mask{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1000;height:100%;background-color:rgba(0, 0, 0, 0.45);pointer-events:none;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-mask .ant-image-preview-hidden{display:none;}.css-16cpc8y.ant-image-preview-root .ant-image-preview-wrap{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1000;overflow:auto;outline:0;-webkit-overflow-scrolling:touch;}.css-16cpc8y.ant-image-preview-root .ant-fade-enter,.css-16cpc8y.ant-image-preview-root .ant-fade-appear{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-image-preview-root .ant-fade-leave{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-image-preview-root .ant-fade-enter.ant-fade-enter-active,.css-16cpc8y.ant-image-preview-root .ant-fade-appear.ant-fade-appear-active{animation-name:css-16cpc8y-antFadeIn;animation-play-state:running;}.css-16cpc8y.ant-image-preview-root .ant-fade-leave.ant-fade-leave-active{animation-name:css-16cpc8y-antFadeOut;animation-play-state:running;pointer-events:none;}.css-16cpc8y.ant-image-preview-root .ant-fade-enter,.css-16cpc8y.ant-image-preview-root .ant-fade-appear{opacity:0;animation-timing-function:linear;}.css-16cpc8y.ant-image-preview-root .ant-fade-leave{animation-timing-function:linear;}.css-16cpc8y.ant-image-preview-root .ant-zoom-enter,.css-16cpc8y.ant-image-preview-root .ant-zoom-appear{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-image-preview-root .ant-zoom-leave{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-image-preview-root .ant-zoom-enter.ant-zoom-enter-active,.css-16cpc8y.ant-image-preview-root .ant-zoom-appear.ant-zoom-appear-active{animation-name:css-16cpc8y-antZoomIn;animation-play-state:running;}.css-16cpc8y.ant-image-preview-root .ant-zoom-leave.ant-zoom-leave-active{animation-name:css-16cpc8y-antZoomOut;animation-play-state:running;pointer-events:none;}.css-16cpc8y.ant-image-preview-root .ant-zoom-enter,.css-16cpc8y.ant-image-preview-root .ant-zoom-appear{transform:scale(0);opacity:0;animation-timing-function:cubic-bezier(0.08, 0.82, 0.17, 1);}.css-16cpc8y.ant-image-preview-root .ant-zoom-enter-prepare,.css-16cpc8y.ant-image-preview-root .ant-zoom-appear-prepare{transform:none;}.css-16cpc8y.ant-image-preview-root .ant-zoom-leave{animation-timing-function:cubic-bezier(0.78, 0.14, 0.15, 0.86);}.css-16cpc8y.ant-fade-enter,.css-16cpc8y.ant-fade-appear{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-fade-leave{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-fade-enter.ant-fade-enter-active,.css-16cpc8y.ant-fade-appear.ant-fade-appear-active{animation-name:css-16cpc8y-antFadeIn;animation-play-state:running;}.css-16cpc8y.ant-fade-leave.ant-fade-leave-active{animation-name:css-16cpc8y-antFadeOut;animation-play-state:running;pointer-events:none;}.css-16cpc8y.ant-fade-enter,.css-16cpc8y.ant-fade-appear{opacity:0;animation-timing-function:linear;}.css-16cpc8y.ant-fade-leave{animation-timing-function:linear;}.anticon{display:inline-flex;align-items:center;color:inherit;font-style:normal;line-height:0;text-align:center;text-transform:none;vertical-align:-0.125em;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.anticon >*{line-height:1;}.anticon svg{display:inline-block;}.anticon .anticon .anticon-icon{display:block;}.anticon{display:inline-flex;align-items:center;color:inherit;font-style:normal;line-height:0;text-align:center;text-transform:none;vertical-align:-0.125em;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.anticon >*{line-height:1;}.anticon svg{display:inline-block;}.anticon .anticon .anticon-icon{display:block;}:focus {
  outline: none;
}
:focus-visible {
  outline: none;
}
input::-ms-clear,
input::-ms-reveal {
  display: none;
}
*,
*::before,
*::after {
  box-sizing: border-box;
}
.container {
  margin: 0;
  text-align: left;
  font-weight: initial;
  font-family: sans-serif;
  -webkit-font-smoothing: auto;
  -moz-osx-font-smoothing: initial;
  line-height: 1.15;
  -webkit-text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -ms-overflow-style: scrollbar;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
  --font: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto,
    'Helvetica Neue', Arial, 'Noto Sans', sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  --cnfont:  -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Microsoft YaHei", "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", sans-serif;
  --size: 20px;
  --width: 800px;
  --space: 0;
  --lineheight: 1.8;
  --weight: 400;
  --blockspace: 20px;
  --indent: 0em;
  --titlealign: left;
  --align: left;
  --imageleft: auto;
  --imageright: auto;
  --imagehide: block;
  --margin: 80px;
  --padding: 80px;
  --offsetleft: 0px;
  --offsetright: 0px;
  --columncount: 2;
  --columngap: 60px;
  --columnwidth: 90%;
  --title1: 2em;
  --title1weight: 500;
  --title1color: #1b1b1b;
  --title2: 1.6em;
  --title2weight: 500;
  --title2color: #1b1b1b;
  --title3: 1.2em;
  --title3weight: 500;
  --title3color: #1b1b1b;
  --title4: 1em;
  --title4weight: 500;
  --title4color: #1b1b1b;
  --title5: 1em;
  --title5weight: 500;
  --title5color: #1b1b1b;
  --title6: 1em;
  --title6weight: 500;
  --title6color: #1b1b1b;
  --color: #1b1b1b;
  --link: #416ed2;
  --hover: #305ab7;
  --visited: #305ab7;
  --select: #1b1b1b;
  --selectbg: #bbd6fc;
  --bg-r: 237;
  --bg-g: 237;
  --bg-b: 237;
  --bg: #ffffff;
  --canvas: #ededed;
  --track-width: 8px;
  --track: #e2e2e2;
  --thumb: #9e9e9e;
  --radius: 4px;
}
.container .ant-app {
  color:  #1b1b1b;
  font-size: var(--size);
  font-family:  -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Microsoft YaHei", "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", sans-serif;
  font-weight: var(--weight);
  line-height: var(--lineheight);
  padding-left: var(--offsetleft);
  padding-right: var(--offsetright);
}
@-ms-viewport {
  width: device-width;
}
[tabindex='-1']:focus {
  outline: none;
}
hr {
  box-sizing: content-box;
  height: 0;
  overflow: visible;
}
h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 500;
}
p {
  margin-top: 0;
  margin-bottom: 1em;
}
abbr[title],
abbr[data-original-title] {
  -webkit-text-decoration: underline dotted;
  text-decoration: underline;
  text-decoration: underline dotted;
  border-bottom: 0;
  cursor: help;
}
address {
  margin-bottom: 1em;
  font-style: normal;
  line-height: inherit;
}
input[type='text'],
input[type='password'],
input[type='number'],
textarea {
  -webkit-appearance: none;
}
ol,
ul,
dl {
  margin-top: 0;
  margin-bottom: 1em;
}
ol ol,
ul ul,
ol ul,
ul ol {
  margin-bottom: 0;
}
dt {
  font-weight: 500;
}
dd {
  margin-bottom: 0.5em;
  margin-left: 0;
}
blockquote {
  margin: 0 0 1em;
}
dfn {
  font-style: italic;
}
b,
strong {
  font-weight: bolder;
}
small {
  font-size: 80%;
}
sub,
sup {
  position: relative;
  font-size: 75%;
  line-height: 0;
  vertical-align: baseline;
}
sub {
  bottom: -0.25em;
}
sup {
  top: -0.5em;
}
pre,
code,
kbd,
samp {
  font-size: 1em;
  font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
}
pre {
  margin-top: 0;
  margin-bottom: 1em;
  overflow: auto;
}
figure {
  margin: 0 0 1em;
}
img {
  vertical-align: middle;
  border-style: none;
}
a,
area,
button,
[role='button'],
input:not([type='range']),
label,
select,
summary,
textarea {
  touch-action: manipulation;
}
table {
  border-collapse: collapse;
}
caption {
  padding-top: 0.75em;
  padding-bottom: 0.3em;
  text-align: left;
  caption-side: bottom;
}
input,
button,
select,
optgroup,
textarea {
  margin: 0;
  color: inherit;
  font-size: inherit;
  font-family: inherit;
  line-height: inherit;
}
button,
input {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html [type='button'],
[type='reset'],
[type='submit'] {
  -webkit-appearance: button;
}
button::-moz-focus-inner,
[type='button']::-moz-focus-inner,
[type='reset']::-moz-focus-inner,
[type='submit']::-moz-focus-inner {
  padding: 0;
  border-style: none;
}
input[type='radio'],
input[type='checkbox'] {
  box-sizing: border-box;
  padding: 0;
}
input[type='date'],
input[type='time'],
input[type='datetime-local'],
input[type='month'] {
  -webkit-appearance: listbox;
}
textarea {
  overflow: auto;
  resize: vertical;
}
fieldset {
  min-width: 0;
  margin: 0;
  padding: 0;
  border: 0;
}
legend {
  display: block;
  width: 100%;
  max-width: 100%;
  margin-bottom: 0.5em;
  padding: 0;
  color: inherit;
  font-size: 1.5em;
  line-height: inherit;
  white-space: normal;
}
progress {
  vertical-align: baseline;
}
[type='number']::-webkit-inner-spin-button,
[type='number']::-webkit-outer-spin-button {
  height: auto;
}
[type='search'] {
  outline-offset: -2px;
  -webkit-appearance: none;
}
[type='search']::-webkit-search-cancel-button,
[type='search']::-webkit-search-decoration {
  -webkit-appearance: none;
}
::-webkit-file-upload-button {
  font: inherit;
  -webkit-appearance: button;
}
output {
  display: inline-block;
}
summary {
  display: list-item;
}
template {
  display: none;
}
[hidden] {
  display: none !important;
}
mark {
  padding: 0.2em;
}

.copyright {
  line-height: normal;
  display: flex;
  align-items: center;
  flex-direction: column;
  padding-bottom: 20px;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only {
  width: auto;
  height: auto;
  opacity: 0.2;
  padding: 7px 10px;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only svg {
  width: 40px;
  height: 40px;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only:hover {
  opacity: 1;
  background: transparent;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only.like:hover {
  color: #f10606;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only.unlike:hover {
  color: #e7c207;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only.coffee:hover {
  color: #ce6a07;
}
.copyright .ant-btn.ant-btn-sm {
  font-size: 12px;
  opacity: 0.4;
  padding-left: 6px;
  padding-right: 6px;
  height: auto;
}
.copyright .ant-btn.ant-btn-sm:hover {
  opacity: 1;
}
.copyright .ant-typography.ant-typography-secondary {
  font-size: 12px;
  opacity: 0.5;
  margin-top: 3px;
}

.container .ant-app {
  min-height: 100vh;
}
.container a {
  text-decoration: none;
}
.container code,
.container tt {
  border-radius: 3px;
  padding: 0 5px;
  margin: 0 5px;
}
.container pre {
  padding: 0.6em 0.8em;
  word-wrap: break-word;
  border-radius: 3px;
}
.container pre code {
  display: inline;
  margin: 0;
  padding: 0;
  background: 0 0;
  border: 0;
}
.container kbd {
  margin: 0 0.2em;
  padding: 0.15em 0.4em 0.1em;
  font-size: 90%;
  border-bottom-width: 2px;
  border-radius: 3px;
}
.container blockquote {
  padding: 0.1em 1em;
  opacity: 0.85;
}
.container blockquote > :first-child {
  margin-top: 0px;
}
.container blockquote > :last-child {
  margin-bottom: 0px;
}
.container blockquote p {
  margin: 0;
}
.container hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  padding: 0;
}
.container table {
  width: 100%;
  max-width: 100%;
  border-collapse: collapse;
}
.container table pre {
  white-space: pre-wrap;
}
.container figure {
  overflow: auto;
}
.container figure > img,
.container figure > table,
.container figure > pre {
  margin: 0 !important;
}
.container tr th,
.container tr td {
  padding: 6px 13px;
}
.container tr th p {
  margin: 0;
}
.container img {
  object-fit: contain;
}
.container img,
.container video {
  max-width: 100%;
  height: auto;
}
.container embed {
  max-width: 100%;
}
.container li div,
.container li p {
  margin: 0;
}
.container mark {
  padding: 0;
  margin: 0;
  color: initial;
  background-color: initial;
}
.container :focus {
  outline: none;
}
.container [role='separator']::after {
  content: '';
  display: block;
  clear: both;
  margin: 25px 0;
  height: 2px;
  overflow: hidden;
  border: none;
  opacity: 0.1;
  background:  #1b1b1b;
  padding: 0;
}
.container [role='separator']:last-child::after {
  display: none;
}
.container .line {
  display: block;
}
.container .left {
  float: left;
  margin-right: 12px;
}
.container .right {
  float: right;
  margin-left: 12px;
}
.container .both {
  clear: both;
}
.container .full-width {
  width: 100%;
  max-width: 100% !important;
}
.container .noise {
  display: none !important;
}
.container .block {
  display: block;
}
.container .inline-block {
  display: inline-block;
}
.container .flex {
  display: flex;
  flex-wrap: wrap;
}
.container .inline-flex {
  display: inline-flex;
  flex-wrap: wrap;
}
.container .zone-width {
  min-width: 0;
}

.container {
  color:  #1b1b1b;
  background:  #ffffff;
}
.container code,
.container tt {
  border: 1px solid rgba( 237,  237,  237, 0.6);
  background: rgba( 237,  237,  237, 0.4);
}
.container pre {
  background: rgba( 237,  237,  237, 0.5);
}
.container kbd {
  border: 1px solid rgba( 237,  237,  237, 1);
}
.container blockquote {
  border-left: 4px solid rgba( 237,  237,  237, 1);
}
.container hr {
  border-bottom: 1px solid rgba( 237,  237,  237, 1);
}
.container table th:hover,
.container table tr:hover {
  background: rgba( 237,  237,  237, 0.6);
}
.container table tr {
  border-bottom: 1px solid rgba( 237,  237,  237, 0.6);
}
.container table thead tr:first-child,
.container table tbody tr:first-child {
  background: rgba( 237,  237,  237, 0.4);
}
.container table thead + tbody tr:first-child {
  background: transparent;
}
.container table tr td {
  border-inline-end: 1px solid  rgb(237, 237, 237);
}
.container h1,
.container h2,
.container h3,
.container h4,
.container h5,
.container h6 {
  color:  #1b1b1b;
}
.container a {
  color:  #416ed2;
}
.container a:hover {
  color:  #305ab7;
}
.container a:active {
  color:  #305ab7;
}
.container a:visited {
  color:  #305ab7;
}
.container ::selection {
  color:  #1b1b1b;
  background-color:  #bbd6fc;
}
.container .title a {
  color:  #1b1b1b;
}
.container .title a:hover {
  color:  #416ed2;
}
.container .footer {
  color:  #1b1b1b;
}
.container.solid {
  background:  rgb(237, 237, 237);
}

.container {
  padding: 1px;
  text-shadow: none;
  font-variant: tabular-nums;
  font-feature-settings: 'tnum';
}
.container h1 {
  font-size: var(--title1);
  color:  #1b1b1b;
  font-weight: var(--title1weight);
}
.container h2 {
  font-size: var(--title2);
  color:  #1b1b1b;
  font-weight: var(--title2weight);
}
.container h3 {
  font-size: var(--title3);
  color:  #1b1b1b;
  font-weight: var(--title3weight);
}
.container h4 {
  font-size: var(--title4);
  color:  #1b1b1b;
  font-weight: var(--title4weight);
}
.container h5 {
  font-size: var(--title5);
  color:  #1b1b1b;
  font-weight: var(--title5weight);
}
.container h6 {
  font-size: var(--title6);
  color:  #1b1b1b;
  font-weight: var(--title6weight);
}
.container h1,
.container h2,
.container h3,
.container h4,
.container h5,
.container h6 {
  text-indent: 0;
}
.container a.link {
  margin: 0 6px;
}
.container a.link-with-img {
  display: inline-block;
}
.container a.link-with-img img {
  margin: 0 !important;
}
.container ol,
.container ul {
  text-indent: 0;
}
.container pre {
  white-space: pre-wrap;
}
.container pre,
.container code,
.container blockquote {
  text-indent: 0;
  text-align: left;
}
.container p,
.container blockquote,
.container ul,
.container ol,
.container dl,
.container table,
.container pre,
.container section,
.container figcaption,
.container li,
.container .block {
  margin: var(--blockspace) 0;
}
.container table {
  width: var(--width);
  max-width: var(--width);
  border-top: 1px solid  rgb(237, 237, 237);
  border-bottom: 1px solid  rgb(237, 237, 237);
}
.container .flex p,
.container .inline-flex p,
.container .flex blockquote,
.container .inline-flex blockquote,
.container .flex ul,
.container .inline-flex ul,
.container .flex ol,
.container .inline-flex ol,
.container .flex dl,
.container .inline-flex dl,
.container .flex table,
.container .inline-flex table,
.container .flex pre,
.container .inline-flex pre,
.container .flex section,
.container .inline-flex section,
.container .flex figcaption,
.container .inline-flex figcaption,
.container .flex .block,
.container .inline-flex .block {
  margin: 0;
}
.container .flex > *,
.container .inline-flex > * {
  margin-left: 10px !important;
  margin-right: 10px !important;
}
.container .flex > *:first-child,
.container .inline-flex > *:first-child {
  margin-left: 0 !important;
}
.container .flex > *:last-child,
.container .inline-flex > *:last-child {
  margin-right: 0 !important;
}
.container .pages {
  min-height: 610px;
  max-width: var(--width);
  margin:  80px auto;
}
.container .pages svg {
  width: 20px;
  height: 20px;
}
.container .pages .MathJax_SVG svg {
  width: auto;
  height: auto;
}
.container .page {
  padding-bottom: 20px;
  margin-bottom: 20px;
  word-break: break-word;
  text-indent: var(--indent);
  letter-spacing: var(--space);
  text-align: var(--align);
}
.container .page > *:first-child {
  margin-top: 0;
}
.container .page > *:last-child {
  margin-bottom: 0;
}
.container .page:last-child {
  margin-bottom: 0;
  border-bottom-width: 0;
}
.container .page:before,
.container .page:after {
  content: '';
  display: block;
  clear: both;
  height: 0px;
}
.container .page img {
  display: var(--imagehide);
}
.container .page img.large {
  margin-top: var(--blockspace) !important;
  margin-bottom: var(--blockspace) !important;
  margin-left: var(--imageleft) !important;
  margin-right: var(--imageright) !important;
}
.container .page img.tiny,
.container .page img.base64 {
  width: auto;
}
.container .page figure {
  overflow: auto;
}
.container .page figure > img.large,
.container .page figure > table,
.container .page figure > pre {
  margin-top: 0 !important;
  margin-bottom: 0 !important;
}
.container .page figure > figcaption {
  text-align: center;
  font-size: 90%;
  opacity: 0.8;
  margin-top: 6px;
}
.container .page p > img.large,
.container .page a img {
  margin-top: 0 !important;
  margin-bottom: 0 !important;
}
.container .page .title {
  text-indent: 0;
  margin-bottom: 0;
  text-align: var(--titlealign);
}
.container .page .meta {
  font-size: 80%;
  text-indent: 0;
  text-align: var(--titlealign);
  margin: 0 0 var(--blockspace);
}
.container .page .meta > span {
  opacity: 0.7;
  position: relative;
  margin-right: 24px;
}
.container .page .meta > span.ant-avatar {
  margin-right: 2px;
}
.container .page .meta > span.ant-avatar img {
  display: block;
}
.container .page .meta > span::after {
  content: '';
  position: absolute;
  right: -12px;
  top: 47%;
  display: block;
  width: 3px;
  height: 3px;
  background:  #1b1b1b;
  border-radius: 50%;
}
.container .page .meta > span:last-child::after {
  display: none;
}
.container .page .meta.avatar .ant-avatar {
  float: left;
  opacity: 1;
  width: 46px;
  height: 46px;
  margin-right: 16px;
  background:  #ffffff;
  border-radius: 6px;
  overflow: hidden;
}
.container .page .meta.avatar .ant-avatar img {
  display: block;
}
.container .page .meta.avatar .author {
  opacity: 1;
  line-height: 1.3;
  display: block;
}
.container .page .meta.avatar .author::after {
  display: none;
}
.container .page .meta::after {
  content: '';
  display: block;
  clear: both;
}
.container .page .excerpt {
  text-indent: 0;
  border-color:  #416ed2;
  background:  rgb(237, 237, 237);
}
.container .page .excerpt p {
  font-size: 90%;
}
.container .page .cover {
  text-indent: 0;
  margin-bottom: var(--blockspace);
}
.container .page .cover .ant-image {
  width: 100%;
  background: #f5f5f5;
}
.container .page .cover img {
  margin: 0 !important;
  width: 100%;
}
.container .page .footer {
  text-indent: 0;
  margin: calc(var(--blockspace) * 2) 0 0;
}
.container .page .footer .ant-tag {
  cursor: pointer;
  min-height: 27px;
  border-radius: 15px;
  background:  #ffffff;
  padding: 3px 12px 3px 3px;
  margin: 0 5px 4px 0;
  display: inline-block;
}
.container .page .footer .ant-tag::before {
  content: '#';
  margin-right: 4px;
  background:  rgb(237, 237, 237);
  width: 20px;
  height: 20px;
  text-align: center;
  font-size: 12px;
  color:  #416ed2;
  line-height: 20px;
  border-radius: 50px;
  display: inline-block;
}
.container .page .footer .ant-tag:hover {
  color:  #1b1b1b;
  background-color:  #bbd6fc;
}
.container .page .page-empty.ant-result {
  padding: 180px 0;
}
.container .page .page-empty.ant-result .ant-result-icon svg {
  width: auto;
  height: auto;
}
.container .page.zh {
  font-family:  -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Microsoft YaHei", "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", sans-serif;
}
.container .tex,
.container .katex-display {
  text-align: center;
  margin: 50px 0;
}
.container .sub-page::before {
  content: '';
  display: block;
  height: var(--blockspace);
  border-top: 1px solid  #1b1b1b;
  opacity: 0.1;
  margin-left: calc(-1 *  80px);
  margin-right: calc(-1 *  80px);
}
.container .sub-page .sub-title {
  margin-bottom: -2px;
}
@media screen and (max-width: 600px) {
  .container .pages {
    margin: 20px 16px;
  }
}

.container.paper {
  background:  rgb(237, 237, 237);
}
.container.paper table {
  margin: var(--blockspace) calc( 80px * -1);
}
.container.paper .page {
  border-radius: 6px;
  background:  #ffffff;
  padding:  80px;
  box-shadow: 0 0 6px rgba(0, 0, 0, 0.15);
}
.container.paper .page .cover {
  margin: var(--blockspace) calc( 80px * -1);
}
.container.paper .page-block {
  padding-left:  80px;
  padding-right:  80px;
}
.container.paper .page-block .cover {
  margin: 0;
  max-width: 100%;
}
@media screen and (max-width: 600px) {
  .container.paper .page {
    padding: 20px;
  }
  .container.paper .page .cover {
    max-width: calc(100% + 40px);
    margin: -10px -20px 0;
  }
}

@media print {
  .container {
    padding-right: 0 !important;
  }
  .container .pages {
    margin: 0;
    max-width: 100%;
  }
  .container .pages pre,
  .container .pages img,
  .container .pages table {
    break-inside: avoid;
    page-break-inside: avoid;
  }
  .container .page-block {
    column-count: auto;
    margin-bottom: 0;
    padding-bottom: 0;
    padding-top: 0;
  }
  .container.paper .page {
    box-shadow: none;
  }
}

.container.shot.paper .page {
  border-radius: 0;
  box-shadow: none;
}

@-webkit-keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }

  100% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }
}

@keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }

  100% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }
}

@-webkit-keyframes fadeOut {
  0% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }

  100% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }
}

@keyframes fadeOut {
  0% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }

  100% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }
}

.container.in .pages {
  -webkit-animation-name: fadeIn;
  animation-name: fadeIn;
  -webkit-animation-duration: .3s;
  animation-duration: .3s;
  -webkit-animation-fill-mode: both;
  animation-fill-mode: both;
}

.container.out .pages {
  -webkit-animation-name: fadeOut;
  animation-name: fadeOut;
  -webkit-animation-duration: .2s;
  animation-duration: .2s;
  -webkit-animation-fill-mode: both;
  animation-fill-mode: both;
}

    .container {
      --codefont: inherit;
      --codesize: 20px;
      --codelineheight: 1;
      --codecomment: #697070;
      --codetag: #444a;
      --codeattr: #444a;
      --codenumber: #800;
      --codeblock: #800;
      --codevariable: #ab5656;
      --codeliteral: #695;
      --code: #397300;
      --codemeta: #1f7199;
      --codestring: #38a;
    }
    .hljs {
      border-radius: 3px;
      font-family: var(--codefont);
      font-size: var(--codesize);
      line-height: var(--codelineheight);
    }
    .hljs.large {
      position: fixed;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      padding: 50px;
      background:  #ffffff;
      z-index: 600;
      margin: 0;
      word-break: break-word;
      overflow: auto;
      white-space: pre-wrap;
    }
    
    .hljs-comment {
      color: var(--codecomment);
    }
    .hljs-punctuation,
    .hljs-tag {
      color: var(--codetag);
    }
    .hljs-tag .hljs-attr,
    .hljs-tag .hljs-name {
      color: var(--codeattr);
    }
    .hljs-attribute,
    .hljs-doctag,
    .hljs-keyword,
    .hljs-meta .hljs-keyword,
    .hljs-name,
    .hljs-selector-tag {
      font-weight: 700;
    }
    .hljs-deletion,
    .hljs-number,
    .hljs-quote,
    .hljs-selector-class,
    .hljs-selector-id,
    .hljs-string,
    .hljs-template-tag,
    .hljs-type {
      color: var(--codenumber);
    }
    .hljs-section,
    .hljs-title {
      color: var(--codeblock);
      font-weight: 700;
    }
    .hljs-link,
    .hljs-operator,
    .hljs-regexp,
    .hljs-selector-attr,
    .hljs-selector-pseudo,
    .hljs-symbol,
    .hljs-template-variable,
    .hljs-variable {
      color: var(--codevariable);
    }
    .hljs-literal {
      color: var(--codeliteral);
    }
    .hljs-addition,
    .hljs-built_in,
    .hljs-bullet,
    .hljs-code {
      color: var(--code);
    }
    .hljs-meta {
      color: var(--codemeta);
    }
    .hljs-meta .hljs-string {
      color: var(--codestring);
    }
    .hljs-emphasis {
      font-style: italic;
    }
    .hljs-strong {
      font-weight: 700;
    }
    img[action="zoom-in"] {cursor: zoom-in;}</style><title>（徒手搓LLM）逐行代码从0构造一个LLM——LlaMa篇 - Circle 阅读助手</title></head><body><div class=" container paper in"><div class="ant-app"><div class="pages"><div class="page"><h1 class="title" id="outline_0"><a href="https://zhuanlan.zhihu.com/p/1674261485" target="_blank" rel="noreferrer">（徒手搓LLM）逐行代码从0构造一个LLM——LlaMa篇</a></h1><p class="meta avatar"><span class="ant-avatar ant-avatar-circle ant-avatar-image css-16cpc8y"><img src="https://picx.zhimg.com/v2-b402103ff7c9c37dbcf5730bcd348a72_l.jpg?source=172ae18b 2x" class="tiny"></span><span class="author">mc112611</span><span>2025-02-19 18:50</span><span>共 35294 字</span><span>阅读需 141 分钟</span></p><div class="cover"><div srcset="" class="ant-image css-16cpc8y"><img class="ant-image-img css-16cpc8y large" src="https://picx.zhimg.com/70/v2-e451ab0f65277a6c13e7f6f2b405689f_1440w.image?source=172ae18b&amp;biz_tag=Post"></div></div><div class="sub-page"><h2 class="sub-title" id="outline_1">（徒手搓LLM）逐行代码从0构造一个LLM——LlaMa篇(1)</h2><p class="meta avatar"><span class="ant-avatar ant-avatar-circle ant-avatar-image css-16cpc8y"><img src="https://picx.zhimg.com/v2-b402103ff7c9c37dbcf5730bcd348a72_l.jpg?source=172ae18b 2x" class="tiny"></span><span class="author">mc112611</span><span>2025-02-19 18:50</span></p><p></p><h2 id="outline_2">本篇为：</h2><ul><li>面向人群：觉得LLM很多复杂的结构和层级，懂很多原理，但是不知道怎么结合到一起</li><li>本篇会很长，但是应该不会又臭又长</li><li>本篇可能像当头一棒，但是有可能：力度刚刚好，懵逼不伤脑。</li><li>逐行拆解LlaMa大模型的所有算子，架构，包括<span><a href="https://zhida.zhihu.com/search?content_id=249357106&amp;content_type=Article&amp;match_order=1&amp;q=RMSNorm&amp;zhida_source=entity" target="_blank" class="link">RMSNorm</a></span>，ROPE，SwiGLU实现</li><li>本篇<b>未</b>采用huggingface的库，全程pytorch实现，没有任何预训练模型</li><li>起始点为一本<span><a href="https://zhida.zhihu.com/search?content_id=249357106&amp;content_type=Article&amp;match_order=1&amp;q=%E3%80%8A%E8%A5%BF%E6%B8%B8%E8%AE%B0%E3%80%8B&amp;zhida_source=entity" target="_blank" class="link">《西游记》</a></span>原文，终点为你自己练的大模型</li><li>准备好pytorch，即使没有显卡也没关系，主要是LLM原理的学习，而不是看完这个文章就可以造个新的大模型架构出来。</li><li>本篇会竭尽所能，全程用大白话去拆分原理。</li></ul><h2 id="outline_3">引言</h2><p>本文全部代码已分享至google_colab，有魔法的可以自行查看，代码逐行注释，不想看文章的，可直接去colab上跑一下，不需要GPU资源，直接最低配弄个CPU，可以运行（2024.10.17亲测）：</p><p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//colab.research.google.com/drive/1dqL8UN1UPNuEPCJzdNPbmin4vEveadgE%3Fusp%3Dsharing" class="link"></a></p><p>如果有魔法的同学，打开上面的连接，看到的代码应该是本厮逐行注释过的，类似这样：</p><p><img src="https://pic3.zhimg.com/v2-a5f81fd51fbe23dbc68c4c53444120b6_r.jpg" class="large"></p><figcaption>示例</figcaption><p>如果有兴趣继续的，那么请扶稳坐好，徒手搓个大模型系列--第一篇LlaMa，准备发车。</p><h2 id="outline_4">构造一个简单的文本生成模型</h2><p>在构造LlaMa之前，我们先构造一个简单的<span><a href="https://zhida.zhihu.com/search?content_id=249357106&amp;content_type=Article&amp;match_order=1&amp;q=seq2seq%E6%A8%A1%E5%9E%8B&amp;zhida_source=entity" target="_blank" class="link">seq2seq模型</a></span>，然后逐步对原本的Seq2seq模型，增加LlaMa中的算子RMS、Rope、SwiGLU，直到完整构造LlaMa。</p><p>首先是一些功能函数的实现，虽然没什么难的，但是最好还是 过一遍，因为脑海里有数据的形状，在模型搭建的时候，知道输入进去的是什么样子的，对于理解深度神经网络有很大帮助。</p><p>导包</p><div><pre><code><span style="font-weight: 600;">import</span> <span>torch</span>
<span style="font-weight: 600;">from</span> <span>torch</span> <span style="font-weight: 600;">import</span> <span>nn</span>
<span style="font-weight: 600;">from</span> <span>torch.nn</span> <span style="font-weight: 600;">import</span> <span>functional</span> <span style="font-weight: 600;">as</span> <span>F</span>
<span style="font-weight: 600;">import</span> <span>numpy</span> <span style="font-weight: 600;">as</span> <span>np</span>
<span style="font-weight: 600;">from</span> <span>matplotlib</span> <span style="font-weight: 600;">import</span> <span>pyplot</span> <span style="font-weight: 600;">as</span> <span>plt</span>
<span style="font-weight: 600;">import</span> <span>time</span>
<span style="font-weight: 600;">import</span> <span>pandas</span> <span style="font-weight: 600;">as</span> <span>pd</span>
<span style="font-weight: 600;">import</span> <span>urllib.request</span></code></pre></div><p>以上是我们所需要用到的全部的库</p><p>接下来，创建一个字典用于存储config</p><div><pre><code>MASTER_CONFIG = {
    # 参数放这里
}</code></pre></div><p>下载一个吴承恩版本的西游记原文数据集</p><div><pre><code>url = "https://raw.githubusercontent.com/mc112611/PI-ka-pi/main/xiyouji.txt"
file_name = "xiyouji.txt"
urllib.request.urlretrieve(url, file_name)</code></pre></div><p>读数据</p><div><pre><code>lines = open("xiyouji.txt", 'r').read()

# 创建简易版词表（字符级）
vocab = sorted(list(set(lines)))

# 查看词表前n个字符
head_num=50
print('词表前{}个:'.format(head_num), vocab[:head_num])

print('词表大小:', len(vocab))

# 输出：
# 词表前50个: ['\n', ' ', '!', '"', '#', '*', ',', '.', '—', '‘', '’', '“', '”', '□', '、', '。', '《', '》', '一', '丁', '七', '万', '丈', '三', '上', '下', '不', '与', '丑', '专', '且', '丕', '世', '丘', '丙', '业', '丛', '东', '丝', '丞', '丢', '两', '严', '丧', '个', '丫', '中', '丰', '串', '临']
# 词表大小: 4325</code></pre></div><p>将词表编码成为数字，普通的整数，如："1":"孙"，"2":"悟" ，"3":"空"，下面那个是键和值对调</p><div><pre><code>itos = {i: ch for i, ch in enumerate(vocab)}

# 双向映射
stoi = {ch: i for i, ch in enumerate(vocab)}</code></pre></div><p>接下来我们创建一个简易的编码器和解码器。用于输入进模型之前，将文本转换为数字，输出之前将数字转换为文本：</p><div><pre><code># 编码器（青春版）
def encode(s):
    return [stoi[ch] for ch in s]

# 解码器（青春版）
def decode(l):
    return ''.join([itos[i] for i in l])

# 来试一下这个“高端”的编解码器
decode(encode("悟空"))
encode("悟空")

# 输出：
# [1318, 2691]</code></pre></div><p>输出的两个数字代表着“悟”字和“空”字，在vocab词表中的编码，映射表的原理</p><p>因为是字符级的编码，不是BPE，或者其他格式的词表构建。  此部分，可以替换为<span><a href="https://zhida.zhihu.com/search?content_id=249357106&amp;content_type=Article&amp;match_order=1&amp;q=tiktoken&amp;zhida_source=entity" target="_blank" class="link">tiktoken</a></span>或者其他的映射器，本文主要以理解原理为主，使用的方式越简单越容易理解。</p><div><pre><code># 对全文进行编码，并映射成为tensor
dataset = torch.tensor(encode(lines), dtype=torch.int16)

# 看一下形状，实际上就是多少个字符，一共65万个字符
print(dataset.shape)
print(dataset)

# 输出：
# torch.Size([658298])
# tensor([   0, 4319, 1694,  ...,   12,    0,    0], dtype=torch.int16)</code></pre></div><p>上面这个代码块，我们将整本《西游记》全部编码，转换成tensor。本文一共有65万多个字符，转换成一维的张量。</p><p>接下来构建batch</p><div><pre><code># 构建batch
def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):
    # 切分训练集，验证集，测试集，比例为，训练80%，验证10%，测试10%
    train = data[:int(0.8 * len(data))]
    val = data[int(0.8 * len(data)): int(0.9 * len(data))]
    test = data[int(0.9 * len(data)):]

    # 将全部的训练数据作为batch，验证集，测试集也换个变量存储（单纯为了方便看）
    batch_data = train
    if split == 'val':
        batch_data = val
    if split == 'test':
        batch_data = test

    # 这里需要学习torch.randint，生成大小为batch_size，内部数值为随机整数的tensor。生成随机数数值域为[0,训练集字符数量-滑动窗口大小-1]之间的整数
    # 详情可以参考官方文档，或者这个博客：https://blog.csdn.net/qq_41813454/article/details/136326473
    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))
    # print('ix输出:')
    # print(ix)


    # 这里需要学习torch.stack，执行操作类似于python的zip关键字，只不过操作对象是tensor张量，指定任意维度的张量进行组合
    # 详情参考官方文档，或者这个博客：https://blog.csdn.net/dongjinkun/article/details/132590205

    # 这里x作为特征，y作为预测值，因为文本生成任务是根据前n个字符，去推理后面的1个字符，因此y的构造会使窗口在保持原大小的基础上向后移一位
    # 通过滑动窗口，对batch_data中的训练数据，进行随机取样，相当于随机选择训练数据。
    # 在原65万多个字符中，随机选取一个字符作为开始，并以这个开始点，向后选取滑动窗口个数的字符，作为训练数据，向后移一位就是其目标值。  因此ix的构造不能超出index。
    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()
    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()

    # 返回特征值，目标值
    return x, y</code></pre></div><p><code>torch.randint</code>以及 <code>torch.stack</code>  需要学习一下，这种的文章有很多。</p><p>重点可以放在这两行代码上：    </p><p><code>x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()</code> </p><p><code>y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long() </code> </p><p>以滑动窗口的形式对数据进行获取特征值与目标值，因为训练的时候，是根据前N个字符去预测后面的1个字符，因此，目标值y的滑动窗口在保持原本大小（长度）的前提下，向右移动一位。</p><div><pre><code># 根据上面构造的get_batchs()函数，更新参数字典。
MASTER_CONFIG.update({
    'batch_size': 8,          # 不解释
    'context_window': 16,      # 滑动窗口采样，设置采样大小
    'vocab_size':4325         # 咱们的西游记数据集，一共包含4325个不重复的汉字，标点符号
})</code></pre></div><p>构造一个字典，用于存储config参数，滑动窗口的值16，代表着采样时候将每条文本，分成多个长度为16的数据。vocab_size，上面有，代表着《西游记》原文所有字符去重后字符数量，也就是词表的大小。</p><p><b>为了方便理解，我们每构造一个函数，或者类，就单独运行一下，看看效果。 如果堆了很多代码，即使执行，看到最终结果，应该也是懵的。</b></p><p>所以我们执行刚刚的get_ batches函数，看一下结果，为了方便观察，使用解码器（青春版）解一下码。</p><div><pre><code># 获取训练数据
xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])

# 因为是随机生成的采样，我们可以看一下数据，其中每个采样数据，来自于原文随机的起始点，每个元组为一个（x,y），可以观察每个x和y的首位去直观感受一下滑动窗口执行的操作
decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]

print(decoded_samples)

# 输出：
# [('姿娇且嫩’ ！”那女子笑\n而悄答', '娇且嫩’ ！”那女子笑\n而悄答道'), ('泼猢狲，打杀我也！”沙僧、八戒问', '猢狲，打杀我也！”沙僧、八戒问道'), ('人家，不惯骑马。”唐僧叫八戒驮着', '家，不惯骑马。”唐僧叫八戒驮着，'), ('著一幅“圯桥进履”的\n画儿。行者', '一幅“圯桥进履”的\n画儿。行者道'), ('从何来？这匹马，他在此久住，必知', '何来？这匹马，他在此久住，必知水'), ('声去，唿哨一声，寂然不见。那一国', '去，唿哨一声，寂然不见。那一国君'), ('刀轮剑砍怎伤怀！\n火烧雷打只如此', '轮剑砍怎伤怀！\n火烧雷打只如此，'), ('鲜。紫竹几竿鹦鹉歇，青松数簇鹧鸪', '。紫竹几竿鹦鹉歇，青松数簇鹧鸪\n')]</code></pre></div><p>输出结果结果是列表，列表中的元素为多个元组，每个元组是特征数据与目标值。可以观察每个元组数据的开始和结束，来看滑动窗口的效果。</p><p>接下来构造一个评估函数</p><div><pre><code># 构造一个评估函数
@torch.no_grad()
def evaluate_loss(model, config=MASTER_CONFIG):
    # 评估结果存储变量
    out = {}

    # 将模型置为评估模式
    model.eval()

    # 分别会在训练集和验证集里通过get_batchs()函数取评估数据
    for split in ["train", "val"]:

        losses = []

        # 评估10个batch
        for _ in range(10):
            # 拿到特征值（输入数据），以及目标值（输出数据）
            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'])

            # 把拿到的数据丢进模型，得到loss值
            _, loss = model(xb, yb)

            # 更新loss存储
            losses.append(loss.item())

        # 这里就是大家经常在控制台看到的 "train_loss"  "valid_loss"由来
        out[split] = np.mean(losses)

    # 评估完了，别忘了把模型再置回训练状态，下一个epoch还要继续训练呢
    model.train()

    return out</code></pre></div><p>到这里应该没有令人“眼前一黑”的操作。</p><p>这里加一个分割线，没什么特别的作用，就是觉得，如果上面的内容过了一遍，在jupyter或者colab跑了一下，没啥问题，那么可以继续。</p><p>说一些题外的，如果想抱着，代码拿来就能用，这篇文章可能没那么神（毕竟一个西游记原文，65万字的数据，还没有预训练模型，能练出个什么惊天大模型出来）。如果是想学习LLM，那么再提醒一次，请认真逐行看一下代码，否则会懵逼且伤脑。</p><hr><p>在进行分析LlaMa架构分析之前，我们从最简单的文本生成模型开始创建，然后在最简单的文本生成模型的基础上，把LlaMa的RSM，Rope等一点点添加进去。为此我们先：</p><ul><li>创建一个有毛病的模型架构</li><li>分析一下这个架构（其实也没什么分析的）</li></ul><div><pre><code>class StupidModel(nn.Module):
    def __init__(self, config=MASTER_CONFIG):
        super().__init__()
        self.config = config

        # embedding层，输入：词表大小，输出：维度大小
        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])

        # 创建线性层用于捕捉特征关系
        # 下面突击检查：这玩意是不是隐藏层！线性层堆叠越多是不是越好！堆叠越多是不是更计算开销越大！
        # LlaMa使用的激活函数是SwiGLU，目前在这个斯丢匹德模型架构里面先用Relu
        self.linear = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            nn.ReLU(),
            nn.Linear(config['d_model'], config['vocab_size']),
        )

        # 这个命令可以背一下，或者复制粘贴到自己的学习笔记。 因为这行命令会直接帮你查看模型的参数量。
        # 否则要么自己手算，要么就是听别人讲某某模型 7B  20B  108B   有了这个命令，你就能直接查看你创建的模型参数量多少
        print("模型参数量：", sum([m.numel() for m in self.parameters()]))</code></pre></div><p>我想，看注释就可以了吧,我们创建了一个有些stupid的模型，结构很简单：嵌入，线性变换，激活函数。唯一比较有意思的是，记住下面这个命令，以后可以用来查参数量。</p><p><code>print("模型参数量：", sum([m.numel() for m in self.parameters()]))</code> </p><p>接下来，我们对上面那个斯丢匹得模型增加前向传播函数，也就是forward，换个名字，叫：“简单的破模型”吧。 实际上broken代表着有问题。至于问题，马上解答。</p><div><pre><code>class SimpleBrokenModel(nn.Module):
    # init里的跟上面一样，没变化
    def __init__(self, config=MASTER_CONFIG):
      super().__init__()
      self.config = config
      self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])
      self.linear = nn.Sequential(
          nn.Linear(config['d_model'], config['d_model']),
          nn.ReLU(),
          nn.Linear(config['d_model'], config['vocab_size']),
      )



      # 添加前向传播函数
    def forward(self, idx, targets=None):
        # 实例化embedding层，输入映射为id的数据，输出嵌入后的数据
        x = self.embedding(idx)

        # 线性层承接embedding层输出的数据
        a = self.linear(x)

        # 对线性层输出的数据在最后一个维度，做softmax，得到概率分布
        logits = F.softmax(a, dim=-1)

        # 如果有目标值（也就是我们前面的y），则计算通过交叉熵损失计算loss结果。给输出的概率矩阵变个形状，再给目标值变个形状。  统一一下输入输出，然后计算loss。其中最后一维代表着一条数据。
        # 此处需要了解tensor.view()函数，带上几何空间想象力去想一下矩阵的形状。
        if targets is not None:

            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss

        # 如果没有目标值，则只返回概率分布的结果
        else:
            return logits

        # 查看参数量
        print("模型参数量：", sum([m.numel() for m in self.parameters()]))</code></pre></div><p>注释里不只有训练阶段，同时也有推理（评估）阶段，条件判断目标值是否存在，如果不存在，则只输出模型输出的结果，如果目标值存在，则还要返还loss值。</p><p><b>OS：注释之前加完了，感觉本篇文章甚至没啥写的了</b></p><div><pre><code># 这里我们设置这个模型为128维的embedding
MASTER_CONFIG.update({
    'd_model': 128,
})

# 实例化模型，传参
model = SimpleBrokenModel(MASTER_CONFIG)

# 再看看参数量
print("咱们的模型这么多参数量:", sum([m.numel() for m in model.parameters()]))
# 于是乎，我们创建了一个1128307个参数的模型，上面参数想怎么改，自己改！电脑不会爆炸！</code></pre></div><p>我们设置嵌入维度为128，LlaMa的嵌入维度是4096，我们弄个小一点的，毕竟在CPU上训练。得到一个112万参数量的模型。</p><p>下面，查看一下没训练之前，模型输出的结果，以及loss，模型输出的结果也就是数字，看loss更直观一些</p><div><pre><code># 获取训练的特征数据与目标数据
xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])

# 扔进模型获取概率分布矩阵与loss
logits, loss = model(xs, ys)
loss

# 输出：
# tensor(8.3722, grad_fn=&lt;NllLossBackward0&gt;)</code></pre></div><p>接下来，更新一下config超参数，实例化模型，以及设置优化器：</p><div><pre><code># 更新参数，训练伦次，batch_size，log日志打印步长
MASTER_CONFIG.update({
    'epochs': 1000,
    'log_interval': 10,      # 每10个batch打印一次log
    'batch_size': 32,
})

# 实例化模型
model = SimpleBrokenModel(MASTER_CONFIG)

# 创建一个Adam优化器，基础知识，
optimizer = torch.optim.Adam(
    model.parameters(),      # 优化器执行优化全部的模型参数
)</code></pre></div><p>再构造一个训练函数，并启动训练：</p><div><pre><code># 构建训练函数
def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):
    # loss存储
    losses = []

    # 训练时间记录开始时间
    start_time = time.time()

    # 循环训练指定epoch的轮数
    for epoch in range(config['epochs']):
        # 优化器要初始化啊，否则每次训练都是基于上一次训练结果进行优化，效果甚微
        optimizer.zero_grad()

        # 获取训练数据
        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])

        # 前向传播计算概率矩阵与loss
        logits, loss = model(xs, targets=ys)

        # 反向传播更新权重参数，更新学习率优化器
        loss.backward()
        optimizer.step()

        # 如果提供学习率调度器，那么学习率会通过调度器进行修改，比如学习率周期性变化，或者梯度减小，增加，具体策略需要综合考虑进行设置，详情自行查询，关键字：lr_scheduler
        if scheduler:
            scheduler.step()

        # 打印log
        if epoch % config['log_interval'] == 0:
            # 训练时间
            batch_time = time.time() - start_time

            # 执行评估函数，在训练集和验证集上计算loss
            x = evaluate_loss(model)

            # Store the validation loss
            losses += [x]

            # 打印进度日志
            if print_logs:
                print(f"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}")

            # 重置开始时间，用于计算下一轮的训练时间
            start_time = time.time()

            # 打印下一轮的学习率，如果使用了lr_scheduler
            if scheduler:
                print("lr: ", scheduler.get_lr())

    # 上面所有epoch训练结束，打印最终的结果
    print("Validation loss: ", losses[-1]['val'])

    # 返还每一步loss值的列表，因为我们要画图，返还的是loss迭代的图像
    return pd.DataFrame(losses).plot()

# 启动训练
train(model, optimizer)</code></pre></div><p>输出的loss变化是这样的：</p><p><img src="https://picx.zhimg.com/v2-778b58412c43c942ff7e43ff6012494d_r.jpg" class="large"></p><p>在1000个epoch的训练后，loss值从原来的8.3722，下降到了8.2150！</p><p><b>所以知道为什么这个模型架构叫“stupid”了吧，或者说“Broken“。</b></p><p><b>.....</b></p><p><b>......</b></p><p><b>.......</b></p><p><b>但是！</b></p><p><img src="https://pic2.zhimg.com/v2-a3663ac98688dac39437c74c4ccff5a1_1440w.jpg" class="large"></p><p><b>咱能修！</b></p><p>先分析一下原因：</p><p>上面那个训练框架存在一些问题。 回到前向传播的代码，也就是forward()中。 我们使用了<code> logits = F.softmax(a, dim=-1) </code>对线性层输出的结果做了一次概率分布的计算。 而loss的计算选择了交叉熵损失， 目标值的词表映射结果是整数，而模型输出的logits是概率矩阵。</p><p>这俩玩意计算，相当于在国内的大街上，遇到外国人，跟人家说一句“hey man,what's up”，老外说：“我会中文，嘎哈呀“，这么整也没问题，但是收敛起来就比较麻烦。</p><p>为了使loss计算更精确，我们需要将softmax去除。 以保证交叉熵损失的计算效果更好。</p><div><pre><code># 拿掉softmax，logits改为获取最后一个线性层输出的结果，不进行softmax计算概率分布。
# 因此将这个架构取名为：不那么蠢的模型架构
class SimpleNotStupidModel(nn.Module):
    def __init__(self, config=MASTER_CONFIG):
      super().__init__()
      self.config = config
      self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])
      self.linear = nn.Sequential(
          nn.Linear(config['d_model'], config['d_model']),
          nn.ReLU(),
          nn.Linear(config['d_model'], config['vocab_size']),
      )
      print("Model parameters:", sum([m.numel() for m in self.parameters()]))

    def forward(self, idx, targets=None):
        x = self.embedding(idx)

        # 看这里，线性层直接输出结果，不转换为概率矩阵，只修改这里，其余不动。
        logits = self.linear(x)
        # print(logits.shape)

        if targets is not None:

            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss
        else:
            return logits
        print("Model parameters:", sum([m.numel() for m in self.parameters()]))</code></pre></div><p>再跑一下：</p><div><pre><code># 再来一次实例化各种功能，再启动一次训练
model = SimpleNotStupidModel(MASTER_CONFIG)
xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])
logits, loss = model(xs, ys)
optimizer = torch.optim.Adam(model.parameters())
train(model, optimizer)

# loss开窍了，下降了很多</code></pre></div><p><img src="https://pic3.zhimg.com/v2-d6ff54ae617b68a8d15de587ecb65eac_r.jpg" class="large"></p><p>接下来构造一个推理函数：</p><p>并不准备传入数据，学习版，咱们就弄5个0，也就是词表中的换行符<code>'\n'</code>去推理。以换行符开始，循环推测后面20个字符</p><div><pre><code># 推理函数（输出结果就别纠结其效果了，权重都没保存，就是根据模型初始化生成的随机数组成的矩阵做的推理）
def generate(model, config=MASTER_CONFIG, max_new_tokens=20):
    # 生成5个0，作为输入数据,5行一列，代表输入5个字符。 这个地方可以自行替换其他随机数测试。
    idx = torch.zeros(5, 1).long()
    print(idx[:, -config['context_window']:])
    for _ in range(max_new_tokens):
        # 因为推理的时候，依赖后面的n个token，所以滑动窗口要从后往前选择输入数据的倒数几个token，这个是超过字符数量会对输入进行截断，只选取最后几个token：idx[:, -config['context_window']:]
        logits = model(idx[:, -config['context_window']:])
        # print(logits.size())
        # 得到模型输出的结果，进行解码，这里logits[:, -1, :]挺抽象的，实际上第一维度是输入的字符数，第二维度是时间步，第三维度是词表
        # 即，对每一步的解码结果，取最后一个时间步的数据，作为输出的数据。解码的过程是第一次解码，输入5个token，第二次解码依赖的是原来5个token的最后4个，加上上一步解码生成的一个，也是5个token，如此循环。
        last_time_step_logits = logits[:, -1, :]
        # print('last_time_step_logits')
        # print(last_time_step_logits.shape)
        # 计算概率分布
        p = F.softmax(last_time_step_logits, dim=-1)
        # print('p_shape')
        # print(p.shape)
        # 根据概率分布计算下一个token，这里使用 torch.multinomial做的是随机采样
        idx_next = torch.multinomial(p, num_samples=1)
        # print('idx_next_shape')
        # print(idx_next.shape)
        # 将新的idx通过张量拼接写入到解码序列中
        idx = torch.cat([idx, idx_next], dim=-1)
    # 使用之前定义的解码函数，将ID转换为汉字，我们得到的5行21列的数据，来源于每一个输入字符作为开始位置，生成20个字符。 因为5个输入都是0，在词表中编号为0的数据是'\n'。
    print(idx.shape)
    return [decode(x) for x in idx.tolist()]

generate(model)</code></pre></div><p>输出结果，不出所料，合乎情理，意料之中，挺差的:</p><div><pre><code>['\n尽行者，咬道：“尽人了卖。”这人。”众僧',
 '\n揪啊？怎么整猜脸。”那怪教沙僧护菩萨，貌',
 '\n你看着国尖，不知请你。”妖王髯明。\n须行',
 '\n老朗哥啊，遇货出便？路，径出聋送做者，似',
 '\n那个急急的八戒、沙僧的树。菩萨莲削，身\n']</code></pre></div><p>OK，到这里构造一个简单的Seq2seq模型，完毕，还是希望对上面的代码跑一下，理解一下，按行自行分析一下，如果有搞不清楚的，打印一下shape，看一下数据形状如何变换的。</p><p>如果上面全部搞清楚了，下面开始进入正题：在上面简单的模型基础上，逐步增加LlaMa的算子，并且每增加一个算子就看一下效果。</p><p>代码依旧是逐行注释，并且有些地方，咱也是为了学习分享，也记录了一些想法。</p><hr><p>加个分割线，提醒一下：喝水，上厕所，活动腰和脊椎，做提肛运动防痔疮。 自问一下：上面的内容是否已通透？马上就要考验高等数学学的是否扎实，是否准备好了？</p><hr><p>正篇开始：</p><h2 id="outline_5">将LlaMa的算子加入到上面的简易模型架构中</h2><p>主要包括：</p><ol><li>RMS_Norm</li><li>RoPE</li><li>SwiGLU</li></ol><h3 id="outline_6">RMSNorm快速了解</h3><p>norm，做标准化，训练过程中的张量标准化操作，通过计算均值和方差，将样本进行归一化。 在大学课程《概率与统计》我们学过，样本的均值代表样本的特征，而方差代表离散程度。</p><p>因此，通过计算，让数据变为均值为0，方差为1的数据。 这样可以使数据服从标准的正态分布。</p><p>记得大学时候，老师讲这一段的时候，着重强调：“高斯分布，正态分布”，也可以叫自然分布，自然界的很多统计情况，几乎都满足高斯分布。 两边向中心靠拢，超过中心的，随着逐渐增大，会越来越少，没超过中心的，距离中心越远，数量也越来越少。而分布的众数永远都是在中间。 </p><p>啊~ 数学之美，但是也美不过高数老师（嘿嘿）。</p><p>使用均值和方差计算数据的标准差，这样既保留了数据的异常值，同时维持数据的异常结构，这样可以稳定梯度，让梯度变化更稳定，减少梯度消失或者爆炸的问题，因为维持了异常结构，也能减少过拟合问题，增强泛化能力。</p><p>RMSNorm出来之前，广泛使用的batch_normlize，针对批次数据做标准化。标准化的数值是一个batch作为一个样本总体，计算其均值与方差。</p><p>而后，又出现了layer_norm，其是针对每个token的特征向量做归一化处理（不知道特征向量，请看本人之前的rope文章。应该可以理解token和特征向量的关系。）依旧需要计算均值和方差。</p><p>RMSNorm和layer_norm的主要区别在于RMSNorm不需要同时计算均值和方差两个统计量，而只需要计算均方根这一个统计量。在模型表现效果几乎与layer_norm持平的前提下，节省7%-64%的计算量。</p><p>RMS_Norm计算公式：</p><p><img src="https://pic1.zhimg.com/v2-db719aba9826a9d1dc71c67b824fc324_1440w.jpg" class="large"></p><h3 id="outline_7">RMSNorm 的工作原理</h3><ul><li><b>计算 RMS</b>： 对于输入的特征（如神经元的输出），首先计算其 RMS 值。</li><li><b>标准化</b>： 将每个特征的值除以其 RMS 值，从而调整数据的尺度，使其均值为 0，标准差为 1。</li><li><b>缩放和平移</b>： 在归一化后，RMSNorm 通常还会引入可学习的参数（缩放因子和偏置），以便模型能够学习适合特定任务的特征表示。</li></ul><p><b>猜想： 既然都平方根了，突然想起那个让程序员之神--约翰·卡马克直呼“卧槽”的快速平方根倒数算法了。 当然，也有可能这么经典的数值计算方法已经被集成进了pytorch。</b></p><p>RMS基本介绍差不多了，下面开始实现RMSNorm模块：</p><div><pre><code>class RMSNorm(nn.Module):
    def __init__(self, layer_shape, eps=1e-8, bias=False):
        super(RMSNorm, self).__init__()

        # torch中register_parameter()功能为：向我们建立的网络module添加parameter
        # 因此，我们需要对pytorch官方封装好的RMSNorm功能模块添加一个可以训练参数的层，命名为scale，并初始化为形状为layer_shape，所有值为1的张量矩阵。
        self.register_parameter("scale", nn.Parameter(torch.ones(layer_shape)))

    def forward(self, x):
        # 计算Frobenius范数（球某个矩阵中所有元素的平方和再开方得到，该范数用来衡量矩阵的大小，详情请百度）, RMS = 1/sqrt(N) * Frobenius
        # 具体来说，torch.linalg.norm(x, dim=(1, 2))计算了x在第1和第2维度上的范数。然后，将结果乘以x[0].numel() ** -.5。x[0].numel()表示x第一个元素（即x的第一行）的元素个数，** -.5表示求平方根的倒数。
        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5
        # print(ff_rms.shape)
        # 将ff_rms算子应用于输入的张量x，依据公式，做除法，因为输入向量x是三维的，因此需要对ff_rms进行升两维，也变成三维的张量。这样可以进行元素之间的计算。
        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)
        # print(raw.shape)

        # 返回缩放后归一化的张量
        # print(self.scale[:x.shape[1], :].unsqueeze(0) * raw)
        return self.scale[:x.shape[1], :].unsqueeze(0) * raw</code></pre></div><p>需要注意的是，<code>raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)</code>这里的计算是针对张量矩阵中的每个元素计算，计算出了范数（实际上就是一个数值，不是矩阵），对原本输入数据的张量矩阵中，每一个元素除以这个归一化范数。因为RMSNorm已经继承在pytorch官方的nn.Moudle中，因此我们对其稍加修改。</p><p>接着：我们将这个RMS_Norm算子加入到上面的简易模型中，代码中很清楚，如何加入的：</p><div><pre><code>class SimpleNotStupidModel_RMS(nn.Module):
    def __init__(self, config=MASTER_CONFIG):
      super().__init__()
      self.config = config
      self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])
      # 在这里，我们添加RMS层
      self.rms = RMSNorm((config['context_window'], config['d_model']))
      self.linear = nn.Sequential(
          nn.Linear(config['d_model'], config['d_model']),
          nn.ReLU(),
          nn.Linear(config['d_model'], config['vocab_size']),
      )
      print("Model parameters:", sum([m.numel() for m in self.parameters()]))

    def forward(self, idx, targets=None):
        x = self.embedding(idx)
        # 在这里，添加实例化后的RMS层，承接Embedding层输出的张量
        x = self.rms(x)

        logits = self.linear(x)
        # print(logits.shape)

        if targets is not None:

            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss
        else:
            return logits
        print("Model parameters:", sum([m.numel() for m in self.parameters()]))</code></pre></div><p>我们添加了RMS_Norm之后，再看一下训练效果：</p><div><pre><code># 好啦，这样我们对原来的NotStupidModel添加了RMSNorm，现在执行一下看看
model = SimpleNotStupidModel_RMS(MASTER_CONFIG)

xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])

logits, loss = model(xs, ys)

optimizer = torch.optim.Adam(model.parameters())

train(model, optimizer)

# 在同样的训练超参数设置上，加入了RMSNorm的训练速度明显加快。</code></pre></div><p><img src="https://pic1.zhimg.com/v2-913914d2af32f204147b8b6a26c08052_r.jpg" class="large"></p><p>如果跑了一下代码，你会感受到计算速度会变快。 </p><p><b>但是别急，下面两个算子会让计算速度更慢</b></p><p><img src="https://picx.zhimg.com/v2-8e7a7bc36539799b67780f61ae11a495_1440w.jpg" class="large"></p><hr><h3 id="outline_8">将旋转位置编码（Rope）加入到模型中</h3><p>RoPE很巧妙的将笛卡尔坐标系的计算，拉到极坐标空间计算。具体原理快速过一下可能说不完，有兴趣的可以看一下本厮的文章：</p><p><a target="_blank" href="https://zhuanlan.zhihu.com/p/780744022" class="link"></a></p><p><a target="_blank" href="https://zhuanlan.zhihu.com/p/830878252" class="link"></a></p><p>这里不再对原理进行分析，主要是代码实现。</p><p><b>还是建议先看完上面两个文章，代码部分就很好理解了</b></p><p>先定义一个函数，用于计算旋转位置编码：</p><div><pre><code>def get_rotary_matrix(context_window, embedding_dim):
    # 初始化一个0填充，形状为（context_window, embedding_dim, embedding_dim）的张量矩阵，其中context_window为token数量，后面两个embedding_dim组成正方形矩阵，与后面的attention计算对齐格式
    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)
    
    # 遍历每一个位置的token
    for position in range(context_window):
        # 还记得我的上一篇文章中说的，对于特征，两两组合吗，因此需要循环的次数为embedding_dim除以2
        for i in range(embedding_dim // 2):
            # 设置θ值，采样频率，或者说旋转频率，旋转角都可以，除以embedding_dim防止梯度问题。(此处由知乎ID为：bignova的大佬修正。
            theta = 10000. ** (-2. * i) / embedding_dim)
            # 根据欧拉公式，计算旋转的角度，分别有sin 和cos，将计算拉到复数空间，并将旋转角度应用在上面的0填充的矩阵
            m_theta = position * theta
            R[position, 2 * i, 2 * i] = np.cos(m_theta)
            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)
            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)
            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)
            # 得到的结果是旋转位置编码矩阵，到这里还没覆盖到attention
    return R</code></pre></div><p>因为旋转位置编码是结合了注意力机制的Q和K进行计算的，因此我们先实现一个单头的注意力机制的算子：</p><div><pre><code># 此为单头注意力机制
class RoPEMaskedAttentionHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # 计算Q权重矩阵
        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)
        # 计算K权重矩阵
        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)
        # 计算V权重矩阵
        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)
        # 获得旋转位置编码矩阵，接下来会覆盖Q和K权重矩阵
        self.R = get_rotary_matrix(config['context_window'], config['d_model'])


    # 这里将上一个代码块中实现的创建旋转位置编码的功能函数原封不动的拿过来
    def get_rotary_matrix(context_window, embedding_dim):
        # 初始化一个0填充，形状为（context_window, embedding_dim, embedding_dim）的张量矩阵，其中context_window为token数量，后面两个embedding_dim组成正方形矩阵，与后面的attention计算对齐格式
        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)
        
        # 遍历每一个位置的token
        for position in range(context_window):
            # 还记得我的上一篇文章中说的，对于特征，两两组合吗，因此需要循环的次数为embedding_dim除以2
            for i in range(embedding_dim // 2):
                # 设置θ值，采样频率，或者说旋转频率，旋转角都可以，除以embedding_dim防止梯度问题。
                theta = 10000. ** (-2. * (i - 1) / embedding_dim)
                # 根据欧拉公式，计算旋转的角度，分别有sin 和cos，将计算拉到复数空间，并将旋转角度应用在上面的0填充的矩阵
                m_theta = position * theta
                R[position, 2 * i, 2 * i] = np.cos(m_theta)
                R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)
                R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)
                R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)
                # 得到的结果是旋转位置编码矩阵，到这里还没覆盖到attention
        return R

    def forward(self, x, return_attn_weights=False):
        # 前向传播时，输入矩阵的形状为(batch, sequence length, dimension)

        b, m, d = x.shape  # batch size, sequence length, dimension

        # 线性变换Q,K,V
        q = self.w_q(x)
        k = self.w_k(x)
        v = self.w_v(x)

        # 将旋转位置编码应用于Q和K，其中torch.bmm为矩阵做外积，transpose是转置，对Q矩阵转置，并与旋转位置编码做外积，再转置回原状，Q便应用了旋转位置编码。
        # 考虑到输入文本的长度，因此对位置编码矩阵在第一维度做截断，因为长了也没用，与文本长度一样。
        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)
        # 同理对K也应用旋转位置编码进行覆盖
        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)

        # 对注意力机制点积进行等比例缩放，防止attention张量过长引发梯度爆炸，对应
        activations = F.scaled_dot_product_attention(
            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True
        )
        # 如果return_attn_weights参数置为1，则需要对attention进行掩码，因为在学习的时候，希望模型能依据前n个token去预测token，而不是开卷考试。
        if return_attn_weights:
            # 创建注意力掩码矩阵，其中torch.tril函数为：对于矩阵，取左下三角，剩下的都置0
            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)
            # 计算注意力机制的权重矩阵，并对最后一维度做归一化，（突击检查）为什么是最后一维！因为最后一维度是每个token的特征向量！
            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask
            attn_weights = F.softmax(attn_weights, dim=-1)
            return activations, attn_weights

        return activations</code></pre></div><p>单头注意力机制实现完毕，下面是多头的注意力机制：</p><div><pre><code># 单头注意力机制实现完毕，下面实现多头注意力机制
class RoPEMaskedMultiheadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # 一个注意力机制头对象构建完毕了，多头的，首先多次创建这个对象。生成多个注意力机制头，塞到一个列表里。
        self.heads = nn.ModuleList([
            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])
        ])
        # 在模型结构上，创建一个线性层（隐藏层），用于线型输出注意力机制头输出的张量矩阵，寻找多头之间的特征，但是更主要的是，x经过多头计算后形状改变了，创建线性层，让张量矩阵变回原来输入的形状。
        # 同时为了防止过拟合，使用随机神经元失活，比率0.1
        # 线性层输入形状：注意力机制的头数，乘以矩阵的维度，关联到俺的上一篇文章，就是key矩阵，在多头之间共享权重，减少计算的思维。 输出为：模型的embedding维度数
        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])  
        self.dropout = nn.Dropout(0.1)  

    def forward(self, x):
        # 输入矩阵形状x： (batch, sequence length, dimension)

        # 每一个注意力机制头，都传入X进行计算。（这个地方开启并行执行会不会快一些，但是不知道pytorch是不是自动调用并行）
        heads = [h(x) for h in self.heads]
        # 输入张量x经过多个头计算attention（同时，attention是已经覆盖了RoPE的），重新拼接成新的矩阵，重新放入变量x。到这里你应该觉得：那矩阵形状不就变了吗
        x = torch.cat(heads, dim=-1)
        
        # 这不，线性层的作用来了
        x = self.linear(x)
        
        # 随机失活一下，防止过拟合
        x = self.dropout(x)
        return x</code></pre></div><p><b>如果已经感觉，除了懵逼，已经伤脑了。还是先学习一下RoPE的原理，即使不是本厮的那两篇，也可以找一些能理解的文章，白话解释，甚至问chatgpt，任何方法，只要作者的讲述方式，和自己的脑电波频率对得上，能理解，就OK。</b></p><p>好啦，上面的功能已实现，让我们更新一下config超参数的字典，设定注意力机制头数，LlaMa是32个注意力机制头，我们创建8个：</p><div><pre><code>MASTER_CONFIG.update({
    'n_heads': 8,
})</code></pre></div><p>接下来，我们更新我们的建议模型，之前更新了RMS_Norm，这次我们在那基础上，将融合了ROPE位置编码的多头注意力机制加入进去！</p><div><pre><code># 我们已经创建完了所需要的算子，  现在积木已创建完毕，将这些积木组合起来！！！！
class RopeModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # Embedding层
        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])
        
        # RMSNorm层
        self.rms = RMSNorm((config['context_window'], config['d_model']))
        
        # 旋转位置编码器+注意力机制
        self.rope_attention = RoPEMaskedMultiheadAttention(config)

        # 线性层+激活函数变为非线性输出！
        self.linear = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            nn.ReLU(),
        )

        # 最终的输出，因为需要解码，因为输出的维度与词表大小统一！！！
        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])

        print("model params:", sum([m.numel() for m in self.parameters()]))
    # 前向传播
    def forward(self, idx, targets=None):
        # embedding，不解释
        x = self.embedding(idx)
        # 归一化数值，不解释
        x = self.rms(x)  
        # 相加，解释一下，因为attention是要覆盖到原矩阵的，想象两个形状一样的矩阵为两张纸，左手一张纸，右手一张纸，双手合十，啪！覆盖。 使用加算，就是将两个矩阵中的元素按位置相加！直接覆盖值！
        x = x + self.rope_attention(x)
        # 再归一化！
        x = self.rms(x)
        # 因为直接计算归一化的数值可能出现梯度问题，因此把归一化的值作为修正系数，再覆盖！  
        x = x + self.linear(x)
        # 到这里，才是最终输出vocab数量的神经元输出！！！！！！
        logits = self.last_linear(x)

        # 训练阶段有目标值
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss
        # 验证或者推理阶段，目标值y没有！只有结果，没有loss！
        else:
            return logits</code></pre></div><p>老规矩，跑一下，看看效果！</p><div><pre><code># 再跑一下！
model = RopeModel(MASTER_CONFIG)
xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])
logits, loss = model(xs, ys)
optimizer = torch.optim.Adam(model.parameters())
train(model, optimizer)</code></pre></div><p><img src="https://pica.zhimg.com/v2-40886e89d29ef87e0a2a5e0a9dc9b356_r.jpg" class="large"></p><p>太惊人了，loss足足下降了0.01！</p><p>额。实际上就是attention没有做mask，虽然功能实现了，但是没有调用，布尔值设置的是false。 </p><h3 id="outline_9">SwiGLU算子</h3><p>将swish和glu结合起来。这两个激活函数单独拿出来都很强，结合起来。</p><p>这玩意挺玄学，说它不好吧，但是这玩意确实比relu这败家子保留更多语义特征参数，不至于某个权重突然到了小于0的区间，然后糊里糊涂的消失。说它好吧，它的计算量确实挺大。   </p><p>swish用了sigmoid，GLU用了门控结构（门控结构思想，可以学习一下RNN,GRU,LSTM什么的）</p><p><b>由于SwiGLU部分，本厮也没深入研究过，因此以下是抄来的东西：</b></p><div><pre><code>Swiglu 的工作原理
Swiglu 将 Swish 和 GLU 结合在一起，计算方式如下：
计算两个部分：
输入通过线性变换得到 a 和 b。
a 使用 Swish 激活，b 使用 Sigmoid 激活。
组合结果：
最终的输出由 a 和 b 的组合决定，形式为： 
优势
灵活性：Swiglu 结合了非线性和门控机制，允许模型更灵活地捕捉复杂的模式。
性能：在某些任务上，Swiglu 已显示出比传统激活函数更好的表现，尤其是在处理复杂数据时。</code></pre></div><p>下面实现一下：</p><div><pre><code>class SwiGLU(nn.Module):
    
    def __init__(self, size):
        super().__init__()
        # 定义一个门控的线性层，输入输出都是门控结构的尺寸 
        self.linear_gate = nn.Linear(size, size) 
        # 门控结构主干线性层 
        self.linear = nn.Linear(size, size)
        # 初始化一个随机数作为beta系数  
        self.beta = torch.randn(1, requires_grad=True)  

        # nn.Parameter用于指定某一层参数为可学习的，即本来不能通过训练更改参数，现在变成了可以经过训练来更新的参数。
        self.beta = nn.Parameter(torch.ones(1))
        # 将随机数beta指定为一个名为beta的神经网络层
        self.register_parameter("beta", self.beta)

    def forward(self, x):
        # Swish门控但愿的计算：（从括号里开始）对于原始输入的数据张量，经过线性变换乘以beta系数，再经过sigmoid变换为0-1之间的值，再乘以原数据经过门控线性变换。总的来说，线型输出经过非线性变换，再应用到线性变换的结果，元素按位置相乘，修正原本数据张量，就是这个门控结构做的事情。
        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))
        # 将门控结构输出的值再按位乘以线型输出的原数据张量
        # 为啥这么做，我不知道，但是论文复现的代码就是这样滴，有兴趣可以研究一下，我没研究过。
        out = swish_gate * self.linear(x)  
        return out</code></pre></div><p>再将刚刚定义好的swiglu算子添加到模型中，该模型我们之前添加过RMS、RopeAttention。</p><div><pre><code># 再将swiglu添加进上面的模型
class RopeModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])
        self.rms = RMSNorm((config['context_window'], config['d_model']))
        self.rope_attention = RoPEMaskedMultiheadAttention(config)
        self.linear = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            # 在这里，增加了SwiGLU层
            SwiGLU(config['d_model']),  
        )
        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])
        print("model params:", sum([m.numel() for m in self.parameters()]))

    def forward(self, idx, targets=None):
        x = self.embedding(idx)
        x = self.rms(x)  
        x = x + self.rope_attention(x)
        x = self.rms(x)
        x = x + self.linear(x)
        logits = self.last_linear(x)

        if targets is not None:
            # Calculate cross-entropy loss if targets are provided
            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss

        else:
            return logits</code></pre></div><p>再跑一次！</p><div><pre><code># 一二三四！再来一次！
model = RopeModel(MASTER_CONFIG)
xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])
logits, loss = model(xs, ys)
optimizer = torch.optim.Adam(model.parameters())
train(model, optimizer)</code></pre></div><p><img src="https://picx.zhimg.com/v2-4975695865f582ce8de0a8878e4e9d41_r.jpg" class="large"></p><p>loss又惊人地下降了0.1。。。。。。。</p><p>到这里，能添加的算子已经添加完毕，但是这只是一个llama的功能块，而llama是由多个功能块堆叠组成的。</p><p>加个分割线，该喝水的喝水，该活动脊椎的活动脊椎，建议消化一下。   或者有精通SwiGLU的大佬也顺手出个教程~~~</p><p>如果可以的话，我们继续，接下来开始实现LlaMa</p><hr><p>上面我们实现了LlaMa所需的算子，下面我们将其组合成为LlaMa的功能块，然后通过功能块的堆叠，组成LlaMa！</p><p>首先，设定一下功能块堆叠的层数：</p><div><pre><code># OK！ 现在我们更新一下，隐藏层维度堆叠多少层，我们先来4层尝尝咸淡！！！！
MASTER_CONFIG.update({
    'n_layers': 4,  
})</code></pre></div><p>然后，组成LlaMa的功能块，实际上就是融合了RMS、AttentionRoPE、SwiGLU激活函数的那个简易模型：</p><div><pre><code># 现在我们拥有了所有的算子，RMS，ROPE,SWIGLU，我们搭建我们的LlaMa！ 首先实现LlaMa的功能块，然后堆叠。
# 功能没什么好讲的，如果仔细看到了这里，下面的每一行代码都难不住你。
class LlamaBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.rms = RMSNorm((config['context_window'], config['d_model']))
        self.attention = RoPEMaskedMultiheadAttention(config)
        self.feedforward = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            SwiGLU(config['d_model']),
        )

    def forward(self, x):

        x = self.rms(x) 
        x = x + self.attention(x)
        x = self.rms(x) 
        x = x + self.feedforward(x)
        return x</code></pre></div><p>看一下超参数字典</p><div><pre><code># 看一下我们的超参数字典
MASTER_CONFIG

# 输出：
#{'batch_size': 32,'context_window': 16, 'vocab_size': 4325,'d_model': 128,'epochs': 1000,'log_interval': 10,'n_heads': 8,'n_layers': 4}</code></pre></div><p>测一下创建的功能块有没有问题：</p><div><pre><code># 用config字典，创建llama的功能块
block = LlamaBlock(MASTER_CONFIG)

# 生成一条随机数据，丢到这个llama功能块里，看一下是不是有bug
random_input = torch.randn(MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'], MASTER_CONFIG['d_model'])

# 执行以下看看输出
output = block(random_input)
output.shape</code></pre></div><p><b>组装LlaMa！</b></p><div><pre><code># 现在，我们组装LlaMa
from collections import OrderedDict
class Llama(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # Embedding不解释
        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])
        # 根据传入的堆叠层数，创建Llama功能块，注意OrderedDict为一种特殊类型的字典数据，保留字典写入的顺序，先插入的数据在前，后插入的数据在后。
        # 这里，我们将llama的功能块堆叠4层
        self.llama_blocks = nn.Sequential(
            OrderedDict([(f"llama_{i}", LlamaBlock(config)) for i in range(config['n_layers'])])
        )
        # FFN层，包含：线性层、激活函数非线性变换、再用线性层输出最终解码数值。
        self.ffn = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            SwiGLU(config['d_model']),
            nn.Linear(config['d_model'], config['vocab_size']),
        )

        # 看看咱们的大模型多少参数！
        print("model params:", sum([m.numel() for m in self.parameters()]))

    def forward(self, idx, targets=None):
        # embedding嵌入
        x = self.embeddings(idx)
        # Llama模型计算
        x = self.llama_blocks(x)
        # FFN计算，得到logits
        logits = self.ffn(x)

        # 推理阶段没有目标值，只输出结果
        if targets is None:
            return logits
        # 训练阶段，有目标值，需要输出结果，以及loss，用于反向传播更新权重！
        else:
            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss</code></pre></div><p>训练LlaMa</p><div><pre><code># 开始训练咱们的Llama
llama = Llama(MASTER_CONFIG)
xs, ys = get_batches(dataset, 'train', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])
logits, loss = llama(xs, ys)
optimizer = torch.optim.Adam(llama.parameters())
train(llama, optimizer)</code></pre></div><p>推理一下：</p><div><pre><code># 再看一下推理效果（实际上也没什么效果-。-）
# 别忘了generate里面的输入数据是咱们弄的5个0，如果替换为encode之后的数也是可以的！组成列表，转换tensor，这个应该没问题的吧~
generated_text = generate(llama, MASTER_CONFIG, 500)[0]
print(generated_text)
 </code></pre></div><p>推理的效果还是挺抽象的。感觉这文本推理像在逆练《新华字典》</p><p>当然，压根就没指望推理效果。所以之前就说了，本篇文章本身就不是开箱即用的，更适合中国宝宝学习大模型的文章。</p><div><pre><code>玄兔春非朦意敖叶祥肤水沉岭日疼清赛。萝范燕旗顺清气瑜灵子庄。海藏御直熟列棱
尉火牙心正花，四位幸堂国如生岖。
辟两只道：千人人纵开羊堤进宝贝，乃人头破马前立阵，腰一年心；足如蜻丛杨馥新一耸飘木
膜须寺颜凤丹叶獐绦；桃送？
添牛福芽蟒菜丛体茵岁鹅宫丰，贼培莲豆龙东兵，九千
蓝穿鸣人罔二无团蛇陌盔，日滚听拿
万盏可帮迎。这壁楼去常路递淫栖询除朱，
渎制然便跑神果。因安群猴俱无发能放。今
钟无削乐都有僵拿，
能可怕饭息饯？”又说他怒道：“师父是两魔头请久得五条大神神净，全
天
前顷飞影多功。有四尺阳皆之挨初山，只然艰薤盘裳见大仙的清过武前，阻雨野轮板雕青应沫。
月海依乃射仙僧岸，更天淡海月蓬为白巅耀院，日花匾盏神润晴涧攒肾壑笠绵，恶非狐成聚三藏灵玉火花，玉西草奇竹主深。
德太影任涨青叶，十莲缨猪大圣天寸耀亮红、平
啸壮名空猿携绩蝶泾帝妖
满冻在我跑旧热力。唬得小子，

又五凤景晚。细抖见出前蓝泛卿西浪花白帘谷琶？一张酥罢旨
严胜锁来鬼处将他几个人师父！”喝道：“我们这厮伏：“可来，我伤得！不要说，有水洒悭迟。
待吾叫做，
俱的劈摆道者怕形，等我个妖精坐在鹤下，钩怪藏
手里罩放发，
天篮，掣身</code></pre></div><p>测试集跑一下，甚至都忘了测试集了吧，忘了的回头看看get_batch那个函数：</p><div><pre><code># 下面是测试集跑一下 
# 获取测试集的特征值和目标值
xs, ys = get_batches(dataset, 'test', MASTER_CONFIG['batch_size'], MASTER_CONFIG['context_window'])

# 丢进Llama获取loss
logits, loss = llama(xs, ys)

print(loss)
# 输出：
# tensor(4.7326, grad_fn=&lt;NllLossBackward0&gt;)</code></pre></div><p>加点玩意，让其看上去更像回事，增加学习率调度器</p><div><pre><code># 还有优化的点哦，别忘了optimizer！以及学习率调度器！
# 调整参数再来一次！

MASTER_CONFIG.update({
    "epochs": 1000
})

# 学习率优化器选择余弦退火
llama_with_cosine = Llama(MASTER_CONFIG)

llama_optimizer = torch.optim.Adam(
    llama.parameters(),
    betas=(.9, .95),
    weight_decay=.1,
    eps=1e-9,
    lr=1e-3
)
# 余弦退火学习率优化器，让学习率逐渐减小，在结束时达到最低值。 详细可以百度，这种文章很多。
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)

# 跑一下！
train(llama_with_cosine, llama_optimizer, scheduler=scheduler)</code></pre></div><p><img src="https://picx.zhimg.com/v2-a35b0d899920148b10e73d8dbac67589_r.jpg" class="large"></p><p>跑2万个epoch就好了。毕竟所有参数都是随便弄的。。。。。</p><p>好咯，剩下的没什么了，我们保存一下模型，然后再写个调用模型推理的函数。</p><p>参数路径自己改哦</p><p><b>保存模型：</b></p><div><pre><code># 保存模型权重
model_save_path = "./hf_model_save/pytorch_model.bin"
torch.save(llama_with_cosine.state_dict(), model_save_path)</code></pre></div><div><pre><code># 生成一个config文件
import json

config_save_path = "./hf_model_save/config.json"
with open(config_save_path, 'w') as f:
    json.dump(MASTER_CONFIG, f)</code></pre></div><div><pre><code># 保存optimizer和学习率调度器的状态，方便继续微调
optimizer_save_path = "./hf_model_save/optimizer.pt"
torch.save(llama_optimizer.state_dict(), optimizer_save_path)

scheduler_save_path = "./hf_model_save/scheduler.pt"
torch.save(scheduler.state_dict(), scheduler_save_path)</code></pre></div><p><b>加载模型：</b></p><div><pre><code># 接下来是加载模型
llama_with_cosine = Llama(MASTER_CONFIG)  

# 加载模型权重
model_save_path = "./hf_model_save/pytorch_model.bin"
llama_with_cosine.load_state_dict(torch.load(model_save_path))

# 设置为评估模式
llama_with_cosine.eval()</code></pre></div><div><pre><code># 加载优化器和学习率调度器，如果需要继续训练什么的。
llama_optimizer.load_state_dict(torch.load(optimizer_save_path))
scheduler.load_state_dict(torch.load(scheduler_save_path))</code></pre></div><div><pre><code># 进行推理
output = generate(llama_with_cosine, MASTER_CONFIG)
print(output)</code></pre></div><hr><p>恭喜你，看到这里的你，炼丹成功！ 成功炼制出了自己的人工智障大模型！</p></div><div class="sub-page"><h2 class="sub-title" id="outline_10">（徒手搓LLM）逐行代码从0构造一个LLM——LlaMa篇(2)</h2><p class="meta avatar"><span class="ant-avatar ant-avatar-circle ant-avatar-image css-16cpc8y"><img src="https://picx.zhimg.com/v2-b402103ff7c9c37dbcf5730bcd348a72_l.jpg?source=172ae18b 2x" class="tiny"></span><span class="author">mc112611</span><span>2025-02-19 18:50</span></p><p></p><div><img src="https://picx.zhimg.com/v2-4df12f558cf624550fb29258cbca1d17_b.jpg" class="large"><p><svg width="50" height="50" viewBox="0 0 60 60" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><ellipse fill="#000" opacity="0.45" cx="30" cy="30" rx="30" ry="30"></ellipse><ellipse stroke="#FFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" opacity="0.5" cx="30" cy="30" rx="26" ry="26"></ellipse></g><svg x="16" y="18.5"><g fill="#fff"><path x="100" y="100" d="M12.8422852,12.9814453 L12.8422852,11.3994141 L7.63916016,11.3994141 L7.63916016,13.0517578 L10.9086914,13.0517578 L10.9086914,13.3242188 C10.8911133,15.2050781 9.46728516,16.4707031 7.39306641,16.4707031 C5.01123047,16.4707031 3.51708984,14.625 3.51708984,11.6367188 C3.51708984,8.70117188 5.00244141,6.84667969 7.34912109,6.84667969 C9.08056641,6.84667969 10.284668,7.68164062 10.7768555,9.2109375 L12.7543945,9.2109375 C12.3237305,6.64453125 10.2319336,5.00976562 7.34912109,5.00976562 C3.79833984,5.00976562 1.50439453,7.61132812 1.50439453,11.6542969 C1.50439453,15.75 3.77197266,18.3076172 7.36669922,18.3076172 C10.6889648,18.3076172 12.8422852,16.2246094 12.8422852,12.9814453 Z M17.5180664,18 L17.5180664,5.31738281 L15.5493164,5.31738281 L15.5493164,18 L17.5180664,18 Z M22.659668,18 L22.659668,12.7441406 L28.1088867,12.7441406 L28.1088867,11.0039062 L22.659668,11.0039062 L22.659668,7.11035156 L28.6098633,7.11035156 L28.6098633,5.31738281 L20.690918,5.31738281 L20.690918,18 L22.659668,18 Z"></path></g></svg></svg></p></div><p>好的，基本上就是这样，有了主干部分，剩下策略部分，或者增加其他算子，只要遵循结构，没什么不可以的。关注点可以放在：输入和输出 。   矩阵的计算需要数学知识，区分是元素计算，还是向量矩阵整体的计算。</p><p>看到这里的你，如果对上面的内容理解差不多了，那么我相信，下面这个llama3源码中的<code>model.py</code>文件，对于你来说已经可以拿捏了。不得不感叹，meta团队是真的想让全世界看懂他们的代码，极具工整性与强解释性的代码，非常适合大模型学习。</p><p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//github.com/meta-llama/llama3/blob/main/llama/model.py" class="link"></a></p><hr><p>留了个花活儿在colab的那个分享的ipynb脚本中，实际上也不是花活儿，就是把刚刚的人工智障，用fastAPI部署一个异步处理的服务。没兴趣的可以撤了。</p><hr><p><b>感谢耐心看到这里的你，如果这篇文章对你的学习、工作有帮助，请点赞收藏，这对于一个正在攒首付的打工仔来说是莫大的帮助（至少心理上有满足）。 如果有错误，请指正。</b></p></div><div class="footer"><div class="ant-space css-16cpc8y ant-space-horizontal ant-space-align-center" style="flex-wrap: wrap;"><div class="ant-space-item"><span class="ant-tag css-16cpc8y">LLM</span></div><div class="ant-space-item"><span class="ant-tag css-16cpc8y">Llama3</span></div><div class="ant-space-item"><span class="ant-tag css-16cpc8y">从零手写大模型</span></div></div></div><p style="text-align:center;margin-top:100px;">由 <a href="https://circlereader.com" target="_blank">Circle 阅读助手</a> 生成</p></div></div></div></div></body></html>