<!-- 前面的代码保持不变 -->

<style>
    img {
        max-width: 100%;
        height: auto;
    }
    </style>
    
    <!-- 后面的代码保持不变 -->
<h1><a href="https://zhuanlan.zhihu.com/p/677650022">聊聊大模型推理服务中的优化问题</a></h1>
<p>![](https://pica.zhimg.com/v2-4b6941f38c02ea8d9f2803d3cf0865ae_l.jpg?source=172ae18b 2x)刀刀宁2024-05-16 22:21共 17528 字阅读需 70 分钟</p>
<p>大师拿起一杯子满满的石头，问满了吗？嗯。大师微微一笑，接着把一些沙子轻轻地倒进了杯子，沙子滑入石头缝中，问满了吗？啊，应该满了吧？大师又微微一笑，接着又拿出一瓶水，缓缓倒入杯子。</p>
<p><img src="https://pic4.zhimg.com/v2-a90cf90ac10f4abd1af381849a76928b_r.jpg" alt=""></p>
<p>杯子里的石头缝里能装沙子，沙子缝里还能装水。石头像极了一个服务系统中跑的大任务，在任务的时间安排中，可能还能跑一些中型任务和更小的任务。甚至，其实单个任务在跑的时候，捎带脚带上一些其他任务一起跑，可以一点都不增加运行时间。当我们优化一个大模型算子之后，相当于把石头磨的更小。但是如果拿到的石头不能变了，石头还是那些石头，但是通过改变石头不同的排列方法，会让相同的一个杯子放下更多的石头。</p>
<p>鉴于大模型自回归的特点，大模型在一个 request decode 的时候是一个 token 一个 token 出来的，那么我们可以把 token 看做小石子，而 request 是多个小石子组成的大石子。因为 decode 结束时机在 decode 中是无法准确预测的，所以 <strong>request 组成的大石子尺寸是不定的</strong>。这是大模型推理服务中一个显著的与一般服务系统（如 CNN 图像推理、搜索引擎检索等）不同之处。</p>
<p><img src="https://pic1.zhimg.com/v2-fd0c4498a1c90bcb5dbd3e808c18640c_r.jpg" alt=""></p>
<p>BurstGPT 中的 request 和 response token 数量分布</p>
<p><a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2401.17644">BurstGPT</a> 中有对 request 和 response 的 token 数量分布的描述，这里选择其中两张图，分别说明在 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=ChatGPT&amp;zhida_source=entity">ChatGPT</a> 和 Llama2 模型下，request 可以在 2000（这里应当是限制了 2048 的上下文长度）以内的长度，而 response 则在 2000 以内的不确定的长度。要知道，现在类似 KIMI 等大模型主打的超长上下文的长度已经在 200k 以上了，而不是这里的 2k 。</p>
<p>那么当整体访问量暴涨到一定规模知乎，例如 ChatGPT 在 2023 年年初出圈爆火之后，OpenAI 官方网站的访问量在<strong>短短一个月内就突破 10 亿次</strong>。并且这样的请求还都是实时无法知道 response 长度的情况下。如何调度这样的大模型特定场景问题，是当下非常有挑战的问题。</p>
<p>具业界的数据看，英伟达全年也就卖出去百万块 H100，据说微软系的公司一共收了 15w 块的样子，毕竟训练更加吃卡，能给到ChatGPT 应用推理服务的占比就还更少。所以，越是头部的公司越是有更大的压力来进行优化。</p>
<h2><strong>服务层调度优化问题</strong></h2>
<p>众所周知，机器学习模型的推理，跟训练相比起来，好像简单很多：不需要<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&amp;zhida_source=entity">反向传播</a>、不需要多轮次运算、不需要准备大量的高质量的数据。因此首先大家会关注到训练阶段，一般会忽略对服务阶段的考虑。</p>
<p>大模型也是一样，所以：</p>
<h3><strong>看得见的推理问题：</strong></h3>
<p>不就是 Transformer 结构跑个前向？再不济最多就是一个 token 一个 token 往外蹦的时候每个 token 都要跑一次前向？最后输出 N 个词就 N 次前向？那我优化好每一步不就好了？</p>
<h3><strong>看不见的推理问题：</strong></h3>
<p>实际上，大语言模型在应用于服务系统的时候，并不是只满足一个用户的一次推理需求。也就是说真实的推理服务场景中，是很多用户同时并发请求，服务器同时满足每个用户的不同长度的请求，并给出不同长度的输出。要求是每个请求的延时都尽量短，如果可以让整个系统在单位时间处理的数据更多，也就是吞吐率，那就更好了。</p>
<p>那么可能有的小明就会问，最简单暴力的方法，用一台机器去服务一个用户，服务完一个对话再串行的服务下一个用户不就完事了吗？<strong>也不是不行，就是太浪费了。</strong></p>
<p>这很像云存储刚刚出现的时候，有人会问在云上用存储空间和我自己在家里用几块大硬盘，也没什么区别，为什么云存储提供商他们需要花费那么多钱和工程师去研究产品？这背后的疑问可能差不多，重点是没有看到<strong>当模型变大、同时使用的人增多的时候</strong>面临着什么单机单卡没有遇到的问题。</p>
<p>大模型的推理优化，本身是个很复杂的领域，拥有多个技术方向。所以，有一丢丢<strong>头疼的地方是</strong>，因为问题比较新，平时交流时会发现，大模型服务优化，大家的理解有不太一样。主要是<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E8%BD%AF%E4%BB%B6%E6%A0%88&amp;zhida_source=entity">软件栈</a>的划分还相对比较模糊。服务层、服务化，略微有些隐形。同时，业界对这个分类的强调不是很多，我们平时看到的推理技术讨论和论文，乍一看上去，很多时候很难分清楚是哪一类问题。并且两类问题都可以用于单机系统也都可以用于分布式系统。</p>
<p>文本重点想讨论的是，<strong>多请求的大模型推理服务</strong>系统的优化问题，与之对应的是<strong>单次请求的<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF&amp;zhida_source=entity">推理优化技术</a></strong>。比如说，vLLM、Paged Attention、Continuing Batching 等技术，就是多请求服务问题。再比如说 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=FlashAttention&amp;zhida_source=entity">FlashAttention</a>、Kernel Fusion 等等这些就是无关单请求还是多请求情况下都能用得到的底层优化。</p>
<p>回到我们前文所说的水杯装石头的比喻中，单请求优化就是让单个石头更小，多请求优化就是在瓶子里优化怎么放石头放的更多更好。</p>
<hr>
<h2>多请求优化时面临的问题主要：</h2>
<p>服务系统核心三大件：<strong>延时</strong>、<strong>吞吐率</strong>、<strong>容错</strong>。也就是：确保响应时间、最大化资源利用、<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6&amp;zhida_source=entity">容错机制</a>。同时，这也是评价优化效果的评价方向。</p>
<p>大模型的服务系统同样也是这三大件，而在大模型问题下，三大件问题有增无减。在大规模并发的大<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F&amp;zhida_source=entity">模型服务系统</a>下，问题则变得更为突出。LLM 服务的请求调度与通用 ML 服务技术具有共性，因为两者都旨在有效管理传入请求并优化资源利用。而这些共同之处包括**<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86&amp;zhida_source=entity">动态批处理</a>、抢占、优先级、交换、模型选择、成本效率、负载平衡和资源分配**等。也需要满足延迟服务水平（service level objective SLO）等通用互联网服务目标。同时，LLM 系统还有其独特的特点，例如模型非常巨大、迭代的自回归解码机制、不可以预测的未知输出长度，以及上下文信息的状态管理，这使得 LLM 也引入了独特的挑战。</p>
<p>以上是在线服务系统的基本特点，其实还有一类推理问题就是离线的推理任务，比如包括用于对齐、蒸馏、分析等的推理任务。这些任务的特点是不着急，但是不要让其对资源的占用变成严重的问题，这时延时不一定是第一位需求，吞吐量大则是刚需。</p>
<p>另外一个不容忽略的问题就是服务系统的容错问题，故障率在分布式服务系统中本身就是极为重要的工作，每张卡每台机器每个互联或者机柜都有可能在服务过程中挂掉，因为当下 LLM 服务中有大量的混和粒度（request、group、token 等，后文会展开）的调度问题，保存实时状态与快速恢复也是一个需要重点优化的问题。除此之外，我们还会关注内存管理方面的问题，因为在实践中，有些模型虽然能加载，但在服务运行过程中，容易在长序列请求中 OOM ，这一般都是因为对内存管理的容错机制出了问题。</p>
<h3>单请求多请求优化与单机优化分布式优化的关系：</h3>
<p>我之前在 <a href="https://www.zhihu.com/question/636301026/answer/3355733107">LLM 推理相比普通 transformer 推理有什么特殊性？</a> 中的回答，当时我是将 LLM 推理等同于<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=2&amp;q=%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%8D%E5%8A%A1%E7%B3%BB%E7%BB%9F&amp;zhida_source=entity">分布式服务系统</a>了，因此 LLM 推理是要比普通 Transformer 推理要复杂不少的。当然，其实多请求优化，也是可以在单机上做的，只是多请求一旦超过一定量，就一定需要扩展到分布式的。所以，很多时候大规模请求服务系统，一定是分布式的。可以说，大语言模型已经是一个庞大的系统工程了，当它遇到分布式系统时，问题又称指数级增加，必将迸发出更多闪亮的火花。</p>
<p>那么，既然是<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=2&amp;q=%E5%88%86%E5%B8%83%E5%BC%8F%E4%BC%98%E5%8C%96&amp;zhida_source=entity">分布式优化</a>，那么传统分布式服务（web 服务、图像 CNN 推理服务）已经发展与实用了很多年了，服务体验也没啥好说的，那么，当下 LLM 推理的优化问题又与他们又什么本质区别呢？简单说，其本质区别就在于不定长度（且长度差异巨大，同时不可预测）的输入和输出。</p>
<hr>
<h2>批处理的重要性：</h2>
<p>批处理也就是同一份参数可以同时一次与多个计算向量共同计算，在芯片底层运算时则是读取一次参数在 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=SRAM&amp;zhida_source=entity">SRAM</a> 中多次计算，在访存带宽受限的情况下，一次读取数据的延时是远长于多次计算的，所以 batch 计算时是非常节省访存时间的。我们所熟悉的 FlashAttention 的本质方法即在 attention 计算时能够尽量使得分块计算的访存复用率最高。</p>
<p>并且，batch 批处理的会出现较大尺寸的<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95&amp;zhida_source=entity">矩阵乘法</a>，在芯片底层还会有更加有效的 GEMM 矩阵加速方法，使得矩阵加速效率更高。</p>
<p>限制于大模型自回归方法的特点，未加优化的单请求一定是 batch size = 1 的矩阵乘法过程。我们可以将单请求的矩阵乘法 GEMM ，其实严格说是 GEMV ，在 llama2-7B 中视为 1x4096 的 X 矩阵（向量）与 4096x4096 的 Wq/k/v 的矩阵相乘，而尺寸为 batch size 为 n 的请求批次视为 nx4096 与 4096x4096 的矩阵相乘。</p>
<p>所以，大规模 request 调度中一个非常重要的发力点就是如何把不同的 request 中的每个 X 向量组织为 batch size 为 n 的批次，同时满足 1）n 尽量大，2）整体逻辑灵活。1）好理解，2）则是因为每个请求达到时间不同，简单粗暴的组织会造成等待的请求增加延时时间，而过于细致的组织会让整个推理服务系统的调度负担，也就是我们常说的 overhead，过重，因为完全的求解最优调度问题可能是 NP 问题。</p>
<h2>大规模 Request 调度的主要的优化思路和方法：</h2>
<p>在 CMU 综述 <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2312.15234">Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems (arxiv 2312.15234)</a> 中，request schedule 出在 3.2.4 一节，与 3.2.1 Low-bit Quantization、3.2.2 Parallel Computation、3.2.3 Memory Management、3.2.5 Kernel Optimization 并列。同属于 3.2 System Optimization 。这篇我们比较熟悉了，<a href="https://www.zhihu.com/question/591646269/answer/3360933376">Hsword：大模型推理加速技术的学习路线是什么?</a> <a href="https://www.zhihu.com/question/591646269/answer/3380020160">刀刀宁：大模型推理加速技术的学习路线是什么?</a> 这几个回答中都是集中讨论的。</p>
<p>另外，在论文 <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2401.17644">Towards Efficient and Reliable LLM Serving: A Real-World Workload Study (ArXiv 2401.17644)</a> ，也就是前文所述的 BurstGPT 中，也对 batching 等技术进行了一定的分析。</p>
<p>在另一篇中山大学、鹏程实验室等的综述 <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2401.02643">Training and Serving System of Foundation Models: A Comprehensive Survey (arxiv 2401.02643)</a> 中，其分类方法和特点与 CMU 综述中的方法并不相同，会会更加细致一些，值得借鉴。</p>
<p><img src="https://pic4.zhimg.com/v2-eb1c85372ee2e4c2d50faca48106411f_r.jpg" alt=""></p>
<p>Training and Serving System of Foundation Models: A Comprehensive Survey （中山大学、鹏程实验室等）中对推理服务系统中的方法分类</p>
<h3></h3>
<p>Request Batch Scheduling：</p>
<p>这里我们重点分析各种 batching 技术，也就是<strong>如何将一大波在相对较短时间内发生同时不能确定长度的请求进行细粒度拆分后再组合调度优化</strong>的基本方法。</p>
<p>第<strong>一</strong>个问题，我们为什么需要 batch？由于推理过程是 memory-bound，在 batch_size 较小时（b&lt;4），模型权重加载占据主要时间花费，增加 batch_size 不会造成太大推理延迟变化，能带来成倍的吞吐量提升。在 <a href="https://link.zhihu.com/?target=https%3A//github.com/ray-project/llm-numbers%3Ftab%3Dreadme-ov-file">llm-numbers文章</a>（<a href="https://link.zhihu.com/?target=http%3A//brenocon.com/dean_perf.html">Jeff Dean numbers</a>）（<a href="https://link.zhihu.com/?target=https%3A//medium.com/%40greg.broadhead/a-brief-guide-to-llm-numbers-parameter-count-vs-training-size-894a81c9258">另一篇类似的文章</a>）中表明通过有效的对请求进行组 batch，可以把吞吐量提升 10 倍以上，在实际场景中，理论计算和评测都证实了上述结论。为了优化服务吞吐、提升资源利用率，组batch 策略是其中最重要的部分。第<strong>二</strong>个问题，像传统推理方法一样直接把同时的请求合并成一组 batch 不行吗？不可以，原因就是因为一来 request 长度不确定，二来 response 长度也不确定，三来两者还没有必然的相关性关系。这样的话，prefill 阶段没法直接合并，<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=decode+%E9%98%B6%E6%AE%B5&amp;zhida_source=entity">decode 阶段</a>也没法合并，都对不齐。第<strong>三</strong>个问题，细粒度需要多细的粒度？第<strong>四</strong>个问题，如何进行再<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96&amp;zhida_source=entity">组合优化</a>？第<strong>五</strong>个问题，该过程中内存空间是否还有进一步优化的空间？</p>
<p>后续这几个问题就是大部分方法需要处理的核心问题了。</p>
<p>我们来看一下，论文中经常出现的名称有<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E8%BF%9E%E7%BB%AD%E6%89%B9%E5%A4%84%E7%90%86&amp;zhida_source=entity">连续批处理</a>（continuous batching），动态批处理（dynamic batching），或者是飞行中批处理（inflight batching），我之前一直想搞清楚这几个概念的具体区别，但后来我发现没有太大必要，其实他们都是一样的大体逻辑，只是策略层面略有不同，以及细节上有区别，并且有的时候还会有叫法的差异。本质上说，他们都有一个共同特点，就是都是迭代级调度（iteration-level scheduling）的批处理。</p>
<p>不过，我们现在需要区分的则是静态和动态的两种方式。</p>
<p>好，这里 <a href="https://link.zhihu.com/?target=https%3A//www.anyscale.com/blog/continuous-batching-llm-inference">经典配图</a> 又来了。</p>
<p><img src="https://pic3.zhimg.com/v2-c5973421f5140923b8cefd7e1925ce10_r.jpg" alt=""></p>
<p>关于 Naive batching / static batching / Continuous batching 的手绘对比图。</p>
<p>这时我们用坐大巴车或者坐电梯来打比方，一般<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E9%9D%99%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86&amp;zhida_source=entity">静态批处理</a>就是所有人都一起上车，中间有人下车的话就空出来，一直等最后一个人下完再开始发下一班车。这样造成的空闲资源是显而易见的。那么，对应的动态批处理就是有人下车就可以安排有人上车了，也就是中间过程中可以重新组织 batch，并且还会设置极限等待时间。显而易见，动态的 continuous batching 的好处是非常明显的，无论是在坐大巴车的时候还是在推理优化中，从整体角度看。（当然也有人讲如果把 dynamic-batching 比喻成坐大巴，那么continous-batching 可以比喻为自动扶梯，可以随时上下，这是粒度问题。）</p>
<p>而实际中，prefill 阶段（黄色块）和 decode 阶段（蓝色块）的运算模式还有一些差异，decode 每次会生成一个 token 输出，而 prefill 阶段是<strong>逻辑上</strong>是同时输出的。并且说 prefill 阶段可以是高度并行的，是计算密集的，但为了调度灵活，有些工作可以将 prefill 拆为和 decode 阶段类似的方式，方便对齐和组织；而 decode 阶段是高度串行的，是访存密集的。而组织成为较大的 batch 之后，则可以有效的将<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E8%AE%BF%E5%AD%98%E5%AF%86%E9%9B%86%E5%9E%8B&amp;zhida_source=entity">访存密集型</a>的计算模式转换为平衡模式的。</p>
<p>接下来借用 <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2312.15234">Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems (arxiv 2312.15234)</a> 综述中的文本对已有的方法进行一下总结。早期的 LLM 服务系统（NVIDIA Triton 上的 FasterTransformer ）仅支持请求级别的调度，这与之前的方法类似。Orca 是在 OSDI22 应是第一个提出 continuous batching 的方法，考虑到可变的输出序列长度，将生成式 LLM 请求调度级别变为 token，它的方法是以先到先服务（<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=FCFS&amp;zhida_source=entity">FCFS</a>）的顺序执行该粒度，<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=Huggingface&amp;zhida_source=entity">Huggingface</a> 的 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=TGI&amp;zhida_source=entity">TGI</a>（Text Generation Inference）就是该方案的开源实现。再就是例如 vLLM 和 RayLLM 中的连续批处理（continuous batching），以及 TensorRT-LLM 中的动态批处理。而大名鼎鼎的 PagedAttention 则是 vLLM 中实现的一种新的注意力机制（它从传统的<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E6%A6%82%E5%BF%B5&amp;zhida_source=entity">操作系统概念</a>如分页和虚拟内存中汲取灵感，允许 KVCache 是非连续的通过在固定大小的 page 进行分配），多说一嘴，其实 PagedAttention 是非常典型的代表了 system 领域结合硬件体系结构的研究方法的。<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=FastServe&amp;zhida_source=entity">FastServe</a> 专注于作业完成时间（JCT），并涉及迭代级别的抢占，以优先处理输入长度较短的请求，而不是FCFS 。SARATHI 针对<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%88%86%E5%B8%83%E5%BC%8F%E6%8E%A8%E7%90%86&amp;zhida_source=entity">分布式推理</a>中由变长输入请求的初始迭代引起的管道泡沫，为了充分利用 GPU 计算资源，它将输入提示分成统一的块，并在可能的情况下将块槽与其他请求的解码迭代一起使用，这也被 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=DeepSpeed-FastGen&amp;zhida_source=entity">DeepSpeed-FastGen</a> 称为 （ Dynamic SplitFuse ）动态分割融合所采用。S3 涉及输出序列长度预测器，并帮助在GPU内存约束条件下调度更多并发请求，以获得更大的批处理大小和更高的<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E6%8E%A8%E7%90%86%E5%90%9E%E5%90%90%E9%87%8F&amp;zhida_source=entity">推理吞吐量</a>。</p>
<p>额外我觉得比较有特点的几个研究细分的点，还有但不限于：1）类似 <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2311.18677">splitwise</a> 的粗粒度异构流水线机制，他的方法中将 prefill 阶段放在访存相对较慢或者本身较老版本的显卡上，而将 decode 放在访存较快或者较为新发布的显卡上，以做到尽量发挥两者的硬件优势。2）<a href="https://zhuanlan.zhihu.com/p/689685429">Adobe Nirvana</a> 利用 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=diffusion&amp;zhida_source=entity">diffusion</a> 模型的服务系统中不同请求在不同时间片上可能的复用机制提出的缓存加速方法，这种方法只有大规模请求系统下才会出现。3）将量化方法和服务系统设计进行 co-design，也就是 <a href="https://zhuanlan.zhihu.com/p/697465029">MIT 韩松组 2305 arxiv 的最新文章 QServe</a>，我在另外一篇博客中进行了一部分都的分析：<a href="https://zhuanlan.zhihu.com/p/691537237">刀刀宁：量化那些事之 KVCache 的量化</a>。4）结合 speculate inference 的方法，</p>
<h2>经典论文分析：</h2>
<p>本文写到这里我一度不知道该怎么写下去了，因为好像技术点就是这样的，但是又感觉远远不能完成对技术的把握。所以这里再展开几篇文章进行一下分析，权当是一个论文小合订级。总的来说，这些文章的面世时间都不长，都是最近一年左右的时间内发布出来的。并且随着本博文的写作，这个列表也一再增长，还欢迎小伙伴们补充新鲜论文。</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT-LLM">https://github.com/NVIDIA/TensorRT-LLM</a> ，也就是 <a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/FasterTransformer">FasterTransformer</a>， 没有 release 论文；</li>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/ray-project/ray-llm">https://github.com/ray-project/ray-llm</a> ，好像也没有 release 论文；</li>
<li>vllm：Efficient Memory Management for Large Language Model Serving with PagedAttention. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2309.06180">arXiv 2309.06180</a></li>
<li>FastServe：Fast Distributed Inference Serving for Large Language Models. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2305.05920">arXiv 2305.05920</a></li>
<li>DeepSpeed-FastGen: High-throughput Text Generation for <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=LLMs&amp;zhida_source=entity">LLMs</a> via MII and DeepSpeed-Inference <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2401.08671">arXiv 2401.08671</a></li>
<li>Efficient Streaming Language Models with Attention Sinks. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2309.17453">arXiv 2309.17453</a></li>
<li>SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2308.16369">arXiv 2308.16369</a></li>
<li>DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2401.09670">arXiv 2401.09670</a></li>
<li>AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2302.11665">arXiv 2302.11665</a></li>
<li>SpotServe: Serving Generative Large Language Models on Preemptible Instances. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2311.15566">arXiv 2311.15566</a></li>
<li>LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2308.16137">arXiv 2308.16137</a></li>
<li>SHEPHERD: Serving DNNs in the Wild. <a href="https://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/nsdi23-zhang-hong.pdf">NSDI, 2023</a></li>
<li>Splitwise: Efficient generative LLM inference using phase splitting. <a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2311.18677">arXiv 2311.18677</a></li>
<li>TurboTransformers: An Efficient GPU Serving System For Transformer Models. <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2010.05680">arXiv 2010.05680</a></li>
</ul>
<p>本博文对于诸多论文的整理相对来讲比较粗浅，欢迎讨论。</p>
<h3><strong>vLLM，PagedAttention：</strong></h3>
<p>Efficient Memory Management for Large Language Model Serving with PagedAttention，作者来自Berkeley、Stanford、以及 UC San Diego。论文提出了一种名为 PagedAttention 的注意力算法，该算法受到<strong>操作系统中虚拟内存和分页技术</strong>的启发，旨在解决<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89&amp;zhida_source=entity">大型语言模型（LLM）</a>服务中的高吞吐量需求和内存管理问题。在文章中还提出来一个 vLLM 的 LLM 服务系统，实现了近乎零浪费的 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=KV+%E7%BC%93%E5%AD%98&amp;zhida_source=entity">KV 缓存</a>内存，并在请求内部和跨请求灵活共享 KV 缓存，进一步减少内存使用。</p>
<p><img src="https://pic1.zhimg.com/v2-f09e778360b02e1f516da3979e3df2d8_r.jpg" alt=""></p>
<p>本文其实被讨论的很多了，核心问题在于简单的按 token 来划分策略的方法固然好，但是过于细碎，但是管理 KVCache 内存方面存在效率低下的问题，导致<strong>内存碎片化</strong>和冗余复制，会<strong>限制批处理大小</strong>。PagedAttention 算法允许将请求的 KV 缓存分成块，每块包含固定数量的令牌的注意力键和值，PagedAttention 允许在非连续的内存空间中存储连续的键和值。最后 vLLM 在保持相同延迟水平的情况下，与现有的最先进的系统（如FasterTransformer 和 Orca）相比，提高了 LLM 服务的吞吐量 2-4 倍。</p>
<h3>FastServe：Fast Distributed Inference Serving for Large Language Models</h3>
<p>Fast Distributed Inference Serving for Large Language Models，作者主要来自北大。这是一个典型的 LLMs 分布式推理服务系统，核心提出了一个输出 token 抢占机制 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=preemptive&amp;zhida_source=entity">preemptive</a>，使用一种多级反馈队列（MLFQ）调度器，核心的技术点是用的 skip-join 方法，来最小化工作完成时间（JCT），也是为了衡量服务系统中延时相应时间的一个标准。</p>
<p>文章中分析，由于现在 decode 解码方式下，工作负载大小未知，我们前面也着重分析了，因此不能直接应用最短剩余时间优先调度算法 SRPT，这是公式。为了解决这个问题，FastServe 提出了使用最小已服务时间 LAS 来近似 SRPT，并且系统中有多个队列，并且每个队列都有不同的优先级，以应付大批量请求时的复杂需求。所以本文提出了一种新型的调度器 skip-join 跳过/加入多级反馈队列（MLFQ）调度器。简单说就是调度器在完成当前处理批次的任务的迭代后，会抢占（preemptive）这个任务，并调用一个其他的符合条件的任务（一般是新到的）过来优先处理，并构建下一轮迭代的新任务批次。</p>
<p><img src="https://pic3.zhimg.com/v2-72ae73d352087fcf48f5e4dcabfcce66_r.jpg" alt=""></p>
<p>文中举例比较了先到先服务、普通多级反馈队列、Skip-Join 多级反馈队列，还有完全信息条件下的调度方法</p>
<p>为了达到更好的效果，对于什么时候需要抢占/跳过，什么时候插入新任务，就变得非常敏感和重要。因此论文提出一系列计算方法实时获得最小已服务时间、剩余时间预测、以及调度优先级，进行调度。我对该问题的理解就是有限制条件下的不完全信息条件下调度问题，很多经典方法都可以利用。</p>
<p>在前述方法之下，还会带来相关的 KVCache 管理问题，论文也做了相关的管理器设计和<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%88%86%E5%B8%83%E5%BC%8F%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E&amp;zhida_source=entity">分布式执行引擎</a>。</p>
<h3><strong>DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</strong></h3>
<p>DeepSpeed-FastGen 是 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=deepspeed&amp;zhida_source=entity">deepspeed</a> 框架中专门进行推理和服务的框架，是<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=DeepSpeed-MII&amp;zhida_source=entity">DeepSpeed-MII</a>和DeepSpeed-Inference的协同组合，代码发布时间已经很久了，仓库的维护和迭代都非常完善。论文是在 2401 上的 arxiv ，我们以论文为基础展开我们的分析，例如其核心展示方法：批处理调度的组合策略 Dynamic SplitFuse。</p>
<p>文中提出这样几个发现：1）输入的 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=batchsize&amp;zhida_source=entity">batchsize</a> 对性能相比 token 长度的影响微乎其微，这个可不是什么突出的发现，这其实是显而易见的，我们前面分析的，batchsize 增加不会改变芯片处理时的访存墙状态，QServe 论文中给出 A100 在 batchsize &lt; 78 之内参数的传输延时都比计算处理时间要长。2）token 数较少时吞吐率较容易优化，当 token 数增加到一定阶段后吞吐率不是很容易再优化了。3）如何在多个 forward 中调度一批 token ，是个很重要的问题，本文的思路是，如果太长的 token 序列，最好分开到不同的 batch 中。结合前三个发现，本文的主旨换言之就是一个 batch 尽量大，但是不要太长，太长的后边再做。</p>
<p><img src="https://pic4.zhimg.com/v2-684aa872422af708e4600e8f889fc513_r.jpg" alt=""></p>
<p>Dynamic Splitfuse 就是这个意思，不要太长就 split 开，但是当下的其他任务我们还是把大家组合在一起一批处理掉，因为限制了最长的 batch 长度，所以所有的处理单元的长度都一样，也方便调度。</p>
<h3>Efficient Streaming Language Models with Attention Sinks</h3>
<p>又称 streamingLLM ，大名鼎鼎的。不过这篇文章中对于 request batch 问题分析不多，这篇文章最大的突出贡献是“注意力汇聚点”（attention sink）现象，以及根据这个现象提出的<strong>留头留尾不留中</strong>的 Attention 窗口设置，也就是只需保留注意力池 token 的 KV 值（只需 4 个初始 token 即可）和<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3&amp;zhida_source=entity">滑动窗口</a>的 KV 值，就能锚定注意力计算并稳定模型的性能。因为中间大部分 Attention 计算被“省略”了，自然会快。</p>
<p><img src="https://pica.zhimg.com/v2-a3c6eb76782d70fc78cbbce964174398_r.jpg" alt=""></p>
<p>本文的解读和讨论是非常丰富的，<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/qIwYJAfD5bXJS6j0xtp2cw">最多400万token上下文、推理提速22倍，StreamingLLM火了，已获GitHub 2.5K星</a>， <a href="https://zhuanlan.zhihu.com/p/659875511">方佳瑞：LLM推理技术之StreamingLLM：如何拥有无限长生成能力</a>，<a href="https://zhuanlan.zhihu.com/p/660119736">Hyacinth：模型解释思考：StreamingLLM原理探究-这个能够接受近乎无限长度文本的大模型结构本质是什么？</a>，最近也是高中 ICLR2024。至于为什么可以留头留尾不留中，作者给出的解释是初始标记接收的是多余的注意力分数，听上去是合理的，但是我是存疑的。既然理论上还是不能完美解释，还需要更多后续工作来进行研究。</p>
<p>我认为 <a href="https://zhuanlan.zhihu.com/p/659875511">方佳瑞：LLM推理技术之StreamingLLM：如何拥有无限长生成能力</a> 中的解释最为合理：在Attention机制中，Softmax的输出代表了 key/query 的匹配程度的概率。因此，如果softmax在某个位置的值非常大，那么在反向传播时，这个位置的权重就会被大幅度地更新。然而，有时候attention机制并不能确定哪个位置更值得关注，但由于Softmax需要所有位置的值的总和为 1，因此必须“表态”给某些位置较大的权重，这就可能导致错误的权重更新，而<strong>这个错误在后续的过程中很难被纠正</strong>。</p>
<h3>LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models</h3>
<p>作者主要来自 University of Illinois Urbana-Champaign, Meta, GenAI Meta。</p>
<p>本文可以和 StreamingLLM 形成一组，研究的问题和思考的方向都大致相同。它引入了 Λ 形注意力掩码（Λ-shaped attention mask）和距离限制来避免过多的注意力标记和未见距离，而无需参数更新或学习。你没看错，这个 Λ 不是 A ，就是 Λ （不知道念个啥）。像不像 StreamingLLM 配图中的 d，对，就是留头留尾不留中的这个 Λ 。</p>
<p>本文通过理论和实证分析处理长文本时出现的跨分布（OOD, Out-of-Distribution）问题，来解释这个方法的原因，而导致长度泛化失败的三个主要因素：未见距离（unseen distances）：当文本变得太长时，一些距离值会增加到未见过的大数值，最终超过预训练中见过的距离；未见的注意力下标记数量（unseen number of tokens under attention）：当文本变得更长时，后续标记需要关注更多的标记，这可能会稀释注意力权重，使注意力分布更加平坦，从而导致信息丢失；隐式编码的位置信息（implicitly encoded positional information）：当文本长度超过训练<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E8%AF%AD%E6%96%99%E5%BA%93&amp;zhida_source=entity">语料库</a>中见过的长度时，初始标记的位置信息处理可能会出现扭曲或丢失。所以这里就和 StreamingLLM 在位置编码方面有一些区别。</p>
<p><img src="https://pic1.zhimg.com/v2-d6d19e213b7bc92dd4dabedbb10f76ec_r.jpg" alt=""></p>
<p>以上两篇文章都依赖于一篇早期文章，先记下来以后研究：Longformer: The Long-Document Transformer，<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2004.05150">arXiv 2004.05150</a></p>
<h3><strong>SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills</strong></h3>
<p>作者主要来自 Microsoft Research India 和 Georgia Institute of Technology，萨拉蒂这个词是印地语，大概好像是战车夫的意思，似乎还有些印度宗教的含义在里面，作者可能是想将自己方法比作成一种大杀四方的优秀推理服务工具。</p>
<p>文章主要思路是通过将 prefill 阶段分解为等大小的块 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=chunk&amp;zhida_source=entity">chunk</a>ed-prefills，并使用单个 chunk 与其他多个 decode 请求（也就是流水线前面请求的 decode）构建批次，文中叫做 decode-maximal batching ，从而提高了GPU的计算利用率，并减少了流水线气泡（pipeline bubbles）。也是一种不错的调度思路，在这个 setting 下进行调度</p>
<p><img src="https://pic4.zhimg.com/v2-6ed86296228ffed424272f79bcd50511_r.jpg" alt=""></p>
<h3>Splitwise: Efficient generative LLM inference using phase splitting</h3>
<p>作者主要来自，University of Washington 和 Microsoft，</p>
<p>本质上这篇文章也是流水线并行的，它将具有不同的延迟、吞吐量、内存和功耗特性的<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E8%AE%A1%E7%AE%97%E5%AF%86%E9%9B%86%E5%9E%8B&amp;zhida_source=entity">计算密集型</a>的提示计算（prompt computation）和内存密集型的标记生成（token generation）分开进行异构流水线并行。比较有意义的就是 decode 需要更快的卡，而 prefill 则可以用较老的显卡，这样还让可能闲置的显卡可以利用起来。这和 sarathi 类似。当然这个文章最大的挑战就是将前续阶段生成的 KVCache 状态 <a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=embedding&amp;zhida_source=entity">embedding</a> 传输到 decode 机器或者显卡上，这里只能用更快的传输卡了。</p>
<h3><strong>DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</strong></h3>
<p>作者主要来自 Peking University、 UC San Diego。其中有三位作者和前面分析的 FastServe 是相同的。</p>
<p>DistServe 为了消除了预填充和解码之间的干扰，将预填充和解码计算分配给不同的 GPU，并充分考虑到额外带来的通信开销。和 FastServe 相类似的，还提出了新的延时评价标准：prefill 阶段的第一个令牌时间（TTFT，Time to First Token）和 decode 阶段每个输出令牌时间（<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=TPOT&amp;zhida_source=entity">TPOT</a>，Time Per Output Token），并根据 TTFT 和 TPOT，为每个阶段共同优化资源分配和并行策略。</p>
<h3><strong>AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving</strong></h3>
<p>本文作者主要来自 UC Berkeley, Peking University, University of Pennsylvania, Stanford University, Google, UC San Diego，其中，有几个作者也是 FastServe 与 DistServe 的作者，有几个作者是 PagedAttention 和 FlexGen 的作者，以及 SHEPHERD 的作者。摩拜摩拜。</p>
<p>AlpaServe 通过模型并行来实现多个设备间的统计<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8&amp;zhida_source=entity">多路复用</a>，即使单个模型可以适合单个设备。揭示了<strong>模型并行</strong>性开销和利用统计多路复用减少服务延迟之间的基本权衡，还给出了一种有效的策略来放置和并行化<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4&amp;zhida_source=entity">分布式集群</a>中的大深度学习模型集合。并在论文中发布了一个系统：AlpaServe2，一个<strong>自动</strong>探索不同并行化和放置策略之间权衡的模型服务系统，这就类似 automl 的思路了，可以按集群资源约束，根据模型和周期性统计数据指定的工作负载<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6&amp;zhida_source=entity">配置文件</a>，对模型进行分割和放置，并调度请求以优化 SLO（<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E6%9C%8D%E5%8A%A1%E7%AD%89%E7%BA%A7%E7%9B%AE%E6%A0%87&amp;zhida_source=entity">服务等级目标</a>）。</p>
<h3>SpotServe: Serving Generative Large Language Models on Preemptible Instances</h3>
<p>作者主要来自 Carnegie Mellon University, Peking University, The Chinese University of Hong Kong ， 和前文 survey 是同一个团队。</p>
<p>SpotServe 是一个在云平台上抢占式 GPU 实例（preemptible GPU instances）来降低生成大型语言模型（LLMs）服务成本的系统，是通过用户视角来考虑问题，这是个比较独特的系统服务问题，setting 与云平台运营商视角下的整体调度是不同的。因为用户权限较低，所以这些 instances 比常规实例便宜得多的价格，但可能会被云服务提供商随时抢占。SpotServe 通过动态调整 LLM并行化配置、最小化实例迁移成本，并利用云平台提供的宽限期（grace period），实现了在便宜实例上的快速可靠服务。还通过将实例迁移问题表述为<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E4%BA%8C%E5%88%86%E5%9B%BE%E5%8C%B9%E9%85%8D&amp;zhida_source=entity">二分图匹配</a>问题，并使用 Kuhn-Munkres 算法来最小化迁移成本。</p>
<h3>SHEPHERD: Serving DNNs in the Wild</h3>
<p>作者主要来自 University of Waterloo, Yale University, UC Berkeley，文章是 2022 年左右的工作，主要面向的是 CNN、BERT 等 CV、<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=NLP+%E6%A8%A1%E5%9E%8B&amp;zhida_source=entity">NLP 模型</a>为主的推理服务系统，对于当下 LLM 推理服务系统是有借鉴意义的。虽然单次服务的长度可预测，但是总体的请求总负载不可精确预测。Shepherd 是牧羊人的意思，名如文意，挺好。</p>
<p>SHEPHERD 期望可以同时保证高系统吞吐量和最大化资源利用率，作者设计了一个两级结构来应对工作负载不可预测性的挑战，将模型服务分解为规划和服务模块。在规划方面，SHEPHERD 利用将个别请求流聚合成中等大小的组可以显著提高可预测性。在服务方面，SHEPHERD采用了一种新颖的在线算法，通过巧妙地利用抢占和模型特定的批处理特性，在工作负载不可预测性的情况下提供保证的吞吐量。</p>
<p>这篇价值是比较大的，未来还需要再研究研究。</p>
<h3>TurboTransformers: An Efficient GPU Serving System For Transformer Models.</h3>
<p>这篇来自腾讯的论文是 2020 年发表出来的，<strong>非常有前瞻性</strong>。他们在 2020 年就提出了 Transformer 结构引入了更多计算对服务的延迟和吞吐量带来的挑战，以及 NLP 任务接收长度可变的输入序列给内存管理和服务优化带来的严重问题。并且提出了高效的 batch 批量归约方法和动态规划的调度方法，还给出了一种内存分配算法来适用变长输入情况，来确保最优吞吐量。赞 Vision 啊！</p>
<h3>对以上论文合订本的一点总结：</h3>
<p>这些论文都在探索如何更高效地处理和服务于大型深度学习模型，特别是在面对计算资源限制和高效率需求时。每篇论文都提出了不同的方法和技术来优化模型的服务和推理过程。</p>
<p>这几篇文章有几个共同的特点：</p>
<ol>
<li>本质问题都是调度问题，可以将调度问题抽象为一个最优优化问题，无论是在线还是离线；</li>
<li>prefill 阶段和 decode 阶段的区分对待以及合并操作等；</li>
<li>异构硬件环境的适配；</li>
<li>在推理阶段尝试在训练中已经验证过很好用的 3D Parallel 中的模型并行、流水线并行等方法；</li>
<li>结合其他问题 setting、结合传统任务、结合 diffusion 等新特点的任务；</li>
</ol>
<h2><strong>各家新鲜框架对分布式优化的支持：</strong></h2>
<p>本节是一个跳不过去的章节，可以说，当前主流的 TGI、vLLM、Ray-serve、fastllm、llama.cpp、deepspeed 等等服务框架都对本文中描述的诸多特性都分别进行了支持。框架是很重要的东西，大家都有自己的想法，很多人就决定自己写一个，有的人基于 hf 的 transformer，有的人基于 torch，有的人甚至直接从 c、<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=cuda&amp;zhida_source=entity">cuda</a> 撸起。还有人为了一个 idea 或者一个模型而写一个系统，也是很正常的，所以卖点不是库，而是那个 idea，比如 vllm 的 paged attention机制。</p>
<p>所以说目前整个行业都还在<strong>试错阶段</strong>，所以呈现百花齐放的景象，最终应该还是会有集大成者胜出，他们可以吸收其他一些框架的 idea，毕竟可能有些框架除了这些 idea 之外有其他内伤。我个人目前比较看好 llama.cpp 和deepspeed，不过 llama.cpp 分布式支持可能会有天然短板，所以从目前看 deepspeed 的架构布局、微软的分布式积累、开发实力，都是最强的，消化吸收其他框架的优势很强。另外微软在大模型时代通过 deepspeed 占领软件生态先机，才是他真正雄心勃勃的目标。至于 llama.cpp 等<a href="https://zhida.zhihu.com/search?content_id=238719040&amp;content_type=Article&amp;match_order=1&amp;q=%E8%BD%BB%E9%87%8F%E7%BA%A7%E6%A1%86%E6%9E%B6&amp;zhida_source=entity">轻量级框架</a>，清晰的架构、优秀的易读性都有它非常可取之处，作为学习框架和小规模部署框架来说比较合适，当然目前 llama.cpp 的 server 部分代码更新也很快，还有就是 llama.cpp 更新实在是太快了，代码乱的很，可能会阻碍它未来的发展。</p>
<hr>
<p>几篇推理服务相关推荐的文章：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/675982217">猛将兄：大模型推理-2-推理引擎和服务性能优化</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/676652273">胡登军：大模型推理-5-大模型推理优化之缓存及调度</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/666452391">尚晋：大语言模型推理加速技术：计算加速篇</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/664106696">爱吃牛油果的璐璐：大模型LLM推理框架整理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/670980266">TanyoKwok 郭天佑：六问大模型推理算法和系统</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/691837238">简枫：如何提升大模型推理服务的吞吐率？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/675169864">thanky：chat gpt大模型推理部署的三种scheduling策略分析</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/676109470">方佳瑞：大模型推理核心技术之Continuous Batching和我的WXG往事</a></li>
<li>Hsword的回答 - <a href="https://www.zhihu.com/question/637480772/answer/3370398015">2024年大模型基础设施领域（训练、推理、硬件）有什么值得关注研究方向？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/682074949">杨军：龙年快乐</a> 部分关于 TensorRT-LLM</li>
<li><a href="https://zhuanlan.zhihu.com/p/613244988">杨军：谈谈对OpenAI Triton的一些理解</a></li>
</ul>
<p>接下来准备在写写 speculatvie decode、MoE 推理调度方面的技术，先准备几篇作为笔记：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/684217993">灰瞳六分仪：Speculative Decoding 论文阅读合订本</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/651359908">方佳瑞：大模型推理妙招—投机采样（Speculative Decoding）</a></li>
</ul>
<p>我关于大模型推理技术和量化的笔记列表，防迷路：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/697465029">刀刀宁：量化那些事之 QServe</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/691537237">刀刀宁：量化那些事之 KVCache 的量化</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/696542303">刀刀宁：量化那些事之 AdaRound/BRECQ/QDrop</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/693474068">刀刀宁：量化那些事之 Diffusion 量化</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/693636710">刀刀宁：量化那些事之 ViT 的量化</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/680578625">刀刀宁：再磕：GPTQ、SparseGPT与Hessian矩阵</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/683215538">刀刀宁：量化那些事之FP8与LLM-FP4</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/684215316">刀刀宁：量化那些事之AWQ</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/684297101">刀刀宁：量化那些事之QARepVGG</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/684658121">刀刀宁：量化那些事之BitNet-b1.58</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/670515231">刀刀宁：笔记：Llama.cpp 代码浅析（一）：并行机制与KVCache</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/671761052">刀刀宁：笔记：Llama.cpp 代码浅析（二）：数据结构与采样方法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/672289691">刀刀宁：笔记：Llama.cpp 代码浅析（三）：计算开销</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/672983861">刀刀宁：笔记：Llama.cpp 代码浅析（四）：量化那些事</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/668181423">刀刀宁：笔记：DeepSpeed inference 代码理解</a></li>
<li><a href="https://www.zhihu.com/answer/3380020160">大模型推理加速技术的学习路线是什么?</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/679376718">刀刀宁：再看大模型稀疏化：SparseGPT、Wanda</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/678891209">刀刀宁：Low-Rank Pruning of Llama2</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/675585887">刀刀宁：论文笔记：DejaVu、LLM in Flash、PowerInfer</a></li>
</ul>
<p>分布式计算</p>
<p>&gt;由 [Circle 阅读助手](https://circlereader.com) 生成</p>
