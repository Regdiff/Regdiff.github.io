<!DOCTYPE html><html><head><meta name="author" content="Circle 阅读助手" /><meta name="description" content="由 [Circle 阅读助手](https://circlereader.com) 生成" /><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"  /><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><style>html,body{padding:0;margin:0;}.card{min-height:100vh;}.container .ant-app{padding-right: 0 !important;}
.anticon {
  display: inline-flex;
  alignItems: center;
  color: inherit;
  font-style: normal;
  line-height: 0;
  text-align: center;
  text-transform: none;
  vertical-align: -0.125em;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

.anticon > * {
  line-height: 1;
}

.anticon svg {
  display: inline-block;
}

.anticon::before {
  display: none;
}

.anticon .anticon-icon {
  display: block;
}

.anticon[tabindex] {
  cursor: pointer;
}

.anticon-spin::before,
.anticon-spin {
  display: inline-block;
  -webkit-animation: loadingCircle 1s infinite linear;
  animation: loadingCircle 1s infinite linear;
}

@-webkit-keyframes loadingCircle {
  100% {
    -webkit-transform: rotate(360deg);
    transform: rotate(360deg);
  }
}

@keyframes loadingCircle {
  100% {
    -webkit-transform: rotate(360deg);
    transform: rotate(360deg);
  }
}
.css-16cpc8y a{color:#416ed2;text-decoration:none;background-color:transparent;outline:none;cursor:pointer;transition:color 0.3s;-webkit-text-decoration-skip:objects;}.css-16cpc8y a:hover{color:#305ab7;}.css-16cpc8y a:active{color:#305ab7;}.css-16cpc8y a:active,.css-16cpc8y a:hover{text-decoration:none;outline:0;}.css-16cpc8y a:focus{text-decoration:none;outline:0;}.css-16cpc8y a[disabled]{color:rgba(0, 0, 0, 0.25);cursor:not-allowed;}.css-16cpc8y[class^="ant-avatar"],.css-16cpc8y[class*=" ant-avatar"]{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y[class^="ant-avatar"]::before,.css-16cpc8y[class*=" ant-avatar"]::before,.css-16cpc8y[class^="ant-avatar"]::after,.css-16cpc8y[class*=" ant-avatar"]::after{box-sizing:border-box;}.css-16cpc8y[class^="ant-avatar"] [class^="ant-avatar"],.css-16cpc8y[class*=" ant-avatar"] [class^="ant-avatar"],.css-16cpc8y[class^="ant-avatar"] [class*=" ant-avatar"],.css-16cpc8y[class*=" ant-avatar"] [class*=" ant-avatar"]{box-sizing:border-box;}.css-16cpc8y[class^="ant-avatar"] [class^="ant-avatar"]::before,.css-16cpc8y[class*=" ant-avatar"] [class^="ant-avatar"]::before,.css-16cpc8y[class^="ant-avatar"] [class*=" ant-avatar"]::before,.css-16cpc8y[class*=" ant-avatar"] [class*=" ant-avatar"]::before,.css-16cpc8y[class^="ant-avatar"] [class^="ant-avatar"]::after,.css-16cpc8y[class*=" ant-avatar"] [class^="ant-avatar"]::after,.css-16cpc8y[class^="ant-avatar"] [class*=" ant-avatar"]::after,.css-16cpc8y[class*=" ant-avatar"] [class*=" ant-avatar"]::after{box-sizing:border-box;}.css-16cpc8y.ant-avatar{box-sizing:border-box;margin:0;padding:0;color:#fff;font-size:14px;line-height:1.5714285714285714;list-style:none;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';position:relative;display:inline-flex;justify-content:center;align-items:center;overflow:hidden;white-space:nowrap;text-align:center;vertical-align:middle;background:rgb(127,127,127);border:1px solid transparent;width:32px;height:32px;border-radius:50%;}.css-16cpc8y.ant-avatar-image{background:transparent;}.css-16cpc8y.ant-avatar .ant-image-img{display:block;}.css-16cpc8y.ant-avatar.ant-avatar-square{border-radius:6px;}.css-16cpc8y.ant-avatar.ant-avatar-icon{font-size:18px;}.css-16cpc8y.ant-avatar.ant-avatar-icon >.anticon{margin:0;}.css-16cpc8y.ant-avatar-lg{width:40px;height:40px;border-radius:50%;}.css-16cpc8y.ant-avatar-lg.ant-avatar-square{border-radius:8px;}.css-16cpc8y.ant-avatar-lg.ant-avatar-icon{font-size:24px;}.css-16cpc8y.ant-avatar-lg.ant-avatar-icon >.anticon{margin:0;}.css-16cpc8y.ant-avatar-sm{width:24px;height:24px;border-radius:50%;}.css-16cpc8y.ant-avatar-sm.ant-avatar-square{border-radius:4px;}.css-16cpc8y.ant-avatar-sm.ant-avatar-icon{font-size:14px;}.css-16cpc8y.ant-avatar-sm.ant-avatar-icon >.anticon{margin:0;}.css-16cpc8y.ant-avatar >img{display:block;width:100%;height:100%;object-fit:cover;}.css-16cpc8y.ant-avatar-group{display:inline-flex;}.css-16cpc8y.ant-avatar-group .ant-avatar{border-color:#ffffff;}.css-16cpc8y.ant-avatar-group >*:not(:first-child){margin-left:-8px;}.css-16cpc8y.ant-avatar-group-popover .ant-avatar+.ant-avatar{margin-left:4px;}.css-16cpc8y.ant-wave{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-wave::before,.css-16cpc8y.ant-wave::after{box-sizing:border-box;}.css-16cpc8y.ant-wave [class^="ant-wave"],.css-16cpc8y.ant-wave [class*=" ant-wave"]{box-sizing:border-box;}.css-16cpc8y.ant-wave [class^="ant-wave"]::before,.css-16cpc8y.ant-wave [class*=" ant-wave"]::before,.css-16cpc8y.ant-wave [class^="ant-wave"]::after,.css-16cpc8y.ant-wave [class*=" ant-wave"]::after{box-sizing:border-box;}.css-16cpc8y.ant-wave{position:absolute;background:transparent;pointer-events:none;box-sizing:border-box;color:var(--wave-color, #416ed2);box-shadow:0 0 0 0 currentcolor;opacity:0.2;}.css-16cpc8y.ant-wave.wave-motion-appear{transition:box-shadow 0.4s cubic-bezier(0.08, 0.82, 0.17, 1),opacity 2s cubic-bezier(0.08, 0.82, 0.17, 1);}.css-16cpc8y.ant-wave.wave-motion-appear-active{box-shadow:0 0 0 6px currentcolor;opacity:0;}.css-16cpc8y.ant-wave.wave-motion-appear.wave-quick{transition:box-shadow 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.35s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-tag{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-tag::before,.css-16cpc8y.ant-tag::after{box-sizing:border-box;}.css-16cpc8y.ant-tag [class^="ant-tag"],.css-16cpc8y.ant-tag [class*=" ant-tag"]{box-sizing:border-box;}.css-16cpc8y.ant-tag [class^="ant-tag"]::before,.css-16cpc8y.ant-tag [class*=" ant-tag"]::before,.css-16cpc8y.ant-tag [class^="ant-tag"]::after,.css-16cpc8y.ant-tag [class*=" ant-tag"]::after{box-sizing:border-box;}.css-16cpc8y.ant-tag{box-sizing:border-box;margin:0;padding:0;color:#1b1b1b;font-size:12px;line-height:20px;list-style:none;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';display:inline-block;height:auto;margin-right:8px;padding-left:7px;padding-right:7px;white-space:nowrap;background:#fafafa;border:1px solid rgba(237,237,237, 0.6);border-radius:4px;opacity:1;transition:all 0.2s;text-align:start;position:relative;}.css-16cpc8y.ant-tag.ant-tag-rtl{direction:rtl;}.css-16cpc8y.ant-tag,.css-16cpc8y.ant-tag a,.css-16cpc8y.ant-tag a:hover{color:#1b1b1b;}.css-16cpc8y.ant-tag .ant-tag-close-icon{margin-left:3px;font-size:10px;color:rgb(57,57,57);cursor:pointer;transition:all 0.2s;}.css-16cpc8y.ant-tag .ant-tag-close-icon:hover{color:#1b1b1b;}.css-16cpc8y.ant-tag.ant-tag-has-color{border-color:transparent;}.css-16cpc8y.ant-tag.ant-tag-has-color,.css-16cpc8y.ant-tag.ant-tag-has-color a,.css-16cpc8y.ant-tag.ant-tag-has-color a:hover,.css-16cpc8y.ant-tag.ant-tag-has-color .anticon-close,.css-16cpc8y.ant-tag.ant-tag-has-color .anticon-close:hover{color:#fff;}.css-16cpc8y.ant-tag-checkable{background-color:transparent;border-color:transparent;cursor:pointer;}.css-16cpc8y.ant-tag-checkable:not(.ant-tag-checkable-checked):hover{color:#416ed2;background-color:rgba(0, 0, 0, 0.06);}.css-16cpc8y.ant-tag-checkable:active,.css-16cpc8y.ant-tag-checkable-checked{color:#fff;}.css-16cpc8y.ant-tag-checkable-checked{background-color:#416ed2;}.css-16cpc8y.ant-tag-checkable-checked:hover{background-color:#6891de;}.css-16cpc8y.ant-tag-checkable:active{background-color:#2c50ab;}.css-16cpc8y.ant-tag-hidden{display:none;}.css-16cpc8y.ant-tag >.anticon+span,.css-16cpc8y.ant-tag >span+.anticon{margin-left:7px;}.css-16cpc8y.ant-tag-borderless{border-color:transparent;background:#fafafa;}.css-16cpc8y.ant-space{display:inline-flex;}.css-16cpc8y.ant-space-rtl{direction:rtl;}.css-16cpc8y.ant-space-vertical{flex-direction:column;}.css-16cpc8y.ant-space-align{flex-direction:column;}.css-16cpc8y.ant-space-align-center{align-items:center;}.css-16cpc8y.ant-space-align-start{align-items:flex-start;}.css-16cpc8y.ant-space-align-end{align-items:flex-end;}.css-16cpc8y.ant-space-align-baseline{align-items:baseline;}.css-16cpc8y.ant-space .ant-space-item:empty{display:none;}.css-16cpc8y.ant-space .ant-space-item>.ant-badge-not-a-wrapper:only-child{display:block;}.css-16cpc8y.ant-space-gap-row-small{row-gap:8px;}.css-16cpc8y.ant-space-gap-row-middle{row-gap:16px;}.css-16cpc8y.ant-space-gap-row-large{row-gap:24px;}.css-16cpc8y.ant-space-gap-col-small{column-gap:8px;}.css-16cpc8y.ant-space-gap-col-middle{column-gap:16px;}.css-16cpc8y.ant-space-gap-col-large{column-gap:24px;}.css-16cpc8y.ant-space-block{display:flex;width:100%;}.css-16cpc8y.ant-space-vertical{flex-direction:column;}.css-16cpc8y.ant-btn{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-btn::before,.css-16cpc8y.ant-btn::after{box-sizing:border-box;}.css-16cpc8y.ant-btn [class^="ant-btn"],.css-16cpc8y.ant-btn [class*=" ant-btn"]{box-sizing:border-box;}.css-16cpc8y.ant-btn [class^="ant-btn"]::before,.css-16cpc8y.ant-btn [class*=" ant-btn"]::before,.css-16cpc8y.ant-btn [class^="ant-btn"]::after,.css-16cpc8y.ant-btn [class*=" ant-btn"]::after{box-sizing:border-box;}.css-16cpc8y.ant-btn{outline:none;position:relative;display:inline-block;font-weight:400;white-space:nowrap;text-align:center;background-image:none;background:transparent;border:1px solid transparent;cursor:pointer;transition:all 0.2s cubic-bezier(0.645, 0.045, 0.355, 1);user-select:none;touch-action:manipulation;color:#1b1b1b;}.css-16cpc8y.ant-btn:disabled>*{pointer-events:none;}.css-16cpc8y.ant-btn >span{display:inline-block;}.css-16cpc8y.ant-btn .ant-btn-icon{line-height:0;}.css-16cpc8y.ant-btn >.anticon+span,.css-16cpc8y.ant-btn >span+.anticon{margin-left:8px;}.css-16cpc8y.ant-btn:not(.ant-btn-icon-only)>.ant-btn-icon.ant-btn-loading-icon,.css-16cpc8y.ant-btn:not(.ant-btn-icon-only)>.ant-btn-icon:not(:last-child){margin-right:8px;}.css-16cpc8y.ant-btn >a{color:currentColor;}.css-16cpc8y.ant-btn:not(:disabled):focus-visible{outline:4px solid #c3d9f7;outline-offset:1px;transition:outline-offset 0s,outline 0s;}.css-16cpc8y.ant-btn.ant-btn-two-chinese-chars::first-letter{letter-spacing:0.34em;}.css-16cpc8y.ant-btn.ant-btn-two-chinese-chars>*:not(.anticon){margin-right:-0.34em;letter-spacing:0.34em;}.css-16cpc8y.ant-btn-icon-only.ant-btn-compact-item{flex:none;}.css-16cpc8y.ant-btn{font-size:14px;line-height:1.5714285714285714;height:32px;padding:4px 15px;border-radius:6px;}.css-16cpc8y.ant-btn.ant-btn-icon-only{width:32px;padding-left:0;padding-right:0;}.css-16cpc8y.ant-btn.ant-btn-icon-only.ant-btn-round{width:auto;}.css-16cpc8y.ant-btn.ant-btn-icon-only .anticon{font-size:16px;}.css-16cpc8y.ant-btn.ant-btn-loading{opacity:0.65;cursor:default;}.css-16cpc8y.ant-btn .ant-btn-loading-icon{transition:width 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.3s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-btn.ant-btn-circle.ant-btn{min-width:32px;padding-left:0;padding-right:0;border-radius:50%;}.css-16cpc8y.ant-btn.ant-btn-round.ant-btn{border-radius:32px;padding-left:16px;padding-right:16px;}.css-16cpc8y.ant-btn-sm{font-size:14px;line-height:1.5714285714285714;height:24px;padding:0px 7px;border-radius:4px;}.css-16cpc8y.ant-btn-sm.ant-btn-icon-only{width:24px;padding-left:0;padding-right:0;}.css-16cpc8y.ant-btn-sm.ant-btn-icon-only.ant-btn-round{width:auto;}.css-16cpc8y.ant-btn-sm.ant-btn-icon-only .anticon{font-size:14px;}.css-16cpc8y.ant-btn-sm.ant-btn-loading{opacity:0.65;cursor:default;}.css-16cpc8y.ant-btn-sm .ant-btn-loading-icon{transition:width 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.3s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-btn.ant-btn-circle.ant-btn-sm{min-width:24px;padding-left:0;padding-right:0;border-radius:50%;}.css-16cpc8y.ant-btn.ant-btn-round.ant-btn-sm{border-radius:24px;padding-left:12px;padding-right:12px;}.css-16cpc8y.ant-btn-lg{font-size:16px;line-height:1.5;height:40px;padding:7px 15px;border-radius:8px;}.css-16cpc8y.ant-btn-lg.ant-btn-icon-only{width:40px;padding-left:0;padding-right:0;}.css-16cpc8y.ant-btn-lg.ant-btn-icon-only.ant-btn-round{width:auto;}.css-16cpc8y.ant-btn-lg.ant-btn-icon-only .anticon{font-size:18px;}.css-16cpc8y.ant-btn-lg.ant-btn-loading{opacity:0.65;cursor:default;}.css-16cpc8y.ant-btn-lg .ant-btn-loading-icon{transition:width 0.3s cubic-bezier(0.645, 0.045, 0.355, 1),opacity 0.3s cubic-bezier(0.645, 0.045, 0.355, 1);}.css-16cpc8y.ant-btn.ant-btn-circle.ant-btn-lg{min-width:40px;padding-left:0;padding-right:0;border-radius:50%;}.css-16cpc8y.ant-btn.ant-btn-round.ant-btn-lg{border-radius:40px;padding-left:20px;padding-right:20px;}.css-16cpc8y.ant-btn.ant-btn-block{width:100%;}.css-16cpc8y.ant-btn-default{background:#ffffff;border-color:rgba(237,237,237, 0.6);color:#1b1b1b;box-shadow:0 2px 0 rgba(0, 0, 0, 0.02);}.css-16cpc8y.ant-btn-default:disabled,.css-16cpc8y.ant-btn-default.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-default:not(:disabled):not(.ant-btn-disabled):hover{color:#6891de;border-color:#6891de;background:#ffffff;}.css-16cpc8y.ant-btn-default:not(:disabled):not(.ant-btn-disabled):active{color:#2c50ab;border-color:#2c50ab;background:#ffffff;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost{color:#ffffff;background:transparent;border-color:#ffffff;box-shadow:none;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-default.ant-btn-dangerous{color:#ff4d4f;border-color:#ff4d4f;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;border-color:#ffa39e;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#d9363e;border-color:#d9363e;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost{color:#ff4d4f;background:transparent;border-color:#ff4d4f;box-shadow:none;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-default.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-default.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-primary{color:#fff;background:#416ed2;box-shadow:0 2px 0 rgba(5, 122, 255, 0.06);}.css-16cpc8y.ant-btn-primary:disabled,.css-16cpc8y.ant-btn-primary.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-primary:not(:disabled):not(.ant-btn-disabled):hover{color:#fff;background:#6891de;}.css-16cpc8y.ant-btn-primary:not(:disabled):not(.ant-btn-disabled):active{color:#fff;background:#2c50ab;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost{color:#416ed2;background:transparent;border-color:#416ed2;box-shadow:none;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;color:#6891de;border-color:#6891de;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;color:#2c50ab;border-color:#2c50ab;}.css-16cpc8y.ant-btn-primary.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous{background:#ff4d4f;box-shadow:0 2px 0 rgba(255, 38, 5, 0.06);color:#fff;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{background:#ff7875;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{background:#d9363e;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost{color:#ff4d4f;background:transparent;border-color:#ff4d4f;box-shadow:none;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;color:#ff7875;border-color:#ff7875;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;color:#d9363e;border-color:#d9363e;}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-primary.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-primary.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-dashed{background:#ffffff;border-color:rgba(237,237,237, 0.6);color:#1b1b1b;box-shadow:0 2px 0 rgba(0, 0, 0, 0.02);border-style:dashed;}.css-16cpc8y.ant-btn-dashed:disabled,.css-16cpc8y.ant-btn-dashed.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-dashed:not(:disabled):not(.ant-btn-disabled):hover{color:#6891de;border-color:#6891de;background:#ffffff;}.css-16cpc8y.ant-btn-dashed:not(:disabled):not(.ant-btn-disabled):active{color:#2c50ab;border-color:#2c50ab;background:#ffffff;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost{color:#ffffff;background:transparent;border-color:#ffffff;box-shadow:none;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous{color:#ff4d4f;border-color:#ff4d4f;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;border-color:#ffa39e;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#d9363e;border-color:#d9363e;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost{color:#ff4d4f;background:transparent;border-color:#ff4d4f;box-shadow:none;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-dashed.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;border-color:rgba(237,237,237, 0.6);color:rgba(0, 0, 0, 0.25);background:rgba(0, 0, 0, 0.04);box-shadow:none;}.css-16cpc8y.ant-btn-link{color:#416ed2;}.css-16cpc8y.ant-btn-link:not(:disabled):not(.ant-btn-disabled):hover{color:#305ab7;background:transparent;}.css-16cpc8y.ant-btn-link:not(:disabled):not(.ant-btn-disabled):active{color:#305ab7;}.css-16cpc8y.ant-btn-link:disabled,.css-16cpc8y.ant-btn-link.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-link.ant-btn-dangerous{color:#ff4d4f;}.css-16cpc8y.ant-btn-link.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;}.css-16cpc8y.ant-btn-link.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#d9363e;}.css-16cpc8y.ant-btn-link.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-link.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-text:not(:disabled):not(.ant-btn-disabled):hover{color:#1b1b1b;background:rgba(0, 0, 0, 0.06);}.css-16cpc8y.ant-btn-text:not(:disabled):not(.ant-btn-disabled):active{color:#1b1b1b;background:rgba(0, 0, 0, 0.15);}.css-16cpc8y.ant-btn-text:disabled,.css-16cpc8y.ant-btn-text.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-text.ant-btn-dangerous{color:#ff4d4f;}.css-16cpc8y.ant-btn-text.ant-btn-dangerous:disabled,.css-16cpc8y.ant-btn-text.ant-btn-dangerous.ant-btn-disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);}.css-16cpc8y.ant-btn-text.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):hover{color:#ff7875;background:#fff2f0;}.css-16cpc8y.ant-btn-text.ant-btn-dangerous:not(:disabled):not(.ant-btn-disabled):active{color:#ff7875;background:#fff2f0;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost{color:#ffffff;background:transparent;border-color:#ffffff;box-shadow:none;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):hover{background:transparent;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost:not(:disabled):not(.ant-btn-disabled):active{background:transparent;}.css-16cpc8y.ant-btn-ghost.ant-btn-background-ghost:disabled{cursor:not-allowed;color:rgba(0, 0, 0, 0.25);border-color:rgba(237,237,237, 0.6);}.css-16cpc8y.ant-btn-group{position:relative;display:inline-flex;}.css-16cpc8y.ant-btn-group >span:not(:last-child),.css-16cpc8y.ant-btn-group >.ant-btn:not(:last-child),.css-16cpc8y.ant-btn-group >span:not(:last-child)>.ant-btn,.css-16cpc8y.ant-btn-group >.ant-btn:not(:last-child)>.ant-btn{border-top-right-radius:0;border-bottom-right-radius:0;}.css-16cpc8y.ant-btn-group >span:not(:first-child),.css-16cpc8y.ant-btn-group >.ant-btn:not(:first-child){margin-left:-1px;}.css-16cpc8y.ant-btn-group >span:not(:first-child),.css-16cpc8y.ant-btn-group >.ant-btn:not(:first-child),.css-16cpc8y.ant-btn-group >span:not(:first-child)>.ant-btn,.css-16cpc8y.ant-btn-group >.ant-btn:not(:first-child)>.ant-btn{border-top-left-radius:0;border-bottom-left-radius:0;}.css-16cpc8y.ant-btn-group .ant-btn{position:relative;z-index:1;}.css-16cpc8y.ant-btn-group .ant-btn:hover,.css-16cpc8y.ant-btn-group .ant-btn:focus,.css-16cpc8y.ant-btn-group .ant-btn:active{z-index:2;}.css-16cpc8y.ant-btn-group .ant-btn[disabled]{z-index:0;}.css-16cpc8y.ant-btn-group .ant-btn-icon-only{font-size:14px;}.css-16cpc8y.ant-btn-group >span:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:last-child)>.ant-btn-primary:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:last-child)>.ant-btn-primary:not(:disabled){border-right-color:#6891de;}.css-16cpc8y.ant-btn-group >span:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:first-child)>.ant-btn-primary:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-primary:not(:first-child)>.ant-btn-primary:not(:disabled){border-left-color:#6891de;}.css-16cpc8y.ant-btn-group >span:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:last-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:last-child)>.ant-btn-danger:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:last-child)>.ant-btn-danger:not(:disabled){border-right-color:#ff7875;}.css-16cpc8y.ant-btn-group >span:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:first-child):not(:disabled),.css-16cpc8y.ant-btn-group >span:not(:first-child)>.ant-btn-danger:not(:disabled),.css-16cpc8y.ant-btn-group >.ant-btn-danger:not(:first-child)>.ant-btn-danger:not(:disabled){border-left-color:#ff7875;}.css-16cpc8y[class^="ant-modal"],.css-16cpc8y[class*=" ant-modal"]{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y[class^="ant-modal"]::before,.css-16cpc8y[class*=" ant-modal"]::before,.css-16cpc8y[class^="ant-modal"]::after,.css-16cpc8y[class*=" ant-modal"]::after{box-sizing:border-box;}.css-16cpc8y[class^="ant-modal"] [class^="ant-modal"],.css-16cpc8y[class*=" ant-modal"] [class^="ant-modal"],.css-16cpc8y[class^="ant-modal"] [class*=" ant-modal"],.css-16cpc8y[class*=" ant-modal"] [class*=" ant-modal"]{box-sizing:border-box;}.css-16cpc8y[class^="ant-modal"] [class^="ant-modal"]::before,.css-16cpc8y[class*=" ant-modal"] [class^="ant-modal"]::before,.css-16cpc8y[class^="ant-modal"] [class*=" ant-modal"]::before,.css-16cpc8y[class*=" ant-modal"] [class*=" ant-modal"]::before,.css-16cpc8y[class^="ant-modal"] [class^="ant-modal"]::after,.css-16cpc8y[class*=" ant-modal"] [class^="ant-modal"]::after,.css-16cpc8y[class^="ant-modal"] [class*=" ant-modal"]::after,.css-16cpc8y[class*=" ant-modal"] [class*=" ant-modal"]::after{box-sizing:border-box;}.css-16cpc8y.ant-modal-root .ant-modal-wrap-rtl{direction:rtl;}.css-16cpc8y.ant-modal-root .ant-modal-centered{text-align:center;}.css-16cpc8y.ant-modal-root .ant-modal-centered::before{display:inline-block;width:0;height:100%;vertical-align:middle;content:"";}.css-16cpc8y.ant-modal-root .ant-modal-centered .ant-modal{top:0;display:inline-block;padding-bottom:0;text-align:start;vertical-align:middle;}@media (max-width: 767px){.css-16cpc8y.ant-modal-root .ant-modal{max-width:calc(100vw - 16px);margin:8px auto;}.css-16cpc8y.ant-modal-root .ant-modal-centered .ant-modal{flex:1;}}.css-16cpc8y.ant-modal{box-sizing:border-box;margin:0 auto;padding:0;color:#1b1b1b;font-size:14px;line-height:1.5714285714285714;list-style:none;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';pointer-events:none;position:relative;top:100px;width:auto;max-width:calc(100vw - 32px);padding-bottom:24px;}.css-16cpc8y.ant-modal .ant-modal-title{margin:0;color:#1b1b1b;font-weight:600;font-size:16px;line-height:1.5;word-wrap:break-word;}.css-16cpc8y.ant-modal .ant-modal-content{position:relative;background-color:#ffffff;background-clip:padding-box;border:0;border-radius:8px;box-shadow:0 6px 16px 0 rgba(0, 0, 0, 0.08),0 3px 6px -4px rgba(0, 0, 0, 0.12),0 9px 28px 8px rgba(0, 0, 0, 0.05);pointer-events:auto;padding:20px 24px;}.css-16cpc8y.ant-modal .ant-modal-close{position:absolute;top:12px;right:12px;z-index:1010;padding:0;color:rgb(57,57,57);font-weight:600;line-height:1;text-decoration:none;background:transparent;border-radius:4px;width:32px;height:32px;border:0;outline:0;cursor:pointer;transition:color 0.2s,background-color 0.2s;}.css-16cpc8y.ant-modal .ant-modal-close-x{display:flex;font-size:16px;font-style:normal;line-height:32px;justify-content:center;text-transform:none;text-rendering:auto;}.css-16cpc8y.ant-modal .ant-modal-close:hover{color:#1b1b1b;background-color:rgba(0, 0, 0, 0.06);text-decoration:none;}.css-16cpc8y.ant-modal .ant-modal-close:active{background-color:rgba(0, 0, 0, 0.15);}.css-16cpc8y.ant-modal .ant-modal-close:focus-visible{outline:4px solid #c3d9f7;outline-offset:1px;transition:outline-offset 0s,outline 0s;}.css-16cpc8y.ant-modal .ant-modal-header{color:#1b1b1b;background:#ffffff;border-radius:8px 8px 0 0;margin-bottom:8px;padding:0;border-bottom:none;}.css-16cpc8y.ant-modal .ant-modal-body{font-size:14px;line-height:1.5714285714285714;word-wrap:break-word;padding:0;}.css-16cpc8y.ant-modal .ant-modal-footer{text-align:end;background:transparent;margin-top:12px;padding:0;border-top:none;border-radius:0;}.css-16cpc8y.ant-modal .ant-modal-footer >.ant-btn+.ant-btn{margin-left:8px;}.css-16cpc8y.ant-modal .ant-modal-open{overflow:hidden;}.css-16cpc8y.ant-modal-pure-panel{top:auto;padding:0;display:flex;flex-direction:column;}.css-16cpc8y.ant-modal-pure-panel .ant-modal-content,.css-16cpc8y.ant-modal-pure-panel .ant-modal-body,.css-16cpc8y.ant-modal-pure-panel .ant-modal-confirm-body-wrapper{display:flex;flex-direction:column;flex:auto;}.css-16cpc8y.ant-modal-pure-panel .ant-modal-confirm-body{margin-bottom:auto;}.css-16cpc8y.ant-modal-root .ant-modal-wrap-rtl{direction:rtl;}.css-16cpc8y.ant-modal-root .ant-modal-wrap-rtl .ant-modal-confirm-body{direction:rtl;}.css-16cpc8y.ant-modal-root .ant-modal.ant-zoom-enter,.css-16cpc8y.ant-modal-root .ant-modal.ant-zoom-appear{transform:none;opacity:0;animation-duration:0.3s;user-select:none;}.css-16cpc8y.ant-modal-root .ant-modal.ant-zoom-leave .ant-modal-content{pointer-events:none;}.css-16cpc8y.ant-modal-root .ant-modal-mask{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1000;height:100%;background-color:rgba(0, 0, 0, 0.45);pointer-events:none;}.css-16cpc8y.ant-modal-root .ant-modal-mask .ant-modal-hidden{display:none;}.css-16cpc8y.ant-modal-root .ant-modal-wrap{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1000;overflow:auto;outline:0;-webkit-overflow-scrolling:touch;}.css-16cpc8y.ant-modal-root .ant-fade-enter,.css-16cpc8y.ant-modal-root .ant-fade-appear{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-modal-root .ant-fade-leave{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-modal-root .ant-fade-enter.ant-fade-enter-active,.css-16cpc8y.ant-modal-root .ant-fade-appear.ant-fade-appear-active{animation-name:css-16cpc8y-antFadeIn;animation-play-state:running;}.css-16cpc8y.ant-modal-root .ant-fade-leave.ant-fade-leave-active{animation-name:css-16cpc8y-antFadeOut;animation-play-state:running;pointer-events:none;}.css-16cpc8y.ant-modal-root .ant-fade-enter,.css-16cpc8y.ant-modal-root .ant-fade-appear{opacity:0;animation-timing-function:linear;}.css-16cpc8y.ant-modal-root .ant-fade-leave{animation-timing-function:linear;}.css-16cpc8y.ant-zoom-enter,.css-16cpc8y.ant-zoom-appear{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-zoom-leave{animation-duration:0.2s;animation-fill-mode:both;animation-play-state:paused;}.css-16cpc8y.ant-zoom-enter.ant-zoom-enter-active,.css-16cpc8y.ant-zoom-appear.ant-zoom-appear-active{animation-name:css-16cpc8y-antZoomIn;animation-play-state:running;}.css-16cpc8y.ant-zoom-leave.ant-zoom-leave-active{animation-name:css-16cpc8y-antZoomOut;animation-play-state:running;pointer-events:none;}.css-16cpc8y.ant-zoom-enter,.css-16cpc8y.ant-zoom-appear{transform:scale(0);opacity:0;animation-timing-function:cubic-bezier(0.08, 0.82, 0.17, 1);}.css-16cpc8y.ant-zoom-enter-prepare,.css-16cpc8y.ant-zoom-appear-prepare{transform:none;}.css-16cpc8y.ant-zoom-leave{animation-timing-function:cubic-bezier(0.78, 0.14, 0.15, 0.86);}@keyframes css-16cpc8y-antFadeIn{0%{opacity:0;}100%{opacity:1;}}@keyframes css-16cpc8y-antFadeOut{0%{opacity:1;}100%{opacity:0;}}@keyframes css-16cpc8y-antZoomIn{0%{transform:scale(0.2);opacity:0;}100%{transform:scale(1);opacity:1;}}@keyframes css-16cpc8y-antZoomOut{0%{transform:scale(1);}100%{transform:scale(0.2);opacity:0;}}.css-16cpc8y.ant-typography{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-typography::before,.css-16cpc8y.ant-typography::after{box-sizing:border-box;}.css-16cpc8y.ant-typography [class^="ant-typography"],.css-16cpc8y.ant-typography [class*=" ant-typography"]{box-sizing:border-box;}.css-16cpc8y.ant-typography [class^="ant-typography"]::before,.css-16cpc8y.ant-typography [class*=" ant-typography"]::before,.css-16cpc8y.ant-typography [class^="ant-typography"]::after,.css-16cpc8y.ant-typography [class*=" ant-typography"]::after{box-sizing:border-box;}.css-16cpc8y.ant-typography{color:#1b1b1b;word-break:break-word;line-height:1.5714285714285714;}.css-16cpc8y.ant-typography.ant-typography-secondary{color:rgb(57,57,57);}.css-16cpc8y.ant-typography.ant-typography-success{color:#52c41a;}.css-16cpc8y.ant-typography.ant-typography-warning{color:#faad14;}.css-16cpc8y.ant-typography.ant-typography-danger{color:#ff4d4f;}a.css-16cpc8y.ant-typography.ant-typography-danger:active,a.css-16cpc8y.ant-typography.ant-typography-danger:focus{color:#d9363e;}a.css-16cpc8y.ant-typography.ant-typography-danger:hover{color:#ff7875;}.css-16cpc8y.ant-typography.ant-typography-disabled{color:rgba(0, 0, 0, 0.25);cursor:not-allowed;user-select:none;}div.css-16cpc8y.ant-typography,.css-16cpc8y.ant-typography p{margin-bottom:1em;}h1.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h1,div.css-16cpc8y.ant-typography-h1>textarea,.css-16cpc8y.ant-typography h1{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:38px;line-height:1.2105263157894737;}h2.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h2,div.css-16cpc8y.ant-typography-h2>textarea,.css-16cpc8y.ant-typography h2{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:30px;line-height:1.2666666666666666;}h3.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h3,div.css-16cpc8y.ant-typography-h3>textarea,.css-16cpc8y.ant-typography h3{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:24px;line-height:1.3333333333333333;}h4.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h4,div.css-16cpc8y.ant-typography-h4>textarea,.css-16cpc8y.ant-typography h4{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:20px;line-height:1.4;}h5.css-16cpc8y.ant-typography,div.css-16cpc8y.ant-typography-h5,div.css-16cpc8y.ant-typography-h5>textarea,.css-16cpc8y.ant-typography h5{margin-bottom:0.5em;color:#1b1b1b;font-weight:600;font-size:16px;line-height:1.5;}.css-16cpc8y.ant-typography+h1.ant-typography,.css-16cpc8y.ant-typography+h2.ant-typography,.css-16cpc8y.ant-typography+h3.ant-typography,.css-16cpc8y.ant-typography+h4.ant-typography,.css-16cpc8y.ant-typography+h5.ant-typography{margin-top:1.2em;}.css-16cpc8y.ant-typography div +h1,.css-16cpc8y.ant-typography ul +h1,.css-16cpc8y.ant-typography li +h1,.css-16cpc8y.ant-typography p +h1,.css-16cpc8y.ant-typography h1 +h1,.css-16cpc8y.ant-typography h2 +h1,.css-16cpc8y.ant-typography h3 +h1,.css-16cpc8y.ant-typography h4 +h1,.css-16cpc8y.ant-typography h5 +h1,.css-16cpc8y.ant-typography div +h2,.css-16cpc8y.ant-typography ul +h2,.css-16cpc8y.ant-typography li +h2,.css-16cpc8y.ant-typography p +h2,.css-16cpc8y.ant-typography h1 +h2,.css-16cpc8y.ant-typography h2 +h2,.css-16cpc8y.ant-typography h3 +h2,.css-16cpc8y.ant-typography h4 +h2,.css-16cpc8y.ant-typography h5 +h2,.css-16cpc8y.ant-typography div +h3,.css-16cpc8y.ant-typography ul +h3,.css-16cpc8y.ant-typography li +h3,.css-16cpc8y.ant-typography p +h3,.css-16cpc8y.ant-typography h1 +h3,.css-16cpc8y.ant-typography h2 +h3,.css-16cpc8y.ant-typography h3 +h3,.css-16cpc8y.ant-typography h4 +h3,.css-16cpc8y.ant-typography h5 +h3,.css-16cpc8y.ant-typography div +h4,.css-16cpc8y.ant-typography ul +h4,.css-16cpc8y.ant-typography li +h4,.css-16cpc8y.ant-typography p +h4,.css-16cpc8y.ant-typography h1 +h4,.css-16cpc8y.ant-typography h2 +h4,.css-16cpc8y.ant-typography h3 +h4,.css-16cpc8y.ant-typography h4 +h4,.css-16cpc8y.ant-typography h5 +h4,.css-16cpc8y.ant-typography div +h5,.css-16cpc8y.ant-typography ul +h5,.css-16cpc8y.ant-typography li +h5,.css-16cpc8y.ant-typography p +h5,.css-16cpc8y.ant-typography h1 +h5,.css-16cpc8y.ant-typography h2 +h5,.css-16cpc8y.ant-typography h3 +h5,.css-16cpc8y.ant-typography h4 +h5,.css-16cpc8y.ant-typography h5 +h5{margin-top:1.2em;}.css-16cpc8y.ant-typography code{margin:0 0.2em;padding-left:0.4em;padding-right:0.4em;padding-top:0.2em;padding-bottom:0.1em;font-size:85%;font-family:'SFMono-Regular',Consolas,'Liberation Mono',Menlo,Courier,monospace;background:rgba(150, 150, 150, 0.1);border:1px solid rgba(100, 100, 100, 0.2);border-radius:3px;}.css-16cpc8y.ant-typography kbd{margin:0 0.2em;padding-left:0.4em;padding-right:0.4em;padding-top:0.15em;padding-bottom:0.1em;font-size:90%;font-family:'SFMono-Regular',Consolas,'Liberation Mono',Menlo,Courier,monospace;background:rgba(150, 150, 150, 0.06);border:1px solid rgba(100, 100, 100, 0.2);border-bottom-width:2px;border-radius:3px;}.css-16cpc8y.ant-typography mark{padding:0;background-color:#ffe58f;}.css-16cpc8y.ant-typography u,.css-16cpc8y.ant-typography ins{text-decoration:underline;text-decoration-skip-ink:auto;}.css-16cpc8y.ant-typography s,.css-16cpc8y.ant-typography del{text-decoration:line-through;}.css-16cpc8y.ant-typography strong{font-weight:600;}.css-16cpc8y.ant-typography ul,.css-16cpc8y.ant-typography ol{margin-left:0;margin-right:0;margin-top:0;margin-bottom:1em;padding:0;}.css-16cpc8y.ant-typography ul li,.css-16cpc8y.ant-typography ol li{margin-left:20px;margin-right:0;margin-top:0;margin-bottom:0;padding-left:4px;padding-right:0;padding-top:0;padding-bottom:0;}.css-16cpc8y.ant-typography ul{list-style-type:circle;}.css-16cpc8y.ant-typography ul ul{list-style-type:disc;}.css-16cpc8y.ant-typography ol{list-style-type:decimal;}.css-16cpc8y.ant-typography pre,.css-16cpc8y.ant-typography blockquote{margin:1em 0;}.css-16cpc8y.ant-typography pre{padding:0.4em 0.6em;white-space:pre-wrap;word-wrap:break-word;background:rgba(150, 150, 150, 0.1);border:1px solid rgba(100, 100, 100, 0.2);border-radius:3px;font-family:'SFMono-Regular',Consolas,'Liberation Mono',Menlo,Courier,monospace;}.css-16cpc8y.ant-typography pre code{display:inline;margin:0;padding:0;font-size:inherit;font-family:inherit;background:transparent;border:0;}.css-16cpc8y.ant-typography blockquote{padding-left:0.6em;padding-right:0;padding-top:0;padding-bottom:0;border-left:4px solid rgba(100, 100, 100, 0.2);opacity:0.85;}a.css-16cpc8y.ant-typography,.css-16cpc8y.ant-typography a{color:#416ed2;text-decoration:none;outline:none;cursor:pointer;transition:color 0.3s;}a.css-16cpc8y.ant-typography:focus,.css-16cpc8y.ant-typography a:focus,a.css-16cpc8y.ant-typography:hover,.css-16cpc8y.ant-typography a:hover{color:#305ab7;}a.css-16cpc8y.ant-typography:active,.css-16cpc8y.ant-typography a:active{color:#305ab7;}a.css-16cpc8y.ant-typography:active,.css-16cpc8y.ant-typography a:active,a.css-16cpc8y.ant-typography:hover,.css-16cpc8y.ant-typography a:hover{text-decoration:none;}a.css-16cpc8y.ant-typography[disabled],.css-16cpc8y.ant-typography a[disabled],a.css-16cpc8y.ant-typography.ant-typography-disabled,.css-16cpc8y.ant-typography a.ant-typography-disabled{color:rgba(0, 0, 0, 0.25);cursor:not-allowed;}a.css-16cpc8y.ant-typography[disabled]:active,.css-16cpc8y.ant-typography a[disabled]:active,a.css-16cpc8y.ant-typography.ant-typography-disabled:active,.css-16cpc8y.ant-typography a.ant-typography-disabled:active,a.css-16cpc8y.ant-typography[disabled]:hover,.css-16cpc8y.ant-typography a[disabled]:hover,a.css-16cpc8y.ant-typography.ant-typography-disabled:hover,.css-16cpc8y.ant-typography a.ant-typography-disabled:hover{color:rgba(0, 0, 0, 0.25);}a.css-16cpc8y.ant-typography[disabled]:active,.css-16cpc8y.ant-typography a[disabled]:active,a.css-16cpc8y.ant-typography.ant-typography-disabled:active,.css-16cpc8y.ant-typography a.ant-typography-disabled:active{pointer-events:none;}.css-16cpc8y.ant-typography .ant-typography-expand,.css-16cpc8y.ant-typography .ant-typography-edit,.css-16cpc8y.ant-typography .ant-typography-copy{color:#416ed2;text-decoration:none;outline:none;cursor:pointer;transition:color 0.3s;margin-left:4px;}.css-16cpc8y.ant-typography .ant-typography-expand:focus,.css-16cpc8y.ant-typography .ant-typography-edit:focus,.css-16cpc8y.ant-typography .ant-typography-copy:focus,.css-16cpc8y.ant-typography .ant-typography-expand:hover,.css-16cpc8y.ant-typography .ant-typography-edit:hover,.css-16cpc8y.ant-typography .ant-typography-copy:hover{color:#305ab7;}.css-16cpc8y.ant-typography .ant-typography-expand:active,.css-16cpc8y.ant-typography .ant-typography-edit:active,.css-16cpc8y.ant-typography .ant-typography-copy:active{color:#305ab7;}.css-16cpc8y.ant-typography-edit-content{position:relative;}div.css-16cpc8y.ant-typography-edit-content{left:-12px;margin-top:-12px;margin-bottom:calc(1em - 12px);}.css-16cpc8y.ant-typography-edit-content .ant-typography-edit-content-confirm{position:absolute;right:10px;bottom:8px;color:rgb(57,57,57);font-weight:normal;font-size:14px;font-style:normal;pointer-events:none;}.css-16cpc8y.ant-typography-edit-content textarea{margin:0!important;-moz-transition:none;height:1em;}.css-16cpc8y.ant-typography .ant-typography-copy-success,.css-16cpc8y.ant-typography .ant-typography-copy-success:hover,.css-16cpc8y.ant-typography .ant-typography-copy-success:focus{color:#52c41a;}.css-16cpc8y.ant-typography .ant-typography-copy-icon-only{margin-left:0;}a.css-16cpc8y.ant-typography-ellipsis,span.css-16cpc8y.ant-typography-ellipsis{display:inline-block;max-width:100%;}.css-16cpc8y.ant-typography-single-line{white-space:nowrap;}.css-16cpc8y.ant-typography-ellipsis-single-line{overflow:hidden;text-overflow:ellipsis;}a.css-16cpc8y.ant-typography-ellipsis-single-line,span.css-16cpc8y.ant-typography-ellipsis-single-line{vertical-align:bottom;}.css-16cpc8y.ant-typography-ellipsis-single-line >code{padding-top:0;padding-bottom:0;max-width:calc(100% - 1.2em);display:inline-block;overflow:hidden;text-overflow:ellipsis;vertical-align:bottom;box-sizing:content-box;}.css-16cpc8y.ant-typography-ellipsis-multiple-line{display:-webkit-box;overflow:hidden;-webkit-line-clamp:3;-webkit-box-orient:vertical;}.css-16cpc8y.ant-typography-rtl{direction:rtl;}.css-16cpc8y.ant-app{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';font-size:14px;box-sizing:border-box;}.css-16cpc8y.ant-app::before,.css-16cpc8y.ant-app::after{box-sizing:border-box;}.css-16cpc8y.ant-app [class^="ant-app"],.css-16cpc8y.ant-app [class*=" ant-app"]{box-sizing:border-box;}.css-16cpc8y.ant-app [class^="ant-app"]::before,.css-16cpc8y.ant-app [class*=" ant-app"]::before,.css-16cpc8y.ant-app [class^="ant-app"]::after,.css-16cpc8y.ant-app [class*=" ant-app"]::after{box-sizing:border-box;}.css-16cpc8y.ant-app{color:#1b1b1b;font-size:14px;line-height:1.5714285714285714;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'Helvetica Neue',Arial,'Noto Sans',sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol','Noto Color Emoji';}.anticon{display:inline-flex;align-items:center;color:inherit;font-style:normal;line-height:0;text-align:center;text-transform:none;vertical-align:-0.125em;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.anticon >*{line-height:1;}.anticon svg{display:inline-block;}.anticon .anticon .anticon-icon{display:block;}.anticon{display:inline-flex;align-items:center;color:inherit;font-style:normal;line-height:0;text-align:center;text-transform:none;vertical-align:-0.125em;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.anticon >*{line-height:1;}.anticon svg{display:inline-block;}.anticon .anticon .anticon-icon{display:block;}:focus {
  outline: none;
}
:focus-visible {
  outline: none;
}
input::-ms-clear,
input::-ms-reveal {
  display: none;
}
*,
*::before,
*::after {
  box-sizing: border-box;
}
.container {
  margin: 0;
  text-align: left;
  font-weight: initial;
  font-family: sans-serif;
  -webkit-font-smoothing: auto;
  -moz-osx-font-smoothing: initial;
  line-height: 1.15;
  -webkit-text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -ms-overflow-style: scrollbar;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
  --font: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto,
    'Helvetica Neue', Arial, 'Noto Sans', sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  --cnfont:  -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Microsoft YaHei", "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", sans-serif;
  --size: 20px;
  --width: 800px;
  --space: 0;
  --lineheight: 1.8;
  --weight: 400;
  --blockspace: 20px;
  --indent: 0em;
  --titlealign: left;
  --align: left;
  --imageleft: auto;
  --imageright: auto;
  --imagehide: block;
  --margin: 80px;
  --padding: 80px;
  --offsetleft: 0px;
  --offsetright: 0px;
  --columncount: 2;
  --columngap: 60px;
  --columnwidth: 90%;
  --title1: 2em;
  --title1weight: 500;
  --title1color: #1b1b1b;
  --title2: 1.6em;
  --title2weight: 500;
  --title2color: #1b1b1b;
  --title3: 1.2em;
  --title3weight: 500;
  --title3color: #1b1b1b;
  --title4: 1em;
  --title4weight: 500;
  --title4color: #1b1b1b;
  --title5: 1em;
  --title5weight: 500;
  --title5color: #1b1b1b;
  --title6: 1em;
  --title6weight: 500;
  --title6color: #1b1b1b;
  --color: #1b1b1b;
  --link: #416ed2;
  --hover: #305ab7;
  --visited: #305ab7;
  --select: #1b1b1b;
  --selectbg: #bbd6fc;
  --bg-r: 237;
  --bg-g: 237;
  --bg-b: 237;
  --bg: #ffffff;
  --canvas: #ededed;
  --track-width: 8px;
  --track: #e2e2e2;
  --thumb: #9e9e9e;
  --radius: 4px;
}
.container .ant-app {
  color:  #1b1b1b;
  font-size: var(--size);
  font-family:  -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Microsoft YaHei", "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", sans-serif;
  font-weight: var(--weight);
  line-height: var(--lineheight);
  padding-left: var(--offsetleft);
  padding-right: var(--offsetright);
}
@-ms-viewport {
  width: device-width;
}
[tabindex='-1']:focus {
  outline: none;
}
hr {
  box-sizing: content-box;
  height: 0;
  overflow: visible;
}
h1,
h2,
h3,
h4,
h5,
h6 {
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 500;
}
p {
  margin-top: 0;
  margin-bottom: 1em;
}
abbr[title],
abbr[data-original-title] {
  -webkit-text-decoration: underline dotted;
  text-decoration: underline;
  text-decoration: underline dotted;
  border-bottom: 0;
  cursor: help;
}
address {
  margin-bottom: 1em;
  font-style: normal;
  line-height: inherit;
}
input[type='text'],
input[type='password'],
input[type='number'],
textarea {
  -webkit-appearance: none;
}
ol,
ul,
dl {
  margin-top: 0;
  margin-bottom: 1em;
}
ol ol,
ul ul,
ol ul,
ul ol {
  margin-bottom: 0;
}
dt {
  font-weight: 500;
}
dd {
  margin-bottom: 0.5em;
  margin-left: 0;
}
blockquote {
  margin: 0 0 1em;
}
dfn {
  font-style: italic;
}
b,
strong {
  font-weight: bolder;
}
small {
  font-size: 80%;
}
sub,
sup {
  position: relative;
  font-size: 75%;
  line-height: 0;
  vertical-align: baseline;
}
sub {
  bottom: -0.25em;
}
sup {
  top: -0.5em;
}
pre,
code,
kbd,
samp {
  font-size: 1em;
  font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
}
pre {
  margin-top: 0;
  margin-bottom: 1em;
  overflow: auto;
}
figure {
  margin: 0 0 1em;
}
img {
  vertical-align: middle;
  border-style: none;
}
a,
area,
button,
[role='button'],
input:not([type='range']),
label,
select,
summary,
textarea {
  touch-action: manipulation;
}
table {
  border-collapse: collapse;
}
caption {
  padding-top: 0.75em;
  padding-bottom: 0.3em;
  text-align: left;
  caption-side: bottom;
}
input,
button,
select,
optgroup,
textarea {
  margin: 0;
  color: inherit;
  font-size: inherit;
  font-family: inherit;
  line-height: inherit;
}
button,
input {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html [type='button'],
[type='reset'],
[type='submit'] {
  -webkit-appearance: button;
}
button::-moz-focus-inner,
[type='button']::-moz-focus-inner,
[type='reset']::-moz-focus-inner,
[type='submit']::-moz-focus-inner {
  padding: 0;
  border-style: none;
}
input[type='radio'],
input[type='checkbox'] {
  box-sizing: border-box;
  padding: 0;
}
input[type='date'],
input[type='time'],
input[type='datetime-local'],
input[type='month'] {
  -webkit-appearance: listbox;
}
textarea {
  overflow: auto;
  resize: vertical;
}
fieldset {
  min-width: 0;
  margin: 0;
  padding: 0;
  border: 0;
}
legend {
  display: block;
  width: 100%;
  max-width: 100%;
  margin-bottom: 0.5em;
  padding: 0;
  color: inherit;
  font-size: 1.5em;
  line-height: inherit;
  white-space: normal;
}
progress {
  vertical-align: baseline;
}
[type='number']::-webkit-inner-spin-button,
[type='number']::-webkit-outer-spin-button {
  height: auto;
}
[type='search'] {
  outline-offset: -2px;
  -webkit-appearance: none;
}
[type='search']::-webkit-search-cancel-button,
[type='search']::-webkit-search-decoration {
  -webkit-appearance: none;
}
::-webkit-file-upload-button {
  font: inherit;
  -webkit-appearance: button;
}
output {
  display: inline-block;
}
summary {
  display: list-item;
}
template {
  display: none;
}
[hidden] {
  display: none !important;
}
mark {
  padding: 0.2em;
}

.copyright {
  line-height: normal;
  display: flex;
  align-items: center;
  flex-direction: column;
  padding-bottom: 20px;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only {
  width: auto;
  height: auto;
  opacity: 0.2;
  padding: 7px 10px;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only svg {
  width: 40px;
  height: 40px;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only:hover {
  opacity: 1;
  background: transparent;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only.like:hover {
  color: #f10606;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only.unlike:hover {
  color: #e7c207;
}
.copyright .ant-btn.ant-btn-lg.ant-btn-icon-only.coffee:hover {
  color: #ce6a07;
}
.copyright .ant-btn.ant-btn-sm {
  font-size: 12px;
  opacity: 0.4;
  padding-left: 6px;
  padding-right: 6px;
  height: auto;
}
.copyright .ant-btn.ant-btn-sm:hover {
  opacity: 1;
}
.copyright .ant-typography.ant-typography-secondary {
  font-size: 12px;
  opacity: 0.5;
  margin-top: 3px;
}

.container .ant-app {
  min-height: 100vh;
}
.container a {
  text-decoration: none;
}
.container code,
.container tt {
  border-radius: 3px;
  padding: 0 5px;
  margin: 0 5px;
}
.container pre {
  padding: 0.6em 0.8em;
  word-wrap: break-word;
  border-radius: 3px;
}
.container pre code {
  display: inline;
  margin: 0;
  padding: 0;
  background: 0 0;
  border: 0;
}
.container kbd {
  margin: 0 0.2em;
  padding: 0.15em 0.4em 0.1em;
  font-size: 90%;
  border-bottom-width: 2px;
  border-radius: 3px;
}
.container blockquote {
  padding: 0.1em 1em;
  opacity: 0.85;
}
.container blockquote > :first-child {
  margin-top: 0px;
}
.container blockquote > :last-child {
  margin-bottom: 0px;
}
.container blockquote p {
  margin: 0;
}
.container hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  padding: 0;
}
.container table {
  width: 100%;
  max-width: 100%;
  border-collapse: collapse;
}
.container table pre {
  white-space: pre-wrap;
}
.container figure {
  overflow: auto;
}
.container figure > img,
.container figure > table,
.container figure > pre {
  margin: 0 !important;
}
.container tr th,
.container tr td {
  padding: 6px 13px;
}
.container tr th p {
  margin: 0;
}
.container img {
  object-fit: contain;
}
.container img,
.container video {
  max-width: 100%;
  height: auto;
}
.container embed {
  max-width: 100%;
}
.container li div,
.container li p {
  margin: 0;
}
.container mark {
  padding: 0;
  margin: 0;
  color: initial;
  background-color: initial;
}
.container :focus {
  outline: none;
}
.container [role='separator']::after {
  content: '';
  display: block;
  clear: both;
  margin: 25px 0;
  height: 2px;
  overflow: hidden;
  border: none;
  opacity: 0.1;
  background:  #1b1b1b;
  padding: 0;
}
.container [role='separator']:last-child::after {
  display: none;
}
.container .line {
  display: block;
}
.container .left {
  float: left;
  margin-right: 12px;
}
.container .right {
  float: right;
  margin-left: 12px;
}
.container .both {
  clear: both;
}
.container .full-width {
  width: 100%;
  max-width: 100% !important;
}
.container .noise {
  display: none !important;
}
.container .block {
  display: block;
}
.container .inline-block {
  display: inline-block;
}
.container .flex {
  display: flex;
  flex-wrap: wrap;
}
.container .inline-flex {
  display: inline-flex;
  flex-wrap: wrap;
}
.container .zone-width {
  min-width: 0;
}

.container {
  color:  #1b1b1b;
  background:  #ffffff;
}
.container code,
.container tt {
  border: 1px solid rgba( 237,  237,  237, 0.6);
  background: rgba( 237,  237,  237, 0.4);
}
.container pre {
  background: rgba( 237,  237,  237, 0.5);
}
.container kbd {
  border: 1px solid rgba( 237,  237,  237, 1);
}
.container blockquote {
  border-left: 4px solid rgba( 237,  237,  237, 1);
}
.container hr {
  border-bottom: 1px solid rgba( 237,  237,  237, 1);
}
.container table th:hover,
.container table tr:hover {
  background: rgba( 237,  237,  237, 0.6);
}
.container table tr {
  border-bottom: 1px solid rgba( 237,  237,  237, 0.6);
}
.container table thead tr:first-child,
.container table tbody tr:first-child {
  background: rgba( 237,  237,  237, 0.4);
}
.container table thead + tbody tr:first-child {
  background: transparent;
}
.container table tr td {
  border-inline-end: 1px solid  rgb(237, 237, 237);
}
.container h1,
.container h2,
.container h3,
.container h4,
.container h5,
.container h6 {
  color:  #1b1b1b;
}
.container a {
  color:  #416ed2;
}
.container a:hover {
  color:  #305ab7;
}
.container a:active {
  color:  #305ab7;
}
.container a:visited {
  color:  #305ab7;
}
.container ::selection {
  color:  #1b1b1b;
  background-color:  #bbd6fc;
}
.container .title a {
  color:  #1b1b1b;
}
.container .title a:hover {
  color:  #416ed2;
}
.container .footer {
  color:  #1b1b1b;
}
.container.solid {
  background:  rgb(237, 237, 237);
}

.container {
  padding: 1px;
  text-shadow: none;
  font-variant: tabular-nums;
  font-feature-settings: 'tnum';
}
.container h1 {
  font-size: var(--title1);
  color:  #1b1b1b;
  font-weight: var(--title1weight);
}
.container h2 {
  font-size: var(--title2);
  color:  #1b1b1b;
  font-weight: var(--title2weight);
}
.container h3 {
  font-size: var(--title3);
  color:  #1b1b1b;
  font-weight: var(--title3weight);
}
.container h4 {
  font-size: var(--title4);
  color:  #1b1b1b;
  font-weight: var(--title4weight);
}
.container h5 {
  font-size: var(--title5);
  color:  #1b1b1b;
  font-weight: var(--title5weight);
}
.container h6 {
  font-size: var(--title6);
  color:  #1b1b1b;
  font-weight: var(--title6weight);
}
.container h1,
.container h2,
.container h3,
.container h4,
.container h5,
.container h6 {
  text-indent: 0;
}
.container a.link {
  margin: 0 6px;
}
.container a.link-with-img {
  display: inline-block;
}
.container a.link-with-img img {
  margin: 0 !important;
}
.container ol,
.container ul {
  text-indent: 0;
}
.container pre {
  white-space: pre-wrap;
}
.container pre,
.container code,
.container blockquote {
  text-indent: 0;
  text-align: left;
}
.container p,
.container blockquote,
.container ul,
.container ol,
.container dl,
.container table,
.container pre,
.container section,
.container figcaption,
.container li,
.container .block {
  margin: var(--blockspace) 0;
}
.container table {
  width: var(--width);
  max-width: var(--width);
  border-top: 1px solid  rgb(237, 237, 237);
  border-bottom: 1px solid  rgb(237, 237, 237);
}
.container .flex p,
.container .inline-flex p,
.container .flex blockquote,
.container .inline-flex blockquote,
.container .flex ul,
.container .inline-flex ul,
.container .flex ol,
.container .inline-flex ol,
.container .flex dl,
.container .inline-flex dl,
.container .flex table,
.container .inline-flex table,
.container .flex pre,
.container .inline-flex pre,
.container .flex section,
.container .inline-flex section,
.container .flex figcaption,
.container .inline-flex figcaption,
.container .flex .block,
.container .inline-flex .block {
  margin: 0;
}
.container .flex > *,
.container .inline-flex > * {
  margin-left: 10px !important;
  margin-right: 10px !important;
}
.container .flex > *:first-child,
.container .inline-flex > *:first-child {
  margin-left: 0 !important;
}
.container .flex > *:last-child,
.container .inline-flex > *:last-child {
  margin-right: 0 !important;
}
.container .pages {
  min-height: 610px;
  max-width: var(--width);
  margin:  80px auto;
}
.container .pages svg {
  width: 20px;
  height: 20px;
}
.container .pages .MathJax_SVG svg {
  width: auto;
  height: auto;
}
.container .page {
  padding-bottom: 20px;
  margin-bottom: 20px;
  word-break: break-word;
  text-indent: var(--indent);
  letter-spacing: var(--space);
  text-align: var(--align);
}
.container .page > *:first-child {
  margin-top: 0;
}
.container .page > *:last-child {
  margin-bottom: 0;
}
.container .page:last-child {
  margin-bottom: 0;
  border-bottom-width: 0;
}
.container .page:before,
.container .page:after {
  content: '';
  display: block;
  clear: both;
  height: 0px;
}
.container .page img {
  display: var(--imagehide);
}
.container .page img.large {
  margin-top: var(--blockspace) !important;
  margin-bottom: var(--blockspace) !important;
  margin-left: var(--imageleft) !important;
  margin-right: var(--imageright) !important;
}
.container .page img.tiny,
.container .page img.base64 {
  width: auto;
}
.container .page figure {
  overflow: auto;
}
.container .page figure > img.large,
.container .page figure > table,
.container .page figure > pre {
  margin-top: 0 !important;
  margin-bottom: 0 !important;
}
.container .page figure > figcaption {
  text-align: center;
  font-size: 90%;
  opacity: 0.8;
  margin-top: 6px;
}
.container .page p > img.large,
.container .page a img {
  margin-top: 0 !important;
  margin-bottom: 0 !important;
}
.container .page .title {
  text-indent: 0;
  margin-bottom: 0;
  text-align: var(--titlealign);
}
.container .page .meta {
  font-size: 80%;
  text-indent: 0;
  text-align: var(--titlealign);
  margin: 0 0 var(--blockspace);
}
.container .page .meta > span {
  opacity: 0.7;
  position: relative;
  margin-right: 24px;
}
.container .page .meta > span.ant-avatar {
  margin-right: 2px;
}
.container .page .meta > span.ant-avatar img {
  display: block;
}
.container .page .meta > span::after {
  content: '';
  position: absolute;
  right: -12px;
  top: 47%;
  display: block;
  width: 3px;
  height: 3px;
  background:  #1b1b1b;
  border-radius: 50%;
}
.container .page .meta > span:last-child::after {
  display: none;
}
.container .page .meta.avatar .ant-avatar {
  float: left;
  opacity: 1;
  width: 46px;
  height: 46px;
  margin-right: 16px;
  background:  #ffffff;
  border-radius: 6px;
  overflow: hidden;
}
.container .page .meta.avatar .ant-avatar img {
  display: block;
}
.container .page .meta.avatar .author {
  opacity: 1;
  line-height: 1.3;
  display: block;
}
.container .page .meta.avatar .author::after {
  display: none;
}
.container .page .meta::after {
  content: '';
  display: block;
  clear: both;
}
.container .page .excerpt {
  text-indent: 0;
  border-color:  #416ed2;
  background:  rgb(237, 237, 237);
}
.container .page .excerpt p {
  font-size: 90%;
}
.container .page .cover {
  text-indent: 0;
  margin-bottom: var(--blockspace);
}
.container .page .cover .ant-image {
  width: 100%;
  background: #f5f5f5;
}
.container .page .cover img {
  margin: 0 !important;
  width: 100%;
}
.container .page .footer {
  text-indent: 0;
  margin: calc(var(--blockspace) * 2) 0 0;
}
.container .page .footer .ant-tag {
  cursor: pointer;
  min-height: 27px;
  border-radius: 15px;
  background:  #ffffff;
  padding: 3px 12px 3px 3px;
  margin: 0 5px 4px 0;
  display: inline-block;
}
.container .page .footer .ant-tag::before {
  content: '#';
  margin-right: 4px;
  background:  rgb(237, 237, 237);
  width: 20px;
  height: 20px;
  text-align: center;
  font-size: 12px;
  color:  #416ed2;
  line-height: 20px;
  border-radius: 50px;
  display: inline-block;
}
.container .page .footer .ant-tag:hover {
  color:  #1b1b1b;
  background-color:  #bbd6fc;
}
.container .page .page-empty.ant-result {
  padding: 180px 0;
}
.container .page .page-empty.ant-result .ant-result-icon svg {
  width: auto;
  height: auto;
}
.container .page.zh {
  font-family:  -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Microsoft YaHei", "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", sans-serif;
}
.container .tex,
.container .katex-display {
  text-align: center;
  margin: 50px 0;
}
.container .sub-page::before {
  content: '';
  display: block;
  height: var(--blockspace);
  border-top: 1px solid  #1b1b1b;
  opacity: 0.1;
  margin-left: calc(-1 *  80px);
  margin-right: calc(-1 *  80px);
}
.container .sub-page .sub-title {
  margin-bottom: -2px;
}
@media screen and (max-width: 600px) {
  .container .pages {
    margin: 20px 16px;
  }
}

.container.paper {
  background:  rgb(237, 237, 237);
}
.container.paper table {
  margin: var(--blockspace) calc( 80px * -1);
}
.container.paper .page {
  border-radius: 6px;
  background:  #ffffff;
  padding:  80px;
  box-shadow: 0 0 6px rgba(0, 0, 0, 0.15);
}
.container.paper .page .cover {
  margin: var(--blockspace) calc( 80px * -1);
}
.container.paper .page-block {
  padding-left:  80px;
  padding-right:  80px;
}
.container.paper .page-block .cover {
  margin: 0;
  max-width: 100%;
}
@media screen and (max-width: 600px) {
  .container.paper .page {
    padding: 20px;
  }
  .container.paper .page .cover {
    max-width: calc(100% + 40px);
    margin: -10px -20px 0;
  }
}

@media print {
  .container {
    padding-right: 0 !important;
  }
  .container .pages {
    margin: 0;
    max-width: 100%;
  }
  .container .pages pre,
  .container .pages img,
  .container .pages table {
    break-inside: avoid;
    page-break-inside: avoid;
  }
  .container .page-block {
    column-count: auto;
    margin-bottom: 0;
    padding-bottom: 0;
    padding-top: 0;
  }
  .container.paper .page {
    box-shadow: none;
  }
}

.container.shot.paper .page {
  border-radius: 0;
  box-shadow: none;
}

@-webkit-keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }

  100% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }
}

@keyframes fadeIn {
  0% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }

  100% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }
}

@-webkit-keyframes fadeOut {
  0% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }

  100% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }
}

@keyframes fadeOut {
  0% {
    opacity: 1;
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
  }

  100% {
    opacity: 0;
    -webkit-transform: translate3d(0, 1600px, 0);
    transform: translate3d(0, 1600px, 0);
  }
}

.container.in .pages {
  -webkit-animation-name: fadeIn;
  animation-name: fadeIn;
  -webkit-animation-duration: .3s;
  animation-duration: .3s;
  -webkit-animation-fill-mode: both;
  animation-fill-mode: both;
}

.container.out .pages {
  -webkit-animation-name: fadeOut;
  animation-name: fadeOut;
  -webkit-animation-duration: .2s;
  animation-duration: .2s;
  -webkit-animation-fill-mode: both;
  animation-fill-mode: both;
}

    .container {
      --codefont: inherit;
      --codesize: 20px;
      --codelineheight: 1;
      --codecomment: #697070;
      --codetag: #444a;
      --codeattr: #444a;
      --codenumber: #800;
      --codeblock: #800;
      --codevariable: #ab5656;
      --codeliteral: #695;
      --code: #397300;
      --codemeta: #1f7199;
      --codestring: #38a;
    }
    .hljs {
      border-radius: 3px;
      font-family: var(--codefont);
      font-size: var(--codesize);
      line-height: var(--codelineheight);
    }
    .hljs.large {
      position: fixed;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      padding: 50px;
      background:  #ffffff;
      z-index: 600;
      margin: 0;
      word-break: break-word;
      overflow: auto;
      white-space: pre-wrap;
    }
    
    .hljs-comment {
      color: var(--codecomment);
    }
    .hljs-punctuation,
    .hljs-tag {
      color: var(--codetag);
    }
    .hljs-tag .hljs-attr,
    .hljs-tag .hljs-name {
      color: var(--codeattr);
    }
    .hljs-attribute,
    .hljs-doctag,
    .hljs-keyword,
    .hljs-meta .hljs-keyword,
    .hljs-name,
    .hljs-selector-tag {
      font-weight: 700;
    }
    .hljs-deletion,
    .hljs-number,
    .hljs-quote,
    .hljs-selector-class,
    .hljs-selector-id,
    .hljs-string,
    .hljs-template-tag,
    .hljs-type {
      color: var(--codenumber);
    }
    .hljs-section,
    .hljs-title {
      color: var(--codeblock);
      font-weight: 700;
    }
    .hljs-link,
    .hljs-operator,
    .hljs-regexp,
    .hljs-selector-attr,
    .hljs-selector-pseudo,
    .hljs-symbol,
    .hljs-template-variable,
    .hljs-variable {
      color: var(--codevariable);
    }
    .hljs-literal {
      color: var(--codeliteral);
    }
    .hljs-addition,
    .hljs-built_in,
    .hljs-bullet,
    .hljs-code {
      color: var(--code);
    }
    .hljs-meta {
      color: var(--codemeta);
    }
    .hljs-meta .hljs-string {
      color: var(--codestring);
    }
    .hljs-emphasis {
      font-style: italic;
    }
    .hljs-strong {
      font-weight: 700;
    }
    img[action="zoom-in"] {cursor: zoom-in;}</style><title>LLM入门指南 - Circle 阅读助手</title></head><body><div class=" container paper in"><div class="ant-app"><div class="pages"><div class="page"><h1 class="title" id="outline_0"><a href="https://zhuanlan.zhihu.com/p/669193585" target="_blank" rel="noreferrer">LLM入门指南</a></h1><p class="meta avatar"><span class="ant-avatar ant-avatar-circle ant-avatar-image css-16cpc8y"><img src="https://pica.zhimg.com/v2-c0b94d9e470c92e33dff0672bda2d24d_l.jpg?source=172ae18b 2x" class="tiny"></span><span class="author">密排六方橘子</span><span>2024-01-08 15:21</span><span>共 20248 字</span><span>阅读需 81 分钟</span></p><div class="sub-page"><h2 class="sub-title" id="outline_1">LLM入门指南(1)</h2><p class="meta avatar"><span class="ant-avatar ant-avatar-circle ant-avatar-image css-16cpc8y"><img src="https://pica.zhimg.com/v2-c0b94d9e470c92e33dff0672bda2d24d_l.jpg?source=172ae18b 2x" class="tiny"></span><span class="author">密排六方橘子</span><span>2024-01-08 15:21</span></p><p></p><p>前情提要：本人是学CV的，然后公司梭哈LLM，直接被拉去充了壮丁……</p><p>本文并不是从0开始学NLP，所以有些基础知识可能需要自己去了解一下，比如<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=transformer&amp;zhida_source=entity" target="_blank" class="link">transformer</a></span>结构之类的。</p><p>那么本文主要关注的是<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=%E6%98%86%E4%BB%91%E5%A4%A9%E5%B7%A5&amp;zhida_source=entity" target="_blank" class="link">昆仑天工</a></span>和<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=%E9%98%BF%E9%87%8C%E9%80%9A%E4%B9%89%E5%8D%83%E9%97%AE&amp;zhida_source=entity" target="_blank" class="link">阿里通义千问</a></span>（2023.11），至于为什么是这俩……天工是因为他们的开源号称是最有诚意的，阿里的千问则是因为他们的效果目前看起来是最好的。</p><p>文章内容似乎有点多，可以参照目录自行查阅需要的内容。</p><h2 id="outline_2">从Scaling Law说起</h2><p>众所周知，根据<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=OpenAI&amp;zhida_source=entity" target="_blank" class="link">OpenAI</a></span>的scaling圣经（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2001.08361.pdf" target="_blank" rel="nofollow noreferrer"><span>https://</span><span>arxiv.org/pdf/2001.0836</span><span>1.pdf</span></a>）有一些简单的结论，比如：</p><p>C ≈ 6NBS，或者写成C ≈ 6ND</p><p>其中C代表计算量（单位PF-days），N代表模型参数量（需要去掉PE和词表），D代表数据集大小（单位token），你自然可以把D换成BS（即batchsize*step），因为大模型pretrain一般也不会有第二个epoch，所以这两个东西基本上是等价的。</p><p>于是我们只要预设一个计算量（你能负担得起的时间和显卡成本），以及你需要的模型规模，就可以大致推算出你的数据集得要多大的；反过来也是可以的。</p><p>根据原文，天工13B用512张A800训了39天，我们假定它打满了算力峰值（实际根据文章中是56.5%），可以直接计算出总算力是 <span><span><span class="tex2jax_ignore math-holder">512\times312\times39\times86400\approx5.38\times10^{11}</span></span></span> TFlops，也就是大概5e23的计算量，其中312是A800的TF16算力（尽管天工用的是BF16，不过根据论文里的一些佐证，这个数应该是对的）。</p><p>考虑到算力的损失，天工总共花了3e23左右的算力，去做一个13B（1.3e10）模型，需要的数据量差不多就是4e12，也就是4TB token左右的数据，而摘要的第一句就说了天工使用了超过3.2T的token，说明天工整体依然是按照scaling law来做的，并没有在这种经验规律上搞什么花活儿。</p><p>以上就是scaling law的第一个小功能：<b>在一定的算力/模型规模/数据规模的预算下，如何去确定你的最优训练策略</b>，或者说是某种经验主义的“最优配比”。</p><p>Scaling law的另一个结论如下：</p><p><img src="https://pica.zhimg.com/v2-53e742750300c48665c3fdf54fea2934_r.jpg" class="large"></p><p>这三个式子反映的是CND在两者固定的情况下，单独调整某一个变量对模型loss带来的影响。除此之外还有一些公式，因为理解起来有点麻烦就不放在这了。</p><p>PS：我们知道了C ≈ 6ND，可以来看一下这三个式子是什么情况。1 PF-day = 8.64e19Flops，于是我们全用这边的基数做计算，那么6ND大概是2.85e28，C大概是2.68e28，能对上，没毛病。</p><p>然后你发现，根据C ≈ 6ND计算最优配比之后，如果实际情况稍有区别（比如我的算力有限导致模型未能完全收敛），这时候根据（1.3）就可以立即知道在你的算力条件下，loss能达到多少；当然更极端一点说，如果所有的模型都忠实地按照scaling law来走（不仅是幂律，包括参数的数值），模型在出生之前就已经决定结果了（比如无限算力+无限数据，根据1.1式，模型Loss只跟网络规模有关），你所能做的无非是让数据集更加贴近测试指标……</p><p>于是这就是scaling law的第二个功能了：<b>你可以在跑实验之前使用scaling law直接预测其他模型的表现</b>。因为大模型训练成本实在是太高了，很难支持“跑多次实验“这样的鬼故事。</p><p>当然实际情况下，这个幂次的具体数值不一定能直接套用到你的模型结构上，因为模型之间亦有差距。因此可以参考<a href="https://zhuanlan.zhihu.com/p/631357320" target="_blank" class="link">cingti：介绍一些Scaling Laws</a>里详细描述的内容（本质是上面的1.1式），也就是我们可以先跑几个小模型，然后画多条损失-计算量曲线（L-C，一个模型画一条），如下图所示：</p><p><img src="https://picx.zhimg.com/v2-3315c7f873056134a08a585b2cd307a9_r.jpg" class="large"></p><p>从图上可以看出，收敛的拐点在对数图上连起来差不多是一条直线，这样实际上你可以知道多个小模型收敛时CND各是多少，这样就能估计大模型的表现。</p><p>题外话：</p><p>在这两年经过大家的反复验证之后，大家逐渐发现scaling law甚至像物理定律一样准确。这时候你突然意识到一个有些严重的问题，就是如果这玩意真这么准的话，那么……</p><p><b>大模型在出生之前，它的性能就已经决定了。</b></p><p>以前主流的AI research的范式都是改网络结构，或者改一些细节（比如RoPE，激活函数等等），你能见到不少闪光的思路。而著名的国产大模型在llama2出来之后很多都沿用了llama（或者说transformer decoder），能改的也就是网络深度宽度这种不涉及结构变化的东西……但是这其实也是scaling law的预言，因为它专门在文中提了一句说<b>模型收敛时的损失跟网络规模的关系很大，而跟网络结构的关系很小</b>……</p><p>再直白一点说，如果scaling law存在，那么最近所有国产大模型的提升都来源于数据的增加（数量或者质量），<b>能提多少完全取决于你用的训练集跟CEVAL和<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=MMLU&amp;zhida_source=entity" target="_blank" class="link">MMLU</a></span>这种数据集有多少的domain gap，</b>至于模型本身的能力完全是换汤不换药式的提升，scaling law就是站在那里的一堵高墙，除非你能越过它，否则就是在折腾CND，仅此而已。</p><p>如果说以前刷榜好歹是提出了一些新方法（哪怕它并不实用），但是现在甚至退化了，只是折腾折腾数据去刷SOTA……意义在哪里呢？甚至不如搞VLM和RLHF更有创意，而实际上GPT4V也在走VLM的道路。</p><p>当然我并不是要说最近的大模型一无是处，毕竟我只是评价冰箱制不制冷。实际做的时候肯定有诸多细节只有踩一遍坑才知道，哪怕理论创新有限，能从头到尾实现一遍也是不可多得的经验，至少比我这种只会口嗨的强多了。</p><h2 id="outline_3">数据集</h2><h3 id="outline_4">训练数据集（天工）</h3><p>天工开源了一个150B的数据集（<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=SkyPile&amp;zhida_source=entity" target="_blank" class="link">SkyPile</a></span>），当然他们自家的数据集是6T的，这只是其中非常小的一部分。这150B的数据显然是不够训一个完整的LLM的，所以我们只是介绍一下清洗数据的过程。</p><p>在这里我要点名表扬一下天工的GRE选手，请品鉴：</p><p>In the pursuit of cultivating a profoundly adept LLM, the model’s exposure must encompass a diverse array of content spanning an extensive spectrum of domains. Prior endeavors within the field have entailed the task of assigning categorical labels to each individual document or webpage, thereby manually dictating the composition of the training corpus. However, we posit that the corpus employed for LLM training has burgeoned to such an extent that the knowledge it encapsulates can not be compartmentalized discretely. Consequently, eschewing a label-centric approach, our methodology centers on benchmarking the semantic affinities existing between textual segments, thereby identifying and omitting those text blocks characterized by an exceedingly high recurrence rate.</p><p>我觉得你们要是之后打算投稿也就算了……要是只是作为造福社区的技术报告，我建议还是稍微user friendly一些，毕竟隔壁qwen，llama乃至scaling law也没见谁把论文写成这样吧。</p><p>当然我也没什么资格说，毕竟开源代码是我当伸手党……</p><p>总之数据清洗大致可以分为以下四个部分：</p><ol><li>结构化过滤：因为训练数据主要来源于网页，所以需要去掉一些没用的东西（比如导航条，网页上的联系方式等等）</li><li>分布过滤：没太看懂什么意思。大致是说其他工作一般都会给文本分一个标签，但是这里认为文本涉及的内容可能很丰富（包含不止一个标签），所以不是按标签，而是测试“文本之间的亲和力”从而去掉一些过于重复的语段，但是这个semantic affinitiy到底是什么东西好像也没说，全文就提过一次affinity，说好最有诚意的开源呢（恼）。千问在去重这方面说得更清楚一些，大致是用<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=MinHash&amp;zhida_source=entity" target="_blank" class="link">MinHash</a></span>和<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=LSH&amp;zhida_source=entity" target="_blank" class="link">LSH</a></span>来做模糊匹配来筛掉重复内容，之后还有一些精确匹配进一步去重。</li><li>去重：总而言之是分布过滤的一部分。</li><li>质量过滤：用了<span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=CCNet&amp;zhida_source=entity" target="_blank" class="link">CCNet</a></span>模型来判断 1）质量低的文本 2）中英文以外的语言。除此之外还训练了一个二分类器（是否适合放进维基百科），通过这个分类器筛选出一部分高质量文本专门放到high quality groups里。</li></ol><p>其他还有一些细节但是还挺重要的东西（比如说构建了一个完全平行的中英文语料库放进整个数据集里，还有就是保留了一部分带格式的数据，比如xml和json等等）。</p><p>最后的数据构成（整个6T数据集，不是150B的）如下图所示：</p><p><img src="https://pic1.zhimg.com/v2-6468a8e2f7af78a51b392f4bf6944dde_r.jpg" class="large"></p><p>注意对于一些高质量的语料（比维基百科）可以在数据集里多重复几遍，但是根据某些论文里的经验，最好不要超过5遍。</p><h3 id="outline_5">评价数据集</h3><p>评价数据集也是数据集，就放在这里一块说了。</p><p>常见的数据集如下：</p><p><b>MMLU：</b>多任务语言理解（包括STEM和社科之类的内容），形式为选择题。</p><p><b><span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=C-Eval&amp;zhida_source=entity" target="_blank" class="link">C-Eval</a></span>：</b>中文知识问答，选择题。</p><p><b><span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=GSM8K&amp;zhida_source=entity" target="_blank" class="link">GSM8K</a></span>：</b>数学题，一般回答形式是problem-solution-final answer，最后根据答案判定是否做对。</p><p><b><span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=MATH&amp;zhida_source=entity" target="_blank" class="link">MATH</a></span>：</b>数学题，但是格式是latex。</p><p><b><span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=HumanEval&amp;zhida_source=entity" target="_blank" class="link">HumanEval</a></span>：</b>代码生成，包含功能描述和输入输出样例。这里的评价指标包含pass@1和pass@x，pass@1表示根据greedy生成的通过率，pass@x代表多写几次去测，当然pass@x的效果还是要高很多的……</p><p><b><span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=MBPP&amp;zhida_source=entity" target="_blank" class="link">MBPP</a></span>：</b>跟HumaEval差不多，也是代码生成。</p><p><b><span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=BBH&amp;zhida_source=entity" target="_blank" class="link">BBH</a></span>：</b>跟MMLU差不多，纯英文，选择题。</p><p><b><span><a href="https://zhida.zhihu.com/search?content_id=236840043&amp;content_type=Article&amp;match_order=1&amp;q=CMMLU&amp;zhida_source=entity" target="_blank" class="link">CMMLU</a></span>：</b>看名字就知道是中文的一个评价基准，跟C-Eval差不多。</p><p>以下为私货。</p><p>我去看了一眼这几个数据集，<b>中文数据集都有一个毛病，就是偏向于“知识问答”</b>，问题很直球，回答也很直球。</p><p>MMLU和BBH有一些问题像英语听力阅读理解，相比而言更难一些。还有一些问题搞得更加花里胡哨，比如说“which of the following is a humorous edit of this artist of movie name”，要求模型有更深的语言理解能力，甚至还有一些脑洞大开的测试，有兴趣可以自己去hugging face的BBH看一下。</p><p>CMMLU只有在涉及modern Chinese的时候才会偶尔出现这种需要动下脑子的题目，可惜的是整个数据集加起来也没多少条，C-Eval里也是寥寥无几。</p><p>这就带来一个问题，就是在一些情况下<b>对语言的理解能力需求远远高于对领域知识的需求</b>（比如长文本summary或者写小说），因此我的建议是<b>不要迷信评价指标</b>，尤其是C-Eval这种纯知识问答类的评价指标。</p><p>这还只是评价数据集本身的问题，更不要说在训练时有可能出现的数据泄露。因为现在的pretrain数据集很多都是来源于网页，但是有一些测试集里面的题目你在网上能搜到原题。</p><h2 id="outline_6">关于位置编码</h2><p>这部分我的建议是看苏剑林（苏神）的两个博客（<a href="https://link.zhihu.com/?target=https%3A//kexue.fm/archives/8231" target="_blank" rel="nofollow noreferrer" class="link">Transformer升级之路：1、Sinusoidal位置编码追根溯源 - 科学空间|Scientific Spaces</a>和<a href="https://link.zhihu.com/?target=https%3A//kexue.fm/archives/8265" target="_blank" rel="nofollow noreferrer" class="link">Transformer升级之路：2、博采众长的旋转式位置编码 - 科学空间|Scientific Spaces</a>)。</p><p>不过大家来都来了，我就用更加容易理解的说法来介绍一下位置编码。</p><p>位置编码的主要目的是给transformer（attention）掺入位置信息，因为<b>attention本身并不包含位置信息</b>（你任意调换一个句子中两个字的位置，对attention结果是没有影响的，比如说“我吃饭”和“饭吃我”这两个东西显然不应该是同一个概率），因此一个好办法是<b>在embedding/encoding的时候在每个位置的输入上都掺一个东西</b>，让这种对称性被破坏掉，这样attention的时候就会带着这个位置信息，网络就能学得更好。</p><p>然后问题就来了，位置信息应该怎么掺。一种思路是把<b>绝对位置</b>直接塞进去，另一种显然就是使用<b>相对位置</b>。但是无论是绝对还是相对位置，最好<b>需要保持一定的外推能力</b>，否则只能处理特定长度以内的文本，因为超过预设长度的embedding/encoding网络在训练的时候是没见过的，测试阶段如果超了长度就变成了一个OOD样本，你不知道他会输出什么东西。</p><p>至于相对位置和绝对位置到底哪个更好，尽管没有明确定论，但是在LLM大行其道的年代，谁能更方便地外推谁就是大哥。</p><h3 id="outline_7">Sinusoidal位置编码</h3><p>经典的绝对位置编码，来源于Transformer原始论文，这里简单做一个解释。</p><p>我们把attention看作函数 <span><span><span class="tex2jax_ignore math-holder">f</span></span></span> ，于是对于位置 <span><span><span class="tex2jax_ignore math-holder">m</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">n</span></span></span> ，我们有这两个位置的输入 <span><span><span class="tex2jax_ignore math-holder">x_{m}</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">x_{n}</span></span></span> 以及他们的位置编码 <span><span><span class="tex2jax_ignore math-holder">p_{m}</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">p_{n}</span></span></span> 。<b>位置编码可以视作一个小量</b>，对于相加式的的位置编码（比如说<span><span><span class="tex2jax_ignore math-holder">x_{m}</span></span></span>变成<span><span><span class="tex2jax_ignore math-holder">x_{m}+p_{m}</span></span></span> ）可以直接对 <span><span><span class="tex2jax_ignore math-holder">f</span></span></span> 做泰勒展开。这里我连展开公式都不用贴，稍微想一下你就知道<b>一阶项只会跟单一位置有关，某个二阶项才会涉及到两个位置的交互</b>，也就是跟<b>相对位置</b>有关。</p><p>而我们其实是<b>希望这个二阶项能够表达相对位置的信息，或者说能找到一个函数g，这个g(m-n)=这个二阶项，</b>这样m-n的信息就能够体现在attention里了。</p><p>这个二阶项大概长这样： <span><span><span class="tex2jax_ignore math-holder">p_{m}^{T}Hp_{n}</span></span></span> ，其中<span><span><span class="tex2jax_ignore math-holder">H = \frac{\partial^{2}f}{\partial x_{m}\partial x_{n}}</span></span></span> 。直接解释H似乎有点困难，但是你可以大致想一下对于一个attention map而言，这一项实际意味着<b>任意两个位置的<span><span><span class="tex2jax_ignore math-holder">x_{m}</span></span></span>和 <span><span><span class="tex2jax_ignore math-holder">x_{n}</span></span></span>的变动对attention的影响（或者说贡献）是否相同</b>，以及他俩之间<b>是否有相关性</b>。</p><p>理想情况下就是<b>任意两个位置的输入对于attention的贡献相同且互相解耦，此时H直接就是单位阵</b>，我们接下来也会基于这一假设去进一步介绍后面的内容；<b>退一步说</b>，如果他们对attention的贡献不同，但是<b>仍然是互相独立的情况下，H会变成一个对角阵</b>，此时仍然能够保留一些不错的性质；那么<b>最坏的情况下H是一个普通矩阵</b>，这时候我们下面提到的一切性质都不成立。幸运的是，苏神自己check了一下这个矩阵的权重，发现至少在embedding层，把H当成一个对角阵还是合理的，有兴趣可以自己去看一下他的博客。</p><p>好，那我们现在假定H就是单位阵，此时 <span><span><span class="tex2jax_ignore math-holder">p_{m}^{T}Hp_{n}</span></span></span> 直接退化成 <span><span><span class="tex2jax_ignore math-holder">p_{m}^{T}p_{n}</span></span></span> ，<b>注意这里的p其实都是向量</b>，所以它其实就是两个向量的内积。根据某些我已经忘得差不多了的知识，对于一个二维向量，也可以用复平面上的向量来表示；因此我们干脆再简化一点，就假定p也是二维的，于是此时 <span><span><span class="tex2jax_ignore math-holder">p_{m}^{T}p_{n}=Re[p_{m}p_{n}^{*}]</span></span></span> ，其中 <span><span><span class="tex2jax_ignore math-holder">p_{n}^{*}</span></span></span> 代表 <span><span><span class="tex2jax_ignore math-holder">p_{n}</span></span></span> 的共轭，Re代表实部。</p><p>于是现在的目标变成了：找到一个复数 <span><span><span class="tex2jax_ignore math-holder">g</span></span></span> ，使得 <span><span><span class="tex2jax_ignore math-holder">p_{m}p_{n}^{*}=g(m-n)</span></span></span> 。</p><p>那么接下来的事情就非常简单了，我们不妨把它写成复数的指数形式，即：</p><p><span><span><span class="tex2jax_ignore math-holder">p_{m}=r_{m}e^{i\phi_{m}}</span></span></span> , <span><span><span class="tex2jax_ignore math-holder">p_{n}=r_{n}e^{-i\phi_{n}}</span></span></span> , <span><span><span class="tex2jax_ignore math-holder">g(m-n)=R_{m-n}e^{i\Phi_{m-n}}</span></span></span> </p><p>于是很显然，要让他们相等就必然有（模长相等，辐角相等）：</p><p><span><span><span class="tex2jax_ignore math-holder">r_{m}r_{n}=R_{m-n}</span></span></span> , <span><span><span class="tex2jax_ignore math-holder">\phi_{m}-\phi_{n}=\Phi_{m-n}</span></span></span> </p><p>对第一个式子令m=n立刻得到 <span><span><span class="tex2jax_ignore math-holder">r_{m}^{2}=R_{0}</span></span></span> ，也就是说 <span><span><span class="tex2jax_ignore math-holder">r_{m}</span></span></span> 是个常数，方便起见直接令他等于1就行。</p><p>对第二个式子我们一眼就能看出令 <span><span><span class="tex2jax_ignore math-holder">\Phi_{m}=\phi_{m}=m\theta</span></span></span> 就是一个解，也就是说反正是等差数列就完事。</p><p>也就是说我们已经找到了这个式子的一个解了，有没有其他解不重要，反正这个能用。</p><p>回到我们之前说的，我们想要求解这个式子，<b>本质是想找到一个函数g作为位置编码，使得g(m-n)恰好等于f泰勒展开的二阶项，如果找到了，那么f里就会自然而然地带上相对位置信息。</b></p><p>而现在这个g恰好能用，也就是说对于一个二维向量 <span><span><span class="tex2jax_ignore math-holder">x_{m}</span></span></span> ，你只需要给他加上一个形如 <span><span><span class="tex2jax_ignore math-holder">p_{m}=e^{im\theta}</span></span></span> 的向量作为位置编码即可。进一步地，只要这个向量是偶数维，你就每两维（第2i和2i+1维）给他安排一个 <span><span><span class="tex2jax_ignore math-holder">sin(m\theta_{i})</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">cos(m\theta_{i})</span></span></span> 即可。</p><p>我们回头来看transformer论文里写的PE：</p><p><img src="https://pic2.zhimg.com/v2-182c63730d3c6af66b13ad6a08bf1fbd_r.jpg" class="large"></p><p>很显然，我们的结果已经跟他非常像了。这两个式子里的pos对应于我们的m，而唯一的区别就是<b>我们的 <span><span><span class="tex2jax_ignore math-holder">\theta_{i}</span></span></span> 实际上是随机的，你想让他是多少就可以是多少，但是transformer里给它特意加了一个数值</b> <span><span><span class="tex2jax_ignore math-holder">10000^{2i/d}</span></span></span> 。</p><p>这个值的意义在何处呢？</p><p>在于他有一个非常好的性质，就是<b>随着m-n（也就是相对距离）的增大， <span><span><span class="tex2jax_ignore math-holder">p_{m}^{T}p_{n}</span></span></span> 的结果会逐渐减小</b>，反映在transformer里就是两个输入距离越远，相关性越小，这其实是非常符合直觉的。至于为什么会出现这个现象，其实你硬算一下就知道了，在d足够大的时候可以把求和近似转化为震荡积分（或者可以直接画一下图），篇幅所限，这里就不折腾了。至于到底是1w还是1k，似乎并没有太大所谓。</p><p><img src="https://pica.zhimg.com/v2-18e67ef2ca5bcd6814eb6ee0597610ea_r.jpg" class="large"></p><figcaption>来源：https://kexue.fm/archives/8231</figcaption><p>于是我们来对这部分做个总结：</p><p><b>由于attention机制本身并不带有位置信息，所以我们希望在embedding的时候添加位置编码，使得两个输入的相对位置信息能够体现在attention里。落实下来就是通过构造正余弦的位置编码，使得相对位置信息能够体现在attention的二阶展开项中，并且加上一些特殊的设计使得该项能够随着距离而衰减。</b></p><h3 id="outline_8"><b>RoPE位置编码</b></h3><p>上面我们已经说了，如果<b>使用“相加式”的位置编码</b>，通过泰勒展开加上一些假设（位置编码是相对小量，二阶交叉项中H是单位阵）那么可以得到PE的一个解是 <span><span><span class="tex2jax_ignore math-holder">p_{m}=e^{im\theta}</span></span></span> 。由于我们是用泰勒展开做的，所以没有对代表attention的函数 <span><span><span class="tex2jax_ignore math-holder">f</span></span></span> 做任何限制，也就是说无论是什么 <span><span><span class="tex2jax_ignore math-holder">f</span></span></span>（哪怕它不是QKV的形式），这个编码都是可用的。</p><p>但是这似乎不是最优的假设，因为我们已经明确知道 <span><span><span class="tex2jax_ignore math-holder">f</span></span></span> 的形式，而且也不复杂，不用起来有些说不过去；另一方面“相加式”和泰勒展开的假设又有些太强了。</p><p>那么如果我们<b>只考虑transformer QK情况下的</b> <span><span><span class="tex2jax_ignore math-holder">f</span></span></span> ，换句话说 <span><span><span class="tex2jax_ignore math-holder">f</span></span></span> 就是向量内积的形式；<b>而对输入 <span><span><span class="tex2jax_ignore math-holder">x_{m}</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">p_{m}</span></span></span> 的关系不做要求</b>（不预设它是相加还是相乘等形式，而是等之后求解），情况是不是会有所不同呢？</p><p>这里我们干脆把加了位置编码的向量记为 <span><span><span class="tex2jax_ignore math-holder">h(\textbf{q},m)</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">h(\textbf{k},n)</span></span></span> ，其中<b>加粗的代表输入向量，而没加粗的m和n代表位置</b>。（注意，如果你沿用之前的 <span><span><span class="tex2jax_ignore math-holder">x_{m}</span></span></span> 作为 <span><span><span class="tex2jax_ignore math-holder">\textbf{q}</span></span></span> 的记号可能会有一些小问题。因为 <span><span><span class="tex2jax_ignore math-holder">x_{m}</span></span></span> 实际上是任意的，跟m没有直接联系，但是这个记号跟m没有完全解耦，导致有些东西推不出来）</p><p>我们不妨回头看看前面的目标： <span><span><span class="tex2jax_ignore math-holder">p_{m}p_{n}^{*}=g(m-n)</span></span></span> </p><p>巧的是，现在等式左边仍然是向量内积的形式（QK），只不过此时的内积不只是跟位置编码 <span><span><span class="tex2jax_ignore math-holder">p_{m}</span></span></span> 有关，而是跟输入的向量也有关。因此我们需要换成上面的写法，即左边写成 <span><span><span class="tex2jax_ignore math-holder">h(\textbf{q},m)h^{*}(\textbf{k},n)</span></span></span> ；至于等式右边大概率也要跟qk有关系了，所以我们这里稍微改改就变成了新的目标：</p><p><span><span><span class="tex2jax_ignore math-holder">h(\textbf{q},m)h^{*}(\textbf{k},n)=g(\textbf{q}, \textbf{k}, m-n)</span></span></span> </p><p>我们继续接着前面的推理逻辑依葫芦画瓢，同样设为二维向量，同样写成复数的指数形式，同样令模长和辐角都相等，于是就有：</p><p><span><span><span class="tex2jax_ignore math-holder">R_{h}(\textbf{q},m)R_{h}(\textbf{k},n)=R_{g}(\textbf{q},\textbf{k}, m-n)</span></span></span> </p><p><span><span><span class="tex2jax_ignore math-holder">\Theta_{h}(\textbf{q},m)-\Theta_{h}(\textbf{k},n)=\Theta_{g}(\textbf{q},\textbf{k}, m-n)</span></span></span> </p><p>先看第一个式子。我们令m=n立即有 <span><span><span class="tex2jax_ignore math-holder">R_{h}(\textbf{q},m)R_{h}(\textbf{k},m)=R_{g}(\textbf{q},\textbf{k}, 0)</span></span></span> ，也就是说 <span><span><span class="tex2jax_ignore math-holder">R_{h}(\textbf{q},m)</span></span></span> 和m无关。简单起见，我们直接令 <span><span><span class="tex2jax_ignore math-holder">R_{h}(\textbf{q},m)=|| \textbf{q} ||</span></span></span> 就结束战斗了。</p><p>对第二个式子，同样令m=n，有 <span><span><span class="tex2jax_ignore math-holder">\Theta_{h}(\textbf{q},m)-\Theta_{h}(\textbf{k},m)=\Theta_{g}(\textbf{q},\textbf{k}, 0)=\Theta_{h}(\textbf{q},0)-\Theta_{h}(\textbf{k},0)</span></span></span> 。</p><p>于是我们可以知道 <span><span><span class="tex2jax_ignore math-holder">\Theta_{h}(\textbf{q},m)</span></span></span> 和qk似乎也没什么关系，因为简单移一下项就可以得到。</p><p><span><span><span class="tex2jax_ignore math-holder">\Theta_{h}(\textbf{q},m)-\Theta_{h}(\textbf{q},0)=\Theta_{h}(\textbf{k},m)-\Theta_{h}(\textbf{k},0)</span></span></span> </p><p>可以看出它只跟m有关，通过简单的求解（比如令n=m-1）就可以知道，它又是一个等差数列，我们可以把它写成</p><p><span><span><span class="tex2jax_ignore math-holder">\Theta_{h}(\textbf{q},m)=\Theta_{h}(\textbf{q})+m\theta</span></span></span> </p><p>于是把他俩都带入回原来的复数形式，我们有：</p><p><span><span><span class="tex2jax_ignore math-holder">h(\textbf{q},m)=||\textbf{q}||e^{i(\Theta_{h}(\textbf{q})+m\theta)}</span></span></span> </p><p>注意到上面的 <span><span><span class="tex2jax_ignore math-holder">\Theta_{h}(\textbf{q})</span></span></span> 其实就是向量q的辐角，可以直接拿下来变成：</p><p><span><span><span class="tex2jax_ignore math-holder">h(\textbf{q},m)=\textbf{q}e^{im\theta}</span></span></span> </p><p><b>这个东西等价于把原始向量q旋转一个 <span><span><span class="tex2jax_ignore math-holder">m\theta</span></span></span> 角，这也是旋转位置编码（RoPE）名字的由来</b>。</p><p>之后的事情就不用多说了，直接参照sinusoidal的后续流程，每两维添加一次位置编码即可，只不过sinusoidal是直接加在输入向量上，而RoPE是转了一下。 <span><span><span class="tex2jax_ignore math-holder">\theta_{i}</span></span></span> 的取值也可以取 <span><span><span class="tex2jax_ignore math-holder">10000^{2i/d}</span></span></span> ，同样可以实现很好的远程衰减特性。</p><p>我们来对RoPE做个总结。</p><p>不同于Sinusoidal的推导方式，<b>我们专门针对attention的QK内积形式（而不是任意函数），并且放宽了对位置编码和输入向量的关系约束（不要求是相加），希望直接求解一个位置编码方式能够反映相对位置。最后推导的结果显示旋转位置编码可以满足这一条件。</b>所以从某种角度上说，RoPE比Sinusoidal好的原因主要是它更适配attention的计算方式，并且“强假设”少了很多。</p><h3 id="outline_9">NTK(Neural Tangent Kernel)</h3><p>这个名字有点抽象，不过也没必要去抠那么细节，篇幅所限，这里就简单说一下。</p><p>尽管RoPE相对而言有不少优势，不过它依然没解决“测试时文本长度大于训练时文本长度”的情况，比方说你训练时最大长度是1k，测试时是2k长度的文本。</p><p>很显然如果直接把现有编码方法外推到2k长度会出现模型没见过的输入，这时候结果不见得好；另一个想法是内插，就是把2k的位置编码对应到1k，相当于整体放缩一下，但是这样也有个问题，就是如果你放缩太多了会导致编码过分拥挤，相邻两个位置的差异会变得很小。</p><p>于是你只需要稍微动一下脑筋就可以想到……<b>对于长距离的两个字，他们之间的注意力可能没那么重要，毕竟都到1k这个距离了，大部分时候也很难有什么关联；对于很近的两个字，我们最好能忠实地使用训练时的位置编码，这样能够让最重要的attention不受影响。</b></p><p>那么一个直觉的想法就出来了：<b>“非线性缩放”，换句话说叫高频外推，低频内插。</b></p><p>至于为什么叫高频和低频，可以回到RoPE的编码方式：</p><p><span><span><span class="tex2jax_ignore math-holder">h(\textbf{q},m)=\textbf{q}e^{im\theta}</span></span></span> ，其中 <span><span><span class="tex2jax_ignore math-holder">\theta=10000^{-2i/d}</span></span></span> 。可以看到频率主要由 <span><span><span class="tex2jax_ignore math-holder">i</span></span></span> 决定，由于前面有个负号，所以越大的 <span><span><span class="tex2jax_ignore math-holder">i</span></span></span> 代表频率越低（注意区分这里的 <span><span><span class="tex2jax_ignore math-holder">i</span></span></span> 和代表复数的 <span><span><span class="tex2jax_ignore math-holder">i</span></span></span> ），也就是说前面的维度“转”得快一些，后面的“转”得慢。从直觉上说（其实从前面的推导过程也很容易看出来），<b>转得快的高频分量显然会对近距离更敏感</b>，因为只要位置m稍有变化， <span><span><span class="tex2jax_ignore math-holder">m\theta</span></span></span> 角度就会转很多；低频分量则是对远距离较为敏感。</p><p>所以一种可以采用的形式是，<b>让转的最快的 <span><span><span class="tex2jax_ignore math-holder">\theta</span></span></span> 转速保持不变，对于转的最慢的 <span><span><span class="tex2jax_ignore math-holder">\theta</span></span></span> 转速变为原来的1/8，至于中间的维度就按照比例（ <span><span><span class="tex2jax_ignore math-holder">8^{-\frac{2i}{d-2}}</span></span></span> ）来就行</b>。</p><p>或者我们用一个形象一点的总结，你对于近处的物体可能会对一两米的距离锱铢必较，但是对于远处的物体，它到底距离你50米还是100米并其实不那么重要，大多数时候你只要知道有这么个东西存在就差不多了。</p><p><b>通过NTK，我们可以在测试时接受超过训练时的文本长度</b>。尽管性能相比直接在长文本上训练有差距，但是也比不做NTK要强不少。</p><h2 id="outline_10">Act / Norm / Bias</h2><h3 id="outline_11">SwiGLU</h3><p>需要重点提示CV选手们，这里用的激活跟CV里的激活方法稍微有点区别。</p><p>常见的激活函数（ReLU，SiLU，Swish）之类的其实拿着一个输入就行了，但是在现在主流的大模型里稍复杂一些，因为多了一个叫GLU的东西（gated linear unit），简单起见我们直接看代码。</p><div><pre><code><span style="font-weight: 600;">class</span> <span style="font-weight: 600;">FeedForward</span><span>(</span><span>nn</span><span style="font-weight: 600;">.</span><span>Module</span><span>):</span>
    <span style="font-weight: 600;">def</span> <span style="font-weight: 600;">__init__</span><span>(</span><span>self</span><span>,</span> <span>dim</span><span>:</span> <span>int</span><span>,</span> <span>hidden_dim</span><span>:</span> <span>int</span><span>,</span> <span>multiple_of</span><span>:</span> <span>int</span><span>,</span> <span>dropout</span><span>:</span> <span>float</span><span>):</span>
        <span>super</span><span>()</span><span style="font-weight: 600;">.</span><span>__init__</span><span>()</span>
        <span>hidden_dim</span> <span style="font-weight: 600;">=</span> <span>multiple_of</span> <span style="font-weight: 600;">*</span> <span>((</span><span>2</span> <span style="font-weight: 600;">*</span> <span>hidden_dim</span> <span style="font-weight: 600;">//</span> <span>3</span> <span style="font-weight: 600;">+</span> <span>multiple_of</span> <span style="font-weight: 600;">-</span> <span>1</span><span>)</span> <span style="font-weight: 600;">//</span> <span>multiple_of</span><span>)</span>
        <span>self</span><span style="font-weight: 600;">.</span><span>w1</span> <span style="font-weight: 600;">=</span> <span>nn</span><span style="font-weight: 600;">.</span><span>Linear</span><span>(</span><span>dim</span><span>,</span> <span>hidden_dim</span><span>)</span>
        <span>self</span><span style="font-weight: 600;">.</span><span>w2</span> <span style="font-weight: 600;">=</span> <span>nn</span><span style="font-weight: 600;">.</span><span>Linear</span><span>(</span><span>hidden_dim</span><span>,</span> <span>dim</span><span>)</span>
        <span>self</span><span style="font-weight: 600;">.</span><span>w3</span> <span style="font-weight: 600;">=</span> <span>nn</span><span style="font-weight: 600;">.</span><span>Linear</span><span>(</span><span>dim</span><span>,</span> <span>hidden_dim</span><span>)</span>
        <span>self</span><span style="font-weight: 600;">.</span><span>dropout</span> <span style="font-weight: 600;">=</span> <span>nn</span><span style="font-weight: 600;">.</span><span>Dropout</span><span>(</span><span>dropout</span><span>)</span>

    <span style="font-weight: 600;">def</span> <span style="font-weight: 600;">forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>:</span> <span>torch</span><span style="font-weight: 600;">.</span><span>Tensor</span><span>)</span> <span style="font-weight: 600;">-&gt;</span> <span>torch</span><span style="font-weight: 600;">.</span><span>Tensor</span><span>:</span>
        <span style="font-weight: 600;">return</span> <span>self</span><span style="font-weight: 600;">.</span><span>dropout</span><span>(</span><span>self</span><span style="font-weight: 600;">.</span><span>w2</span><span>(</span><span>F</span><span style="font-weight: 600;">.</span><span>silu</span><span>(</span><span>self</span><span style="font-weight: 600;">.</span><span>w1</span><span>(</span><span>x</span><span>))</span> <span style="font-weight: 600;">*</span> <span>self</span><span style="font-weight: 600;">.</span><span>w3</span><span>(</span><span>x</span><span>)))</span></code></pre></div><p>对正常的CV选手来说这样才是正常的形式：</p><div><pre><code>self.w2(F.silu(self.w1(x))</code></pre></div><p>也就是说不知道从哪里多出来一个w3。</p><p>GLU认为silu（或者sigmoid）作为激活函数本质上是“决定保留多少信息”，因此干脆只是把激活函数当作一个门控机制，再额外添加一支作为主要的信息通路，<b>相当于变成了（信息支+门控支）两支通路</b>。因为多加了一个w3导致网络参数量会增加，所以需要对维度数做一些削减以保持参数量与原来一致。</p><h3 id="outline_12">RMS Norm</h3><p>众所周知norm就是归一化均值和方差。BN/LN/GN没有本质上的区别，只是拿来norm的维度不一样而已。</p><p>RMS（Root Mean Square）norm认为归一化均值没什么用，主要是方差比较重要，因此干脆把“减均值”这个操作给去了，直接计算RMS然后做norm就行了。</p><h3 id="outline_13">Bias</h3><p>专门提一句，就是千问说他们在大部分层把bias删了，但是在<b>QKV层里加bias反而对模型外推能力是有帮助的</b>，大致是因为bias这项加入之后会有一项随着相对位置增加而递减，从而变相增强了局部注意力。</p><p>这里就不细说了，有兴趣的可以看这篇：<a href="https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/9577" target="_blank" rel="nofollow noreferrer" class="link">Bias项的神奇作用：RoPE + Bias = 更好的长度外推性</a>。</p><h2 id="outline_14">Training</h2><p>先说天工的训练。</p><p>天工的表示一开始只打算用2T token去训，但是训完2T之后发现还有提升空间，于是又补了1.2T的token继续训。可能是一开始没做好训练规划？或者是一边训一边做的数据集，总之能出现这种失误还挺神奇的……</p><p>然后文章里表示后面加的这些数据跟2T数据的分布有一些小区别，所以很小心谨慎地tune了几个学习率，最后总之是定下来了，然后把后加的1T多的数据又喂进去了。</p><p>接下来是二阶段的pretrain。二阶段的pretrain主要是为了做STEM题（就是理工科那些）。训练方法就是把专门用于STEM的Skypile-STEM和整个数据集Skypile-Main两个掺在一起训（比例为2:8，总共130B左右token），并且实际操作的时候还有一些特别的trick：<b>在训练的过程中逐渐添加STEM数据集所占的比例，</b>比如说一开始只占10%，到训练结束的时候可能占40%。</p><p>然后再来简单提一下千问。</p><p><img src="https://pic2.zhimg.com/v2-a37ec4e337529a0cc42204f78a0f2d9d_r.jpg" class="large"></p><p>整体的数据集规模差不多，参数设置基本都和LLAMA2保持一致，而天工会略有区别：</p><p><img src="https://pic1.zhimg.com/v2-1ca508f17324bd1aefd8cb45b1959448_r.jpg" class="large"></p><p>千问的训练就正常多了，他也没特地强调训练时有什么花活儿。需要强调的是，训练时采用的都是BF16（和FP32能够表示同样的range，但是精度缩减了）。</p><h2 id="outline_15">Flash Attention</h2><p>主要功能是给attention加速。</p><p>由于具体内容实在过于复杂，完全搞懂需要大量时间，因此我对这部分做了大量删减，只留下一些定性的结论给大家。我的建议是有兴趣的（尤其是做cuda加速的）仔细阅读这篇：<a href="https://www.zhihu.com/question/611236756/answer/3310819022" target="_blank" class="link">FlashAttention 的速度优化原理是怎样的？</a>，里面讲得非常清楚，我就不画蛇添足了。</p><h3 id="outline_16">速度瓶颈</h3><p>传统attention流程如下：</p><p><b>从显存中取QK计算-&gt;将结果S写回显存-&gt;从显存读S计算softmax-&gt;将结果P写回显存-&gt;从显存读取P和V进行计算-&gt;将结果O写回显存。</b></p><p>由此可见整个attention过程中多次进行显存读写，而众所周知大模型的速度瓶颈其实很多时候并不在于计算速度，而在于显存读写，因此如何去减少显存读写次数是至关重要的。另一个众所周知的事情是，<b>离CPU/GPU越近，缓存速度就越快，在GPU上的速度是L1&gt;L2&gt;显存，但是容量上L1&lt;L2&lt;显存</b>。</p><p>因此，为了减少显存的读写，一个好的办法是<b>进行分块计算。</b>如果我的数据量足够小，我就能<b>全塞到L1缓存上</b>（比如说A100的L1只有192KB）进行计算了，这样<b>由于L1的读写速度远高于显存，就实现了加速</b>。</p><p><i>PS：此处可能涉及一些硬件相关的知识，建议自行查阅。比如GPU的基本架构以及SRAM/DRAM，还有roofline图等等。由于本人之前已经略有了解，在此就不做说明了。</i></p><p><i>PPS：虽然我上面说的是显存（FlashAttention里用的也是HBM），但是也有部分文章里计算时采用的是L2缓存带宽而不是显存带宽。这个……我只能说还是得上手实操，这里我们就先当它是显存。</i></p><h3 id="outline_17">分块attention</h3><p><img src="https://pic3.zhimg.com/v2-0f1fa95d351496dd7ed8e5625da64f22_r.jpg" class="large"></p><p>以上是分块attention示意图，我们以N为句子长度，d为特征维数。</p><p>正常的attention是直接计算 <span><span><span class="tex2jax_ignore math-holder">QK^{T}</span></span></span> ，但是这里改成分块进行，比如图上是把Q分了3块，Q分了两块。</p><p>我们注意<b>分块和传统attention实际上并不一致</b>。</p><p>首先是这个O的内容和正常的attention有区别。正常情况下在P×V这一步应该是 <span><span><span class="tex2jax_ignore math-holder">(B_{r},N)\times(N,d)</span></span></span> 得到 <span><span><span class="tex2jax_ignore math-holder">(B_{r},d)</span></span></span> 的一个矩阵。<b>现在虽然形状还是 <span><span><span class="tex2jax_ignore math-holder">(B_{r},d)</span></span></span> ，但是由于KV都是分了块的，所以在算结果的时候就有一部分没参与运算，只计算了前</b> <span><span><span class="tex2jax_ignore math-holder">B_{c}</span></span></span> <b>个数的结果，相当于结果是错的</b>。</p><p>但是问题也不止出在这，因为你往前推一步就会发现，前面的softmax也不对。<b>正常的softmax是一整行做softmax，但是这边只有一部分。</b>但是相比后面的output错的没那么离谱，因为这儿只不过切成了两块罢了，做softmax之前是每个数还是正确的。</p><h3 id="outline_18">前向计算</h3><p>我们直接来看算法流程图。</p><p><img src="https://picx.zhimg.com/v2-70eee1fbe8e1e9dc3544a5405b94e1c7_r.jpg" class="large"></p><p>关注5-15行这个循环即可。这个循环的意思大致是，on chip地遍历每一个分块（只读写L1而不是显存，因此速度很快），然后你需要存下来的东西是<b>当前状态下的O矩阵，以及当前状态下的 <span><span><span class="tex2jax_ignore math-holder">l</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">m</span></span></span></b> （即每行的和以及每行的最大值），你就可以计算出跟传统attention一模一样的结果。</p><p>至于为什么这两种计算方法是一样的，由于太复杂了这里就不做赘述了，建议去看本章开头推荐的文章。不过如果你喜欢一些定性的描述，大概可以这么理解：我们前面说了，最大的问题是P×V=O这一步算出来的O只有一部分（本来应该拿N列进行计算，现在只拿了 <span><span><span class="tex2jax_ignore math-holder">B_{c}</span></span></span> 列），<b>由于我们的分块其实是线性可加的，因此你可以迭代计算O的数值然后原地加到上一轮的数值上即可，只不过每次更新的时候需要考虑softmax带来的变化，最终这样更新出来的O和标准attention的O数学上严格等价</b>。</p><p>总之，相比原始attention对HBM的大规模读写（QKSPV），Flash Attention有很大区别，因为它把内层循环跑完只需要在HBM上写<b>一个完整的O矩阵，加上多次l和m（纯数字）。</b>相当于不用写SP这些中间矩阵，但是需要循环地写很多次O。</p><p>考虑复杂度，l和m的复杂度只有O(N)，而Q/K/O的复杂度是O(Nd)，S和P的复杂度是O(N^2)，因此传统attention的复杂度大概是 <span><span><span class="tex2jax_ignore math-holder">O(Nd + N^2)</span></span></span> ，FlashAttention的复杂度大概是 <span><span><span class="tex2jax_ignore math-holder">O(T_{c}Nd)</span></span></span> ，其中 <span><span><span class="tex2jax_ignore math-holder">T_{c}</span></span></span> 是上面算法流程里的外层循环次数。在正常情况下 <span><span><span class="tex2jax_ignore math-holder">T_{c}d</span></span></span> 并不会很大，相比O(N^2)还是省了不少复杂度的。</p><h3 id="outline_19">反向传播</h3><p>因为分块attention没有存储S和P两个矩阵，所以反向传播的时候需要先on chip地把SP算出来（当然不是整个算出来，而是只算那一小块），之后的反向传播过程和正常的attention一致，只不过是分块的。</p><p>至于计算量其实稍微脑测一下也大致有数，尽管迭代了多次，但是总计算量的量级应该不会有太大区别。我之前觉得Flash Attention的计算量好像变大了，因为直觉上多次更新softmax会带来计算量的增加，但是跟O的计算比起来似乎不是大头。</p><p><b>我们总结一下Flash Attention。</b></p><p><b>正常情况下的attention涉及到显存的多次读写导致速度较慢。Flash Attention通过将矩阵分块，可以把一些中间矩阵（如S和P）的计算全都放在L1缓存上，从而避免多次显存读写实现了加速。</b></p><h2 id="outline_20"><b>人类对齐</b></h2><p>天工在report里似乎没怎么提这个事，千问说的比较多。人类对齐的主要作用是让模型“说话”的风格符合人类习惯，一般经过人类对齐的模型主要是用来当聊天机器人的，所以你看各个团队发布的模型一般都会有base模型和chat模型。</p><p>众所周知，对齐最常用的也就两种：Supervised Finetune（SFT）和Reinforcement Learning with Human Feedback（RLHF）。</p><h3 id="outline_21">一些牢骚</h3><p>我们先说一些天工在论文里发的牢骚，虽然跟human alignment无关，但是因为涉及到SFT所以还是放在这里。</p><p>天工提到了一些很有意思的事情：大家做LLM的传统思路是无监督pretrain+有监督finetune，但是实际上你可以用一些投机取巧的办法，尤其是在GPT-4这种东西出来之后<b>获得有监督和高质量数据变得非常容易</b>。</p><p>6月份有个很有名的论文叫textbooks are all you need，用了7B token做pretrain加200M做finetune就训了一个非常好的coding模型出来，而且这个模型还很小（1.3B）。但是如果你从头看到这里，想到scaling law就知道事出反常必有妖。实际上在这篇论文里是先使用GPT-4筛选出一些高质量的code来训一个“判断代码质量高低”的分类器，然后那些pretrain和finetune用的“教科书级的代码”都是用GPT-3.5生成的。</p><p>也就是说，无论是pretrain还是finetune的数据都集中在coding这个领域，并且数据质量极高，这使得模型在coding方面有极度出众的能力，但是这其实稍稍违背了做大模型的初衷。</p><p>如果你只是想在某个特定任务上刷榜，你甚至可以在pretrain的时候就选in-domain的数据，比如说做coding就直接全用代码数据就完了，没必要像通用LLM一样用一大堆无关数据来做pretrain再用领域数据做finetune。或者反过来说，<b>尽管大量无监督数据带来了LLM的通用能力，但是如果你只需要在某些特定task上的优秀表现，全程使用高质量的in-domain数据+小模型可能反而比大模型还强</b>。</p><p>但是你仔细一想，这不是又退化成了传统deep learning了吗……</p><h3 id="outline_22">SFT</h3><p>没什么特别好说的，千问在论文里着重强调了<b>内容安全性</b>，也就是对暴力/偏见/色情之类的话题做了很多标注数据，尽量减少模型在聊天的时候口出狂言。</p><p>此外就是SFT时的格式参照了OpenAI的ChatML，也就是指定角色（system/user/assistant等）的json格式，这样使得模型可以区分消息来源，从而让模型能够分析更复杂的会话数据。</p><h3 id="outline_23">RLHF</h3><p>对强化学习以及PPO不了解的同学建议阅读我很久以前写的文章：<a href="https://zhuanlan.zhihu.com/p/341561826" target="_blank" class="link">密排六方橘子：强化学习：PPO(Proximal Policy Optimization)在谈恋爱中的应用</a>，私以为写得还是很清楚的，但是你如果懒得看，我们这里也可以用几句话简单描述一下强化学习（这里指PPO）的基本流程：</p><p>强化学习的基本思想是“<b>模型在和环境的互动中学到知识</b>”，因此你的模型主要功能是“做出动作”，或者叫它<b>actor</b>。在我们的场景下，LLM就是这个actor，LLM的输出的内容就是“动作“，然后我们对输出的评价其实就是“环境”，因为我们需要告诉actor它的动作是好是坏。<b>此时LLM的参数更新方法和传统的梯度更新一样</b>，也就是你把reference当作gt，然后根据每个act的输出概率做梯度下降即可。</p><p>上述过程在传统RL中叫做Policy Gradient（PG，策略梯度），在此基础上做一些优化之后就变成了PPO。但是这种基于梯度下降的方法也有缺陷，比如说由于有时决策序列很长，因为PG是基于采样的（每次采样的是一个决策序列，搜索空间是指数增长的），所以这种情况下<b>训练稳定性非常差</b>。<b>因此我们可以再添加一个模型来“估计当前状态是好是坏”</b>（有兴趣的可以查一下“优势函数”），这两个模型一起训练，这样actor的训练就会稳定很多，<b>我们管它叫actor-critic方法</b>。</p><p><i>PS：我觉得应该还挺好理解的？看一遍可能有点懵，多看两遍应该就很清晰了。</i></p><p>在RLHF中有四个模型，<b>分别是actor，reference，reward和critic。</b>这四个模型的作用大致如下：</p><p>actor：负责输出结果</p></div><div class="sub-page"><h2 class="sub-title" id="outline_24">LLM入门指南(2)</h2><p class="meta avatar"><span class="ant-avatar ant-avatar-circle ant-avatar-image css-16cpc8y"><img src="https://pica.zhimg.com/v2-c0b94d9e470c92e33dff0672bda2d24d_l.jpg?source=172ae18b 2x" class="tiny"></span><span class="author">密排六方橘子</span><span>2024-01-08 15:21</span></p><p></p><p>critic：辅助actor估计当前状态</p><p>reference：base模型，负责约束actor的输出不要跟base模型差太远</p><p>reward：用于自动评价输出的结果好坏</p><h3 id="outline_25">Reward / Critic model</h3><p>为训练Reward model，我们需要先训一个<b>偏好模型</b>（preference model，注意区分一下reference……），这个模型的作用顾名思义，就是判断回答的好坏。具体来说就是对同一个prompt有两个回答（一个pair），然后模型负责判断哪一个比较好。</p><p>实际上偏好模型也不止这一种，比如instruct GPT是拿了9个回复一起排序；llama2虽然也只用了2个回复，但是在标注的时候还添加了标注人员对自己标注的置信度，以及回复的安全性。总之模型设计上还算比较自由，具体怎么操作可以自己开脑洞。</p><p>偏好模型的训练也分两步，pretrain和finetune，pretrain这一步可以简称PMP。这两个步骤只是数据稍微有点区别，finetune阶段的数据质量要高一些。不过文章里并没有介绍pretrain用的数据集有多大……</p><p><img src="https://pica.zhimg.com/v2-b18560fe3406b3499e908f8250ef7a7a_r.jpg" class="large"></p><p>实际上finetune之后的模型（RM）和只用pretrain的（PMP）主要是在千问自己的数据集上提升比较大（应该是针对性地构造了一些数据），但是在其他数据集上提升并不明显。</p><p>至于<b>preference model本身结构，就直接沿用了千问base模型，然后用上面说的数据进行PMP之后再finetune，finetune之后的模型就是reward model了，注意这个权重也被拿来初始化critic model。</b></p><h3 id="outline_26">Actor / Reference model</h3><p>这两个模型均使用base模型初始化，区别在于reference是不会变的，actor是不断更新的。</p><p>训练流程如下图所示：</p><p><img src="https://pica.zhimg.com/v2-180f4629d2ca870bed2f8efc4af5c890_r.jpg" class="large"></p><p>图中可以看到三个模块，其中initial language model就是reference model，中间的tuned language model就是我们之前说的actor-critic，最后右边红色框里的reward model。</p><p>训练流程也一目了然，reward model负责给模型的输出打分，reference model负责和模型输出计算KL散度（不能偏得太夸张），之后这两个合起来作为reward来给actor-critic更新梯度。</p><h2 id="outline_27">RWKV</h2><p>线性attention的变种，提这篇文章是为了给LLM学习者们提供一些额外的知识，毕竟基于线性attention的工作相比传统attention少很多，总之就算看到了也不必大惊小怪。</p><h3 id="outline_28">AFT</h3><p>讲RWKV之前我们需要先说AFT。</p><p>众所周知，传统Attention定义了QK的相似度： <span><span><span class="tex2jax_ignore math-holder">sim(Q_{i},K_{j})=exp(\frac{Q_{i}K_{j}^{T}}{\sqrt{D}})</span></span></span> ，可以把V写成这样：</p><p><img src="https://pic4.zhimg.com/v2-56e09890deb4536412bc5461f1cdab23_1440w.jpg" class="large"></p><p>这就带来了一个问题，计算的复杂度是随着序列长度平方增长 <span><span><span class="tex2jax_ignore math-holder">O(N^{2})</span></span></span> 的。所以一个直觉的想法是把QK相乘这个东西换掉，比如说它能不能是一次的，而不是二次的。</p><p>题外话：</p><p><i>如果你是学CV的，看到这个东西立刻会有至少两个deja vu的想法：</i></p><p><i>一个是比较古老的<b>Deformable DETR</b>，这个东西里面的attention不是正常的QK相乘，而是直接用query挂了个linear和softmax预测出来的。</i></p><p><i>另一个是非常古老的<b>squeeze-and-excitation</b>，也是某种attention，但是它也跟QK无关。</i></p><p><i>换句话说，attention绝对不止“QK相似度”这一种，只要你胆子够大，linear+softmax就是attention，乘回原始value上就完事了，这种attention不仅是CNN时代最常见的操作，而且复杂度还是一次的……</i></p><p>回到正题，我们考虑一次复杂度（线性attention）的QKV应该如何交互。</p><p>这里不妨先瞅一眼AFT的decoder形式（因为是decoder所以只能看到左边的）：</p><p><img src="https://pica.zhimg.com/v2-4eb52aa69158cb78fe62b1c6eedb7040_1440w.jpg" class="large"></p><p>其中 <span><span><span class="tex2jax_ignore math-holder">\sigma(Q_{i})</span></span></span> 是取sigmoid，右边分数里面的 <span><span><span class="tex2jax_ignore math-holder">w_{i,j}</span></span></span> 是可学习项。</p><p>也就是说，左边这个 <span><span><span class="tex2jax_ignore math-holder">\sigma(Q_{i})</span></span></span> 相当于把Q当成了一个门控函数，而右边的 <span><span><span class="tex2jax_ignore math-holder">K_{j}</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">w_{i,j}</span></span></span> 刚好是wx+b形式的一个值，它是一个自己冒出来的attention值（只通过K而不需要QK交互，和Deformable DETR非常像），突出一个简单粗暴。</p><p>可以想见这种形式的attention大概率还是能work的（我们前面已经铺垫很多次了），只不过没有传统attention复杂而已。</p><p>我们仔细琢磨一下上面那个公式的形式。</p><p>在正常的attention里 <span><span><span class="tex2jax_ignore math-holder">Q_{i}</span></span></span> 是放在求和项里面的，并且拿不出来（因为跟 <span><span><span class="tex2jax_ignore math-holder">Q_{i}</span></span></span> 确实有关系），每来一个 <span><span><span class="tex2jax_ignore math-holder">Q_{i}</span></span></span> 就要重新计算一遍所有QK attention的内容。</p><p>但是AFT里求和项里不包含 <span><span><span class="tex2jax_ignore math-holder">Q_{i}</span></span></span> ，这时候我们就可以把<b>i-1位置的K和V之类的东西都存起来</b>，在i位置可以直接复用这一结果，相当于迭代更新右边的求和项，这个形式跟RNN就非常像了，因此<b>可以实现和RNN类似的sequential decoding</b>。</p><p>但是另一方面AFT也有个问题，就是它仍然是 <span><span><span class="tex2jax_ignore math-holder">O(N^{2})</span></span></span> 的，因为求和项内部还是有个跟 <span><span><span class="tex2jax_ignore math-holder">i</span></span></span> 相关的 <span><span><span class="tex2jax_ignore math-holder">w_{i,j}</span></span></span> ，导致虽然你能sequential decoding，但是<b>每一步迭代新增的计算量是</b> <span><span><span class="tex2jax_ignore math-holder">O(N)</span></span></span> ，最终复杂度还是 <span><span><span class="tex2jax_ignore math-holder">O(N^{2})</span></span></span> 。</p><h3 id="outline_29">RWKV的线性attention</h3><p>我们前面说了由于 <span><span><span class="tex2jax_ignore math-holder">w_{i,j}</span></span></span> 的存在，AFT的计算量仍然没能缩到O(N)，于是有了如下改进方案：</p><p><img src="https://picx.zhimg.com/v2-a07731c3b7300d219d29fe415aa0a2b9_r.jpg" class="large"></p><p>左边的是细节图，右边的是整体框架。</p><p><b>我们先来看下半的time mixing部分</b>。RKV就是transformer里的QKV，黄色框里的 <span><span><span class="tex2jax_ignore math-holder">\mu</span></span></span> 是一个混合用的参数，具体作用就是把上一时刻的输出和这一时刻做一个混合：</p><p><img src="https://pic3.zhimg.com/v2-8ff1c1e94b8c38e52a422e5318b3ef6c_r.jpg" class="large"></p><p>有LSTM那个味儿了。</p><p>R一支自然没什么好说的，跟AFT一样，而WKV模块长这样：</p><p><img src="https://pic2.zhimg.com/v2-f91a149ff6ddfb04c6eea1bb5449d9c3_r.jpg" class="large"></p><p>可以看到分子分母都是两项，前一项是求和，与前t-1项相关，后一项就是当前t时刻的值。</p><p>前一项求和明显是设计过的，很容易发现 <span><span><span class="tex2jax_ignore math-holder">w</span></span></span> 前面的系数 <span><span><span class="tex2jax_ignore math-holder">-(t-1-i)</span></span></span> 是随着i的增加而增加的并且一直小于0，也就是说离当前t的相对位置越远，对当前的影响就越小（文中叫decay），这也是符合直觉的设计。</p><p>后一项里有一个从没见过的东西是 <span><span><span class="tex2jax_ignore math-holder">u</span></span></span> ，作者表示 <span><span><span class="tex2jax_ignore math-holder">w</span></span></span> 在一些情况下可能出现退化（比如说是0），所以这里干脆用一个新的独立参数来防止这个问题。</p><p>对照一下前面的AFT，可以发现RWKV这里就是改了一下 <span><span><span class="tex2jax_ignore math-holder">w</span></span></span> 和 <span><span><span class="tex2jax_ignore math-holder">u</span></span></span> ，这样保留了sequential decoding的优势，同时也把计算复杂度降下来了，可以参考作者在原文里的公式：</p><p><img src="https://pic1.zhimg.com/v2-597c68f3b49abb217925bdb768776bd2_1440w.jpg" class="large"></p><p>显然当前时刻t的结果可以由<b>上一时刻的结果+一些和序列长度无关的O(1)计算</b>得出，因此整体复杂度是O(N)，非常环保。</p><p>然后再来看<b>channel mixing模块</b>。</p><p>这里的R'K'V'都是带撇的，显然意思是值跟前面的RKV不一样，重新算了一下。同样对于左边R一支没什么好说的，右边一支的计算方法显得很莫名其妙：</p><p><img src="https://pic1.zhimg.com/v2-141c85bc035a9ce92b6f6402a92f2106_1440w.jpg" class="large"></p><p>相当于把 <span><span><span class="tex2jax_ignore math-holder">k_{t}'</span></span></span> 先做一个squared ReLU，然后再把 <span><span><span class="tex2jax_ignore math-holder">W_{v}'</span></span></span> 乘上去。</p><p>首先是这个squared ReLU就显得很怪异，它未免比ReLU高明到哪里去，可能实验结果好一点所以用了。</p><p>再者就是把 <span><span><span class="tex2jax_ignore math-holder">W_{v}'</span></span></span> 乘到 <span><span><span class="tex2jax_ignore math-holder">k_{t}'</span></span></span> 上，也就是这里把k当成和v一个东西，我觉得有些抽象……看作者的意思是这个思路也取材于两三年前的MLP-Mixer（我当时看过这篇文章，不过忘了咋回事了）有空的读者大概可以去瞅一眼。</p><p>总之这里我看不出有什么理由一定要是这种形式，不过炼丹本来就是先有结果然后往上凑理由，可能作者比较耿直，懒得写理由了……</p><h3 id="outline_30">评价</h3><p>对RWKV的评价，我觉得参考这个问题里各路大哥的评价即可。</p><p><a href="https://www.zhihu.com/question/602564718" target="_blank" class="link">如何评价最新的RWKV论文 (arXiv 2305.13048)？</a></p><p>以及RWKV虽然长得像RNN，但是可以进行并行训练。这里就不细说了，同样可以参照上面问题的回答。</p><p>以上便是本文全部内容，日后可能会有不定时勘误和内容补充，不过基本算是完工了……要去码下一篇文了。</p></div><div class="footer"><div class="ant-space css-16cpc8y ant-space-horizontal ant-space-align-center" style="flex-wrap: wrap;"><div class="ant-space-item"><span class="ant-tag css-16cpc8y">LLM</span></div><div class="ant-space-item"><span class="ant-tag css-16cpc8y">大语言模型</span></div><div class="ant-space-item"><span class="ant-tag css-16cpc8y">自然语言处理</span></div></div></div><p style="text-align:center;margin-top:100px;">由 <a href="https://circlereader.com" target="_blank">Circle 阅读助手</a> 生成</p></div></div></div></div></body></html>