<h1>simple information theory</h1>
<p>本文使用一个最简单的例子，帮助大家理解信息论。</p>
<h2>一、词汇的编码</h2>
<p>小张是我的好朋友，最近去了美国。</p>
<p>我们保持着邮件联系。小张写信的时候，只使用4个词汇：狗，猫，鱼，鸟。</p>
<p>第一封信写着&quot;狗猫鱼鸟&quot;，第二封信写&quot;鱼猫鸟狗&quot;。</p>
<p>信件需要二进制编码，在互联网传递。两个二进制位就可以表示四个词汇。</p>
<blockquote>
<ul>
<li>狗 00</li>
<li>猫 01</li>
<li>鱼 10</li>
<li>鸟 11</li>
</ul>
</blockquote>
<p>所以，第一封信&quot;狗猫鱼鸟&quot;的编码是<code>00011011</code>，第二封信&quot;鱼猫鸟狗&quot;的编码是<code>10011100</code>。</p>
<h2>二、词汇的分布</h2>
<p>最近，小张开始养狗，信里提到狗的次数，多于其他词汇。假定概率分布如下。</p>
<blockquote>
<ul>
<li>狗：50%</li>
<li>猫：25%</li>
<li>鱼：12.5%</li>
<li>鸟：12.5%</li>
</ul>
</blockquote>
<p>小张的最新一封信是这样的。</p>
<blockquote>
<p>狗狗狗狗猫猫鱼鸟</p>
</blockquote>
<p>上面这封信，用前一节的方法进行编码。</p>
<blockquote>
<p>0000000001011011</p>
</blockquote>
<p>一共需要16个二进制。互联网的流量费很贵，有没有可能找到一种更短编码方式？</p>
<p>很容易想到，&quot;狗&quot;的出现次数最多，给它分配更短的编码，就能减少总的长度。请看下面的编码方式。</p>
<blockquote>
<ul>
<li>狗 0</li>
<li>猫 10</li>
<li>鱼 110</li>
<li>鸟 111</li>
</ul>
</blockquote>
<p>使用新的编码方式，小张的信&quot;狗狗狗狗猫猫鱼鸟&quot;编码如下。</p>
<blockquote>
<p>00001010110111</p>
</blockquote>
<p>这时只需要14个二进制位，相当于把原来的编码压缩了12.5%。</p>
<p>根据新的编码，每个词只需要1.75个二进制位（14 / 8）。可以证明，这是最短的编码方式，不可能找到更短的编码，详见后文。</p>
<h2>三、编码方式的唯一性</h2>
<p>前一节的编码方式，狗的编码是<code>0</code>，这里的问题是，可以把这个编码改成<code>1</code>吗，即下面的编码可行吗？</p>
<blockquote>
<ul>
<li>狗 1</li>
<li>猫 10</li>
<li>鱼 110</li>
<li>鸟 111</li>
</ul>
</blockquote>
<p>回答是否定的。如果狗的编码是<code>1</code>，会造成无法解码，即解码结果不唯一。<code>110</code>有可能是&quot;狗猫&quot;，也可能是&quot;鱼&quot;。只有&quot;狗&quot;为<code>0</code>，才不会造成歧义。</p>
<p>下面是数学证明。一个二进制位有两种可能<code>0</code>和<code>1</code>，如果某个事件有多于两种的结果（比如本例是四种可能），就只能让<code>0</code>或<code>1</code>其中一个拥有特殊含义，另一个必须空出来，保证能够唯一解码。比如，<code>0</code>表示狗，<code>1</code>就必须空出来，不能有特殊含义。</p>
<p>同理，两个二进制位可以表示四种可能：<code>00</code>、<code>01</code>、<code>10</code>和<code>11</code>。上例中，<code>0</code>开头的编码不能用了，只剩下<code>10</code>和<code>11</code>可用，用<code>10</code>表示猫，为了表示&quot;鱼&quot;和&quot;鸟&quot;，必须将<code>11</code>空出来，使用三个二进制位表示。</p>
<p>这就是，上一节的编码方式是如何产生的。</p>
<h2>四、编码与概率的关系</h2>
<p>根据前面的讨论，可以得到一个结论：<strong>概率越大，所需要的二进制位越少。</strong></p>
<blockquote>
<ul>
<li>狗的概率是50%，表示每两个词汇里面，就有一个是狗，因此单独分配给它1个二进制位。</li>
<li>猫的概率是25%，分配给它两个二进制位。</li>
<li>鱼和鸟的概率是12.5%，分配给它们三个二进制位。</li>
</ul>
</blockquote>
<p>香农给出了一个数学公式。<code>L</code>表示所需要的二进制位，<code>p(x)</code>表示发生的概率，它们的关系如下。</p>
<p><img src="https://www.wangbase.com/blogimg/asset/201908/bg2019080107.jpg" alt="img"></p>
<p>通过上面的公式，可以计算出某种概率的结果所需要的二进制位。举例来说，&quot;鱼&quot;的概率是<code>0.125</code>，它的倒数为<code>8</code>， 以 2 为底的对数就是<code>3</code>，表示需要3个二进制位。</p>
<p>知道了每种概率对应的编码长度，就可以计算出一种概率分布的平均编码长度。</p>
<p><img src="https://www.wangbase.com/blogimg/asset/201908/bg2019080108.jpg" alt="img"></p>
<p>上面公式的<code>H</code>，就是该种概率分布的平均编码长度。理论上，这也是最优编码长度，不可能获得比它更短的编码了。</p>
<p>接着上面的例子，看看这个公式怎么用。小张养狗之前，&quot;狗猫鱼鸟&quot;是均匀分布，每个词平均需要2个二进制位。</p>
<blockquote>
<p>H = 0.25 x 2 + 0.25 x 2 + 0.25 x 2 + 0.25 x 2
= 2</p>
</blockquote>
<p>养狗之后，&quot;狗猫鱼鸟&quot;不是均匀分布，每个词平均需要1.75个二进制位。</p>
<blockquote>
<p>H = 0.5 x 1 + 0.25 x 2 + 0.125 x 3 + 0.125 x 3
= 1.75</p>
</blockquote>
<p>既然每个词是 1.75 个二进制位，&quot;狗狗狗狗猫猫鱼鸟&quot;这8个词的句子，总共需要14个二进制位（8 x 1.75）。</p>
<h2>五、信息与压缩</h2>
<p>很显然，不均匀分布时，某个词出现的概率越高，编码长度就会越短。</p>
<p>从信息的角度看，如果信息内容存在大量冗余，重复内容越多，可以压缩的余地就越大。日常生活的经验也是如此，一篇文章翻来覆去都是讲同样的内容，摘要就会很短。反倒是，每句话意思都不一样的文章，很难提炼出摘要。</p>
<p>图片也是如此，单调的图片有好的压缩效果，细节丰富的图片很难压缩。</p>
<p>由于信息量的多少与概率分布相关，所以在信息论里面，信息被定义成不确定性的相关概念：概率分布越分散，不确定性越高，信息量越大；反之，信息量越小。</p>
<h2>六、信息熵</h2>
<p>前面公式里的<code>H</code>（平均编码长度），其实就是信息量的度量。<code>H</code>越大，表示需要的二进制位越多，即可能发生的结果越多，不确定性越高。</p>
<p>比如，<code>H</code>为<code>1</code>，表示只需要一个二进制位，就能表示所有可能性，那就只可能有两种结果。如果<code>H</code>为<code>6</code>，六个二进制位表示有64种可能性，不确定性大大提高。</p>
<p>信息论借鉴了物理学，将<code>H</code>称为&quot;信息熵&quot;（information entropy）。在物理学里，<a href="http://www.ruanyifeng.com/blog/2013/04/entropy.html">熵</a>表示无序，越无序的状态，熵越高。</p>
<h2>七、信息量的实例</h2>
<p>最后，来看一个例子。如果一个人的词汇量为10万，意味着每个词有10万种可能，均匀分布时，每个词需要 16.61 个二进制位。</p>
<blockquote>
<p>log₂(100, 000) = 16.61</p>
</blockquote>
<p>所以，一篇1000个词的文章，需要 1.6 万个二进制位（约为 2KB）。</p>
<blockquote>
<p>16.61 x 1000 = 16,610</p>
</blockquote>
<p>相比之下，一张 480 x 640、16级灰度的图片，需要123万个二进制位（约为 150KB）。</p>
<blockquote>
<p>480 x 640 x log₂(16) = 1,228,800</p>
</blockquote>
<p>所以，一幅图片所能传递的信息远远超过文字，这就是&quot;一图胜千言&quot;吧。</p>
<p>上面的例子是均匀分布的情况，现实生活中，一般都是不均匀分布，因此文章或图片的实际文件大小都是可以大大压缩的。</p>
